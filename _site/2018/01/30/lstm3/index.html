<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
    
  <!-- Enable responsiveness on mobile devices-->
  <!--<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">-->
 

  <title>
    
      Using Pre Trained Word Vector Embeddings for Sequence Classification using LSTM &middot; 
      pyVision
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/pyVision/public/css/poole.css">
  <link rel="stylesheet" href="/pyVision/public/css/syntax.css">
  <link rel="stylesheet" href="/pyVision/public/css/lanyon.css">
  <!--<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|PT+Sans:400">-->
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/pyVisionpublic/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/pyVisionpublic/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js"></script>
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({ 
                config: ["MMLorHTML.js"], 
                extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"], 
                jax: ["input/TeX"], 
                tex2jax: { 
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ], 
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ], 
                    processEscapes: false 
                }, 
                TeX: { 
                    TagSide: "right", 
                    TagIndent: ".8em", 
                    MultLineWidth: "85%", 
                    equationNumbers: { 
                       autoNumber: "AMS", 
                    }, 
                    unicode: { 
                       fonts: "STIXGeneral,'Arial Unicode MS'" 
                    } 
                }, 
                showProcessingMessages: false 
            }); 
</script>
<!--<script src="/pyVision/javascripts/gitdata.js"></script>-->
	<script type="text/javascript">
    	$(function() {
        $("#display-projects").getRepos("pi19404"); //Add your github username.
    	});
	</script>
<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<link href='http://alexgorbatchev.com/pub/sh/2.1.364/styles/shCore.css' rel='stylesheet' type='text/css'/> 
<link href='http://alexgorbatchev.com/pub/sh/2.1.364/styles/shThemeDefault.css' rel='stylesheet' type='text/css'/> 
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shCore.js' type='text/javascript' > </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushCpp.js' type='text/javascript' > </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushCSharp.js' type='text/javascript' > </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushCss.js' type='text/javascript' > </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushJava.js' type='text/javascript' > </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushJScript.js' type='text/javascript' /> </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushPhp.js' type='text/javascript' > </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushPython.js' type='text/javascript' > </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushRuby.js' type='text/javascript' > </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushSql.js' type='text/javascript' > </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushVb.js' type='text/javascript' > </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushXml.js' type='text/javascript' > </script>
<script src='http://alexgorbatchev.com/pub/sh/2.1.364/scripts/shBrushPerl.js' type='text/javascript' > </script>
<script language='javascript'> 
SyntaxHighlighter.config.bloggerMode = false;
SyntaxHighlighter.all();
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-38535188-2', 'auto');
  ga('send', 'pageview');

</script>
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">





  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/pyVision/">Home </a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/pyVision//about/">About</a>
        
      
    
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
    
      
    

    <!-- <a class="sidebar-nav-item" href="http://www.github.com/pi19404/pyVision/zipball/master">Download</a>
    <a class="sidebar-nav-item" href="http://www.github.com/pi19404/pyVision">GitHub project</a> -->
    <!-- <a class="sidebar-nav-item" ><g:plusone size="medium">Categories</g:plusone> </a> -->

    <div >
      <nav class="sidebar-nav-item">
        <g:plusone size="medium">Categories</g:plusone>
        
        
         
         <a class="sidebar-nav-item" href="/pyVision/category1/ai">
            AI
         </a>
        
         
         <a class="sidebar-nav-item" href="/pyVision/category1/embedded-firmware">
            Embedded Firmware
         </a>
        
         
         <a class="sidebar-nav-item" href="/pyVision/category1/linux">
            Linux
         </a>
        
         
         <a class="sidebar-nav-item" href="/pyVision/category1/raspberry-pi">
            Raspberry PI
         </a>
        
         
         <a class="sidebar-nav-item" href="/pyVision/category1/signal-processing">
            Signal Processing
         </a>
        
         
         <a class="sidebar-nav-item" href="/pyVision/category1/software">
            Software
         </a>
        
         
         <a class="sidebar-nav-item" href="/pyVision/category1/software-installation">
            Software Installation
         </a>
        
         
         <a class="sidebar-nav-item" href="/pyVision/category1/software-installation">
            Software installation
         </a>
        
      </nav>
    
    </div>
    <!-- <div>
      <ul>
        
        
        <li><a href="/pyVision//category/AI/index.html">AI</a></li>
        
        
        <li><a href="/pyVision//category/Signal+Processing/index.html">Signal Processing</a></li>
        
        
        <li><a href="/pyVision//category/Raspberry+PI/index.html">Raspberry PI</a></li>
        
        
        <li><a href="/pyVision//category/Linux/index.html">Linux</a></li>
        
        
        <li><a href="/pyVision//category/Software/index.html">Software</a></li>
        
        
        <li><a href="/pyVision//category/Embedded+Firmware/index.html">Embedded Firmware</a></li>
        
        
        <li><a href="/pyVision//category/Software+Installation/index.html">Software Installation</a></li>
        
        
        <li><a href="/pyVision//category/Software+installation/index.html">Software installation</a></li>
        
        </ul>
    
    </div> -->
<a class="sidebar-nav-item" >
<div class='g-person' data-href='https://plus.google.com/115840780257908006648' data-layout='landscape' data-rel='author' data-showcoverphoto='false' data-showtagline='false' data-theme='dark' data-width='100'></a>
</div>

<div class='widget-content'>
<script type="text/javascript" src="http://jj.revolvermaps.com/2/1.js?i=9bj6dpaloim&amp;s=220&amp;m=7&amp;v=true&amp;r=true&amp;b=ffffff&amp;n=false&amp;c=ff0000" async="async"></script>
</div>

<script type="text/javascript">
function cantload() {
img = document.getElementById("clustrMapsImg");
img.onerror = null;
img.src = "http://www2.clustrmaps.com/images/clustrmaps-back-soon.jpg";
document.getElementById("clustrMapsLink").href = "http://www2.clustrmaps.com";
}
img = document.getElementById("clustrMapsImg");
img.onerror = cantload;
</script>
<div class='widget-content'>
<div id="clustrmaps-widget"></div><script type="text/javascript">var _clustrmaps = {'url' : 'http://pi-virtualworld.blogspot.com', 'user' : 1165875, 'server' : '3', 'id' : 'clustrmaps-widget', 'version' : 1, 'date' : '2015-04-05', 'lang' : 'en', 'corners' : 'square' };(function (){ var s = document.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'http://www3.clustrmaps.com/counter/map.js'; var x = document.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x);})();</script><noscript><a href="http://www3.clustrmaps.com/user/ace11ca33"><img src="http://www3.clustrmaps.com/stats/maps-no_clusters/pi-virtualworld.blogspot.com-thumb.jpg" alt="Locations of visitors to this page" /></a></noscript>
</a>

<!-- Copyright (c)2009 Site Meter -->


</div>

<div class='widget-content'>
<script type="text/javascript" src="http://feedjit.com/serve/?vv=1022&amp;tft=3&amp;dd=0&amp;wid=f31e21a2d981f025&amp;pid=0&amp;proid=0&amp;bc=000000&amp;tc=F5F5F5&amp;brd1=454545&amp;lnk=C95050&amp;hc=FFFFFF&amp;hfc=5C5A5A&amp;btn=8A0214&amp;ww=200&amp;went=10"></script><noscript><a href="http://feedjit.com/">Feedjit Live Blog Stats</a></noscript>
</div>
 
<!-- Site Meter -->
<script type="text/javascript" src="http://s30.sitemeter.com/js/counter.js?site=s30pi19404">
</script>
<noscript>
<a href="http://s30.sitemeter.com/stats.asp?site=s30pi19404" target="_top">
<img src="http://s30.sitemeter.com/meter.asp?site=s30pi19404" alt="Site Meter" border="0"/></a>
</noscript>   
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2022. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      
        <div class="container">
          <h1 class="header">
            <a href="/" title="Home">pyVision</a></br>
            <small>A Machine Learning and Signal Processing toolbox</small>
          </h1>

        <section id="downloads" class="clearfix">
          <a href="https://github.com/pi19404/pyVision/zipball/master" id="download-zip" class="button"><span>Download .zip</span></a>
          <a href="https://github.com/pi19404/pyVision/tarball/master" id="download-tar-gz" class="button"><span>Download.tar.gz</span></a>
          <a href="https://github.com/pi19404/pyVision" id="view-on-github" class="button"><span>View on GitHub</span></a>
		
	   <hr>
          </section>
    
	</br>	</br>
      <div class="content">
        <article class="post">
  <h1 class="post-title">Using Pre Trained Word Vector Embeddings for Sequence Classification using LSTM</h1>
  <span class="post-date">30 Jan 2018</span>
  <p>In this article we will look at using pre trained word vector embedding for sequence classification
using LSTM</p>

<p><strong>Using Pre-Trained Word Vector Embeddings</strong></p>

<p>In the article <a href="http://pi19404.github.io/pyVision/2017/05/13/spacy1/">NLP spaCy Word and document vectors</a> we saw how to get the word vector representation trained on common crawl corpus provided by spacy toolkit. We will use this pretrained word vector representation rather than training our own Embedding Layer</p>

<p>We cannot use the dataset provided by keras directly so we download the raw database</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz

</code></pre></div></div>

<p>We will use the following script to extract the data</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#this function will convert text to lowercase and will disconnect punctuation and special symbols from words
function normalize_text {
  awk '{print tolower($0);}' &lt; $1 | sed -e 's/\./ \. /g' -e 's/&lt;br \/&gt;/ /g' -e 's/"/ " /g' \
  -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\!/ \! /g' -e 's/\?/ \? /g' \
  -e 's/\;/ \; /g' -e 's/\:/ \: /g' &gt; $1-norm
}

cd ..
mkdir nbsvm_run; cd nbsvm_run

wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
tar -xvf aclImdb_v1.tar.gz
rm aclImdb_v1.tar.gz

for j in train/pos train/neg test/pos test/neg; do
  for i in `ls aclImdb/$j`; do cat aclImdb/$j/$i &gt;&gt; temp; awk 'BEGIN{print;}' &gt;&gt; temp; done
  normalize_text temp
  mv temp-norm aclImdb/$j/norm.txt
  rm temp
done

mkdir data
mv aclImdb/train/pos/norm.txt data/train-pos.txt
mv aclImdb/train/neg/norm.txt data/train-neg.txt
mv aclImdb/test/pos/norm.txt data/test-pos.txt
mv aclImdb/test/neg/norm.txt data/test-neg.txt

</code></pre></div></div>

<p>The above script store postive examples in file <code class="language-plaintext highlighter-rouge">data/train-pos.txt</code> and negative examples in <code class="language-plaintext highlighter-rouge">data/train-neg.txt</code>.</p>

<p>we will use spacy to get word vector embeddings</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
import imdb
import os
import spacy


def pad_vec_sequences(sequences, maxlen=40):
        new_sequences = []
        for sequence in sequences:

            orig_len, vec_len = np.shape(sequence)

            if orig_len &lt; maxlen:
                new = np.zeros((maxlen, vec_len))
                for k in range(maxlen-orig_len,maxlen):
                    new[k:, :] = sequence[k-maxlen+orig_len]

            else:
                new = np.zeros((maxlen, vec_len))
                for k in range(0,maxlen):
                    new[k:,:] = sequence[k]

            new_sequences.append(new)

        return np.array(new_sequences)

def test():        
    nlp1 = spacy.load('en')

    f="data/train-pos.txt"
    train_data=[]
    Xtrain=[]
    labels=[]
    for sentence in open(f).xreadlines():
        
        row={}

        t=[]
        try:
            sentence1 = ''.join([i if ord(i) &lt; 128 else ' ' for i in sentence])
            r=unicode(sentence1)
            doc = nlp1(r)
            #print doc
            for token in doc:
                t.append(token.vector)

            row['text']=sentence;
            row['feature']=t;
            row['lable']=1;
            Xtrain.append(t)
            labels.append(1)
            train_data.append(row)
        except:
            print "skip data row",sentence


    f = "data/train-neg.txt"
    for sentence in open(f).xreadlines():

        row={}

        t=[]
        try:
            sentence1=''.join([i if ord(i) &lt; 128 else ' ' for i in sentence])
            r=unicode(sentence1)
            doc = nlp1(r)
            #print doc
            for token in doc:
                t.append(token.vector)

            row['text']=sentence;
            row['feature']=t;
            row['lable']=1;
            Xtrain.append(t)
            labels.append(0)
            train_data.append(row)
        except:
            print "skip data row",sentence


    print("completed processing the data")
    max_words=50

    num_classes = np.max(labels) + 1

    X_train = pad_vec_sequences(Xtrain, maxlen=max_words, padding="post", truncating="post")
    y_train = keras.utils.to_categorical(labels, num_classes)
    print("Training data: ")
    print(X_train.shape),(y_train.shape)

    X_train=np.reshape(X_train,(X_train.shape[0],max_words));

    print("input tensor")
    print(X_train.shape),y_train.shape

</code></pre></div></div>

<p>However directly using the word vector embeddings does not often lead to good results,
since the embeddings are trained over the common crawl corpus but it may no be representative
of the current database.</p>

<p>Another approach is to use pretrained word vector embeddings as initializations for Embedding Layer
Thus uses the glove2vec word vector embedding as initialization for the embedding layer of the LSTM
neural network.</p>

<p>You can download the glove2vec word vector embedding data from <a href="https://nlp.stanford.edu/projects/glove/">GloVe: Global Vectors for Word Representation</a> . We will download the glove2vec trained over wikipedia 2014 database using the top 400K words. <a href="http://nlp.stanford.edu/data/glove.6B.zip">http://nlp.stanford.edu/data/glove.6B.zip</a></p>

<p>We will use the 100 dimensional feature represenation in this example.</p>

<p>First we will read all the words from the glove2vec database and create a index of words
and corresponding word vector embedding</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    MAX_SEQUENCE_LENGTH = 50
    EMBEDDING_DIM = 100 
    embeddings_index = {}
    f = open(os.path.join('data/', 'glove.6B.100d.txt'))
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()

    print('Found %s word vectors.' % len(embeddings_index))

    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            embedding_matrix[i] = embedding_vector

</code></pre></div></div>

<p>For the LSTM network the traning data will consists of sequence of word vector indices representing the movie review from the IMDB dataset and the output will be sentiment. We will use the same database as used in the article
<a href="http://pi19404.github.io/pyVision/2018/01/30/lstm2/">Sequence classification with LSTM</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    f="data/tpos.txt"
    train_data=[]
    Xtrain=[]
    labels=[]
    print "reading postive samples"
    text=[]
    for sentence in open(f).xreadlines():
        #print "AAAAAAAAAA",sentence
        row={}
        tt = [];
        t=[]
        try:
            sentence1 = ''.join([i if ord(i) &lt; 128 else ' ' for i in sentence])

            str1 = " ".join(str(x) for x in sentence1);

            labels.append(1)

            text.append(str1)
        except:
            print "pskip data row",sentence
            continue;

    f="data/tneg.txt"
    print "reading negative samples"
    for sentence in open(f).xreadlines():
      
        row={}
        tt = [];
        t=[]
        try:
            sentence1 = ''.join([i if ord(i) &lt; 128 else ' ' for i in sentence])

            str1 = " ".join(str(x) for x in sentence1);
            #Xtrain.append(t)
            labels.append(0)

            text.append(str1)
        except:
            print "nskip data row",sentence
            continue;

</code></pre></div></div>

<p>each element of <code class="language-plaintext highlighter-rouge">text</code> contains an sequence of words. Now we use a Tokenizer to represent words by their word indexes.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical

MAX_NB_WORDS = 100000

tokenizer = Tokenizer(num_words=MAX_NB_WORDS)

#create the word indices
tokenizer.fit_on_texts(text)

word_index = tokenizer.word_index

#convert the training data to sequence of word indexes
sequences = tokenizer.texts_to_sequences(text)
</code></pre></div></div>

<p>This tells the tokenizer to consider only the most frequently occuring 100K words in the training dataset</p>

<p>we pad the sequences to create a sequence of same length to be passed to the LSTM network</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
</code></pre></div></div>

<p>First we create the embedding layer of LSTM network and initialize the weights using the embedding matrix
created in the earlier step. We set trainable to true so that embeddings are updated based on the current training
data as the embedding matrix passed is used as initialization . If we set to trainable to false then the embeddings will not be updated and network will use the embeddings as per the embdding matrix itself.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    embedding_layer = Embedding(input_dim=len(word_index) + 1,
                                output_dim=EMBEDDING_DIM,
                                weights=[embedding_matrix],
                                input_length=MAX_SEQUENCE_LENGTH,
                                trainable=True)
</code></pre></div></div>

<p>Now we will create the network and train the same</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.layers import LSTM
    from keras.layers import Dense, Embedding

    model = Sequential()
    model.add(embedding_layer)


    model.add(LSTM(128, return_sequences=False
               , input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)))


    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    model.fit(data,y_train,epochs=20, batch_size=100, verbose=1)
    save_model(model,"/tmp/model4")

    model.evaluate(X_test,y_test)

</code></pre></div></div>

<p>The approach of using exising word vector embeddings as initialization in embedding layer provides
the best results .</p>

<p>This approach can be also considered as approach for transfer learning where the embeedings learnt over
a large corups are being use to learn embeddings over a much smaller corpus ,how the initializations boosts
learning and generalization ability of the classifier at hand. If we dont have enough training data to learn
good embeddings over current data,it may limit the generalization ability of the network.however by using this approach we are able to use existing existing embedding matrix and tune it specifically for the data at hand.</p>

<p>The complete code for the same can be found at <a href="https://gist.github.com/pi19404/1d2f5cb64380740781338949ec4aac8a">code1</a></p>


</article>
<div id="page-navigation"> 
        <div class="clear">&nbsp;</div> 
        <div class="left"> 
         
                <a href="/pyVision//2018/01/30/lstm2/" title="Previous Post: 
Sequence classification with LSTM">&laquo; Sequence classification with LSTM</a> 
         
        </div> 

        <div class="right"> 
         
                <a href="/pyVision//2018/01/31/lstm4/" title="next Post: 
Natural Language Understanding - Intent Detection with Keras and LSTM">Natural Language Understanding - Intent Detection with Keras and LSTM &raquo; </a> 
         
        </div> 
        <div class="clear">&nbsp;</div> 
</div> 
<div id="disqus_thread"></div>
		<script type="text/javascript">
			/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
			var disqus_shortname = 'pi19404'; // required: replace example with your forum shortname
    			var disqus_identifier = '/2018/01/30/lstm3/';
    			var disqus_url = 'http://pyvision.github.com/2018/01/30/lstm3/';       
 
			/* * * DON'T EDIT BELOW THIS LINE * * */
			(function() {
				var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
				dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
				(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
			})();
		</script>
		<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
		<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>





      </div>
    </div>

      
       <footer>
          pyVision is maintained by <a href="https://github.com/pi19404">pi19404</a><br>
        </footer>
    
    </div>
 <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = true;
        }, false);
      })(document);
    </script>
<script type="text/javascript">
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script>
    <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'pyvision'; // required: replace example with your forum shortname
    var disqus_identifier = '/2018/01/30/lstm3/';
    var disqus_url = 'http://pyvision.github.com/2018/01/30/lstm3/';
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
    </script>

  </body>
</html>
