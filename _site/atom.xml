<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>pyVision</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2022-05-07T16:45:36+00:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>pi19404</name>
   <email>pi19404@gmail.com</email>
 </author>

 
 <entry>
   <title>GitPod Self Host Installation on K3S</title>
   <link href="http://localhost:4000/linux/2022/05/07/gitpod1/"/>
   <updated>2022-05-07T00:00:00+00:00</updated>
   <id>http://localhost:4000/linux/2022/05/07/gitpod1</id>
   <content type="html">&lt;h1 id=&quot;gitpod-installation&quot;&gt;GitPod Installation&lt;/h1&gt;

&lt;h3 id=&quot;step-1-create-a-kubernetes-cluster-&quot;&gt;Step 1: Create a Kubernetes Cluster &lt;a href=&quot;#step-1-create-a-kubernetes-cluster&quot; id=&quot;step-1-create-a-kubernetes-cluster&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Gitpod is a Kubernetes application that uses Kubernetes internally to provision workspaces as Kubernetes pods. Before you can start installing Gitpod, you need to create a compatible Kubernetes cluster. &lt;/p&gt;

&lt;h1 id=&quot;install-k3s-kubernets-cluster&quot;&gt;Install K3S Kubernets Cluster&lt;/h1&gt;

&lt;h2 id=&quot;cluster-set-up-&quot;&gt;Cluster Set-Up &lt;a href=&quot;#cluster-set-up&quot; id=&quot;cluster-set-up&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Gitpod is a Kubernetes application running with certain expectations on the characteristics of the cluster it is running on.&lt;/p&gt;

&lt;p&gt;Gitpod requires Kubernetes as an orchestration technology in order to spin up and down workspaces—ideally in combination with cluster autoscaling to minimise cost. We strongly recommend deploying a dedicated Kubernetes cluster just for Gitpod Self-Hosted.&lt;/p&gt;

&lt;p&gt;In this article we will use k8s to setup a self managed kubernetes cluster&lt;/p&gt;

&lt;p&gt;K3s is a highly available, certified Kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances.&lt;/p&gt;

&lt;p&gt;On each node, we &lt;a href=&quot;https://rancher.com/docs/k3s/latest/en/installation/&quot;&gt;install K3s&lt;/a&gt;. We configure K3s by setting the following environment variables on the nodes.&lt;/p&gt;

&lt;p&gt;K3s config for main node &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;node0&lt;/code&gt;: &lt;/p&gt;

&lt;p&gt;The below configure is for a single node setup with only master node&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;INSTALL_K3S_EXEC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;server --disable traefik --flannel-backend=none --node-label gitpod.io/workload_meta=true --node-label gitpod.io/workload_ide=true -node-label gitpod.io/workload_workspace_services=true --node-label gitpod.io/workload_workspace_regular=true --node-label gitpod.io/workload_workspace_headless=true&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;K3S_CLUSTER_SECRET&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;44dc0c2d471f50bc151aa72515d53067&quot;&lt;/span&gt;
curl &lt;span class=&quot;nt&quot;&gt;-sfL&lt;/span&gt; https://get.k3s.io | sh -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After setting the environment variables, install K3s on every node like this:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-sfL&lt;/span&gt; https://get.k3s.io | sh -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Check for Ready node,
takes maybe 30 seconds
k3s kubectl get node
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can run the below command to start the server &lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl start k3s.service
# Kubeconfig is written to /etc/rancher/k3s/k3s.yaml
k3s kubectl get node
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To enable any other node to join the cluster run the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# On a different node run the below. NODE_TOKEN comes from /var/lib/rancher/k3s/server/node-token
# on your server
export INSTALL_K3S_EXEC=&quot;agent --node-label gitpod.io/workload_workspace_services=true --node-label gitpod.io/workload_workspace_regular=true --node-label gitpod.io/workload_workspace_headless=true&quot;
export K3S_CLUSTER_SECRET=&quot;&amp;lt;your random secret string that is the same on all nodes&amp;gt;&quot;
export K3S_URL=&quot;https://node0:6443&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, you have to install &lt;a href=&quot;https://www.tigera.io/project-calico/&quot;&gt;Calico&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method 1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Download the &lt;a href=&quot;https://docs.projectcalico.org/manifests/calico-vxlan.yaml&quot;&gt;Calico manifest&lt;/a&gt; and add the following line to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;plugins&lt;/code&gt; section of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cni_network_config&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nl&quot;&gt;&quot;container_settings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;allow_ip_forwarding&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/var/lib/rancher/k3s/server/manifests/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Copy that file to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;node0&lt;/code&gt; in the following folder (create folder if missing):&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method 2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Install the Calico operator and custom resource definitions.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f https://projectcalico.docs.tigera.io/manifests/tigera-operator.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Install Calico by creating the necessary custom resource. For more information on configuration options available in this manifest, see &lt;a href=&quot;https://projectcalico.docs.tigera.io/reference/installation/api&quot;&gt;the installation reference&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Before creating this manifest, read its contents and make sure its settings are correct for your environment. For example, you may need to change the default IP pool CIDR to match your pod network CIDR.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Final checks&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Confirm that all of the pods are running using the following command.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;watch kubectl get pods --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Wait until each pod shows the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STATUS&lt;/code&gt; of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Running&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../.gitbook/assets/image (7) (1).png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Confirm that you now have a node in your cluster with the following command.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get nodes -o wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../../.gitbook/assets/image (8).png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
References&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gitpod.io/docs/self-hosted/latest/cluster-set-up/on-k3s&quot;&gt;https://www.gitpod.io/docs/self-hosted/latest/cluster-set-up/on-k3s&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gitpod.io/docs/self-hosted/latest/getting-started#step-2-install-cert-manager&quot;&gt;https://www.gitpod.io/docs/self-hosted/latest/getting-started#step-2-install-cert-manager&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://projectcalico.docs.tigera.io/getting-started/kubernetes/k3s/quickstart&quot;&gt;https://projectcalico.docs.tigera.io/getting-started/kubernetes/k3s/quickstart&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;step-2-install-cert-manager-&quot;&gt;Step 2: Install Cert-Manager &lt;a href=&quot;#step-2-install-cert-manager&quot; id=&quot;step-2-install-cert-manager&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Once you have created your Kubernetes cluster you need to install &lt;a href=&quot;https://cert-manager.io&quot;&gt;cert-manager&lt;/a&gt;. cert-manager is needed in any case even when you bring your own TLS certificate for your domain. &lt;/p&gt;

&lt;h1 id=&quot;install-cert-manager-on-kubernetes-cluster&quot;&gt;Install cert-manager on kubernetes cluster&lt;/h1&gt;

&lt;p&gt;cert-manager is a Kubernetes add-on to automate the management and issuance of TLS certificates from various issuing sources.&lt;/p&gt;

&lt;p&gt;It will ensure certificates are valid and up to date periodically, and attempt to renew certificates at an appropriate time before expiry.&lt;/p&gt;

&lt;p&gt;on the same node where k3s master was installed run the below command to install the cert manager&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;k3s kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.0/cert-manager.yaml
    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By default, cert-manager will be installed into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cert-manager&lt;/code&gt; namespace&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Creating TLS certs for your domain with cert-manager&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;cert-manager provides the Gitpod installation with certificates for internal communication. Besides this, cert-manager can also create a TLS certificate for your domain. Since Gitpod needs wildcard certificates, you must use the &lt;a href=&quot;https://letsencrypt.org/docs/challenge-types/#dns-01-challenge&quot;&gt;DNS-01 challenge&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can use either an &lt;a href=&quot;https://cert-manager.io/docs/concepts/issuer&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Issuer&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterIssuer&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Issuers&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ClusterIssuers&lt;/code&gt;, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honoring certificate signing requests. All cert-manager certificates require a referenced issuer that is in a ready condition to attempt to honor the request.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;we will use the domain&lt;/strong&gt; miko-robot.co.in , DNS provide is onlydomains . We will first delegate the domain to cloudflare and then use cloudflare for DNS01 challenge &lt;/p&gt;

&lt;p&gt;Create and Account and Login into cloudflare . Click on Add a Site option to start the domain delegation process&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../.gitbook/assets/image (2).png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For now choose the Free option and continue&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../.gitbook/assets/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Log in&lt;/strong&gt; to the &lt;strong&gt;administrator account&lt;/strong&gt; for your domain registrar . In this case domain registrar is onlydomain.com . &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;By Default&lt;/strong&gt; the following nameservers are configured&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ns2.onlydomains.com , ns3.onlydomains.com , ns1.onlydomains.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Go to DNS Settings Menu&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Choose the option to delegate name server Add&lt;/strong&gt; Cloudflare’s nameservers&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tegan.ns.cloudflare.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zod.ns.cloudflare.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../../.gitbook/assets/image (4).png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This will update the DNS Settings and allow the DNS to be managed via cloudflare&lt;/p&gt;

&lt;h3 id=&quot;create-cloudflare-api-tokens-&quot;&gt;Create CloudFlare API Tokens &lt;a href=&quot;#api-tokens&quot; id=&quot;api-tokens&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Tokens can be created at &lt;strong&gt;User Profile &amp;gt; API Tokens &amp;gt; API Tokens&lt;/strong&gt;. The following settings are recommended:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Permissions:
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Zone - DNS - Edit&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Zone - Zone - Read&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Zone Resources:
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Include - All Zones&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../.gitbook/assets/image (5).png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Copy The token and save it as it will not be displayed again for security purposes&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Verify that the token is working&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -X GET &quot;https://api.cloudflare.com/client/v4/user/tokens/verify&quot;
-H &quot;Authorization: Bearer {TOKEN}&quot;
-H &quot;Content-Type:application/json&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If token is working then you will see a output similar to one below&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{&quot;result&quot;:{&quot;id&quot;:&quot;cf4a06f05d43d58468667ba715145c34&quot;,&quot;status&quot;:&quot;active&quot;},&quot;success&quot;:true,&quot;errors&quot;:[],&quot;messages&quot;:[{&quot;code&quot;:10000,&quot;message&quot;:&quot;This API Token is valid and active&quot;,&quot;type&quot;:null}]}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create a new Issuer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To create a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Issuer&lt;/code&gt;, first make a Kubernetes secret containing your new API token:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Secret
metadata:
 name: cloudflare-api-token-secret
 namespace: cert-manager
type: Opaque
stringData:
 api-token: {TOKEN}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Please note that the namespace has to be cert-manager for the key else you may encounter error while creating the certificates&lt;/p&gt;

&lt;p&gt;To apply the configuration&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;k3s kubectl apply -f cloudflare_token.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create Issuer configuration file&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: ClusterIssuer
metadata:
  name: gitpod-issuer
spec:
  acme:
    email: prashant@miko.ai
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: gitpod-issuer
    solvers:
    - dns01:
        cloudflare:
          email : aaa@aaa.com
          apiTokenSecretRef:
            name: cloudflare-api-token-secret
            key: api-token
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you complete the gitpod installation create the below certificates in gitpod and kube-system workspace both&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
 name: https-certificates
 namespace : kube-system
spec:
 secretName: https-certificates
 issuerRef:
   name: gitpod-issuer
   kind: ClusterIssuer
 dnsNames:
  - gitpod.miko-robot.co.in
  - &quot;*.gitpod.miko-robot.co.in&quot;
  - &quot;*.ws.gitpod.miko-robot.co.in&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
 name: https-certificates
 namespace : gitpod
spec:
 secretName: https-certificates
 issuerRef:
   name: gitpod-issuer
   kind: ClusterIssuer
 dnsNames:
  - gitpod.miko-robot.co.in
  - &quot;*.gitpod.miko-robot.co.in&quot;
  - &quot;*.ws.gitpod.miko-robot.co.in&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To apply the configuration&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;k3s kubectl apply -f cert.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;While certificate issuance process is in progress you will see the status as False&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get certificate
NAME                        READY   SECRET                      AGE
https-certificates          False    https-certificates          5m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After a few minutes, you should see the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https-certificate&lt;/code&gt; become ready.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get certificate
NAME                        READY   SECRET                      AGE
https-certificates          True    https-certificates          5m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the DNS record has been updated, you can delete all Cert Manager pods to retrigger the certificate request&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl delete pods -n cert-manager --all
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cert-manager.io/docs/configuration/acme/dns01/cloudflare/&quot;&gt;https://cert-manager.io/docs/configuration/acme/dns01/cloudflare/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-3-configure-dns-&quot;&gt;Step 3: Configure DNS &lt;a href=&quot;#step-3-configure-dns&quot; id=&quot;step-3-configure-dns&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;To install Gitpod you need a domain with a TLS certificate. The DNS setup to your domain needs to be configured such that it points to the ingress of your Kubernetes cluster.&lt;/p&gt;

&lt;h1 id=&quot;configure-dns&quot;&gt;Configure DNS&lt;/h1&gt;

&lt;p&gt;To install Gitpod you need a domain with a TLS certificate. The DNS setup to your domain needs to be configured such that it points to the ingress of your Kubernetes cluster. You need to configure your actual domain (say &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;example.com&lt;/code&gt;) as well as the wildcard subdomains &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*.example.com&lt;/code&gt; as well as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*.ws.example.com&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;step-4-install-gitpod-&quot;&gt;Step 4: Install Gitpod &lt;a href=&quot;#step-4-install-gitpod&quot; id=&quot;step-4-install-gitpod&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;h1 id=&quot;gitpod-installation-1&quot;&gt;GitPod Installation&lt;/h1&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;To start with installing Gitpod, you need a terminal where you can run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; to access your cluster. At first, install the KOTS kubectl plugicurl https://kots.io/install&lt;/td&gt;
      &lt;td&gt;bash&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl kots &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;gitpod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You will be asked for the namespace you want to install Gitpod to as well as a password for the admin console. After some time, you will see the following output:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  • Press Ctrl+C to exit
  • Go to http://localhost:8800 to access the Admin Console
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To access the Admin Console again, run kubectl kots admin-console –namespace gitpod&lt;/p&gt;

&lt;p&gt;Open your favorite browser and go to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:8800&lt;/code&gt; (port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;8800&lt;/code&gt; is opened on your node on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost&lt;/code&gt; only—you may want to forward the port to your workstation in order to access the admin console).&lt;/p&gt;

&lt;p&gt;we will use ngrok for port forwarding&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ngrok http 8800
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../../.gitbook/assets/image (6) (1).png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first page will ask you to upload your Gitpod license. Gitpod provides community license for free (right click and save link as &lt;a href=&quot;https://raw.githubusercontent.com/gitpod-io/gitpod/main/install/licenses/Community.yaml&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../.gitbook/assets/image (1).png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Enter the domain name as :gitpod.miko-robot.co.in&lt;/p&gt;

&lt;p&gt;User all the default settings for incluster container registry , mysql database , storage provide&lt;/p&gt;

&lt;p&gt;User cert-manager for SSL certificates and use Issuer type as “Issuer”&lt;/p&gt;

&lt;p&gt;Mark Allow login by into workspace via ssh&lt;/p&gt;

&lt;p&gt;Check if all the pods are running states&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../.gitbook/assets/image (7).png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl -n gitpod patch svc proxy -p '{&quot;spec&quot;: {&quot;type&quot;: &quot;LoadBalancer&quot;, &quot;externalIPs&quot;:[&quot;xxx.xxx.xxx.xx&quot;]}}'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the installation has been finished successfully, you will see the status “Ready” with a small green indicator next to the Gitpod logo. You see which version you installed and which license you are using.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../.gitbook/assets/image (6).png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Install ngrok on Ubuntu 20.04</title>
   <link href="http://localhost:4000/linux/2022/05/05/ngrok/"/>
   <updated>2022-05-05T00:00:00+00:00</updated>
   <id>http://localhost:4000/linux/2022/05/05/ngrok</id>
   <content type="html">&lt;h1 id=&quot;install-ngrok&quot;&gt;Install ngrok&lt;/h1&gt;

&lt;p&gt;create an account and login in to ngrok&lt;/p&gt;

&lt;p&gt;Download the linux package&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz --no-check-certificate
tar -zxvf ngrok-v3-stable-linux-amd64.tgz
mv ngrok /usr/bin/ngrok
chmod 755 /usr/bin/ngrok
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running this command will add your authtoken to the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ngrok.yml&lt;/code&gt; configuration file. This will grant you access to more features and longer session times. Running tunnels will be listed on the &lt;a href=&quot;https://dashboard.ngrok.com/cloud-edge/endpoints&quot;&gt;endpoints page&lt;/a&gt; of the dashboard.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ngrok config add-authtoken {TOKEN}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;o start a HTTP tunnel forwarding to your local port 80, run this next:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ngrok http 8800
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Lets Crypt SSL Certificate Installation on Ubuntu</title>
   <link href="http://localhost:4000/linux/2022/04/24/letscrypt1/"/>
   <updated>2022-04-24T00:00:00+00:00</updated>
   <id>http://localhost:4000/linux/2022/04/24/letscrypt1</id>
   <content type="html">&lt;h1 id=&quot;lets-crypt-ssl-certificate-installation-on-ubuntu&quot;&gt;Lets Crypt SSL Certificate Installation on Ubuntu&lt;/h1&gt;

&lt;p&gt;The first step to using Let’s Encrypt to obtain an SSL certificate is to install the Certbot software on your server.&lt;/p&gt;

&lt;p&gt;Certbot is in very active development, so the Certbot packages provided by Ubuntu tend to be outdated. However, the Certbot developers maintain a Ubuntu software repository with up-to-date versions, so we’ll use that repository instead.&lt;/p&gt;

&lt;p&gt;First, add the repository.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:certbot/certbot
sudo apt-get update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You’ll need to press &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENTER&lt;/code&gt; to accept. Then, update the package list to pick up the new repository’s package information.&lt;/p&gt;

&lt;p&gt;And finally, install Certbot’s Nginx package with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt-get&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install python-certbot-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Certbot is now ready to use, but in order for it to configure SSL for Nginx, we need to verify some of Nginx’s configuration.&lt;/p&gt;

&lt;p&gt;Certbot can automatically configure SSL for Nginx, but it needs to be able to find the correct &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;server&lt;/code&gt; block in your config. It does this by looking for a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;server_name&lt;/code&gt; directive that matches the domain you’re requesting a certificate for.&lt;/p&gt;

&lt;p&gt;If you’re starting out with a fresh Nginx install, you can update the default config file. Open it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nano&lt;/code&gt; or your favorite text editor.&lt;/p&gt;

&lt;p&gt;Find the existing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;server_name&lt;/code&gt; line and replace the underscore, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_&lt;/code&gt;, with your domain name: /etc/nginx/sites-available/default&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;. . .server_name build.miko2.ai;. . .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Save the file and quit your editor.&lt;/p&gt;

&lt;p&gt;Then, verify the syntax of your configuration edits.&lt;/p&gt;

&lt;p&gt;If you get any errors, reopen the file and check for typos, then test it again.&lt;/p&gt;

&lt;h3 id=&quot;obtaining-an-ssl-certificate-with-automatic-verification-&quot;&gt;Obtaining an SSL Certificate with automatic verification &lt;a href=&quot;#bkmrk-obtaining-an-ssl-cer&quot; id=&quot;bkmrk-obtaining-an-ssl-cer&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Certbot provides a variety of ways to obtain SSL certificates, through various plugins. The Nginx plugin will take care of reconfiguring Nginx and reloading the config whenever necessary:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo certbot --nginx -d build.miko2.ai
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This runs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;certbot&lt;/code&gt; with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--nginx&lt;/code&gt; plugin, using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-d&lt;/code&gt; to specify the names we’d like the certificate to be valid for.&lt;/p&gt;

&lt;p&gt;If this is your first time running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;certbot&lt;/code&gt;, you will be prompted to enter an email address and agree to the terms of service. After doing so, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;certbot&lt;/code&gt; will communicate with the Let’s Encrypt server, then run a challenge to verify that you control the domain you’re requesting a certificate for.&lt;/p&gt;

&lt;p&gt;If that’s successful, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;certbot&lt;/code&gt; will ask how you’d like to configure your HTTPS settings.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Please choose whether or not to redirect HTTP traffic to HTTPS, removing HTTP access.
-------------------------------------------------------------------------------
1: No redirect - Make no further changes to the webserver configuration.
2: Redirect - Make all requests redirect to secure HTTPS access. Choose this for
new sites, or if you're confident your site works on HTTPS. You can undo this
change by editing your web server's configuration.
-------------------------------------------------------------------------------
Select the appropriate number [1-2] then [enter] (press 'c' to cancel):
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Select your choice then hit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENTER&lt;/code&gt;. The configuration will be updated, and Nginx will reload to pick up the new settings. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;certbot&lt;/code&gt; will wrap up with a message telling you the process was successful and where your certificates are stored:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;IMPORTANT NOTES:
 - Congratulations! Your certificate and chain have been saved at
   /etc/letsencrypt/live/example.com/fullchain.pem. Your cert will
   expire on 2017-10-23. To obtain a new or tweaked version of this
   certificate in the future, simply run certbot again with the
   &quot;certonly&quot; option. To non-interactively renew *all* of your
   certificates, run &quot;certbot renew&quot;
 - Your account credentials have been saved in your Certbot
   configuration directory at /etc/letsencrypt. You should make a
   secure backup of this folder now. This configuration directory will
   also contain certificates and private keys obtained by Certbot so
   making regular backups of this folder is ideal.
 - If you like Certbot, please consider supporting our work by:

   Donating to ISRG / Let's Encrypt:   https://letsencrypt.org/donate
   Donating to EFF:                    https://eff.org/donate-le
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Your certificates are downloaded, installed, and loaded&lt;/p&gt;

&lt;h3 id=&quot;obtaining--ssl-certificate-from-lets-encrypt-with-manual-verification-&quot;&gt;Obtaining  ssl certificate from Let’s Encrypt with manual verification &lt;a href=&quot;#bkmrk-obtaining-wildcard-s&quot; id=&quot;bkmrk-obtaining-wildcard-s&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;sudo certbot –server &lt;a href=&quot;https://acme-v02.api.letsencrypt.org/directory&quot;&gt;https://acme-v02.api.letsencrypt.org/directory&lt;/a&gt; -d &lt;strong&gt;example.com&lt;/strong&gt; –manual –preferred-challenges dns-01 certonly&lt;/p&gt;

&lt;p&gt;Note:- Replace &lt;strong&gt;example.com&lt;/strong&gt; with your domain name&lt;/p&gt;

&lt;p&gt;Deploy a &lt;strong&gt;DNS TXT&lt;/strong&gt; record provided by Let’s Encrypt certbot after running the above command&lt;/p&gt;

&lt;p&gt;REFERENCES&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-16-04&quot;&gt;https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-16-04&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Install LetsCrypt SSL Wildcard Certificate on Ubuntu</title>
   <link href="http://localhost:4000/linux/2022/04/24/install-letscrypt-ssl-wildcard-certificate-on-ubuntu/"/>
   <updated>2022-04-24T00:00:00+00:00</updated>
   <id>http://localhost:4000/linux/2022/04/24/install-letscrypt-ssl-wildcard-certificate-on-ubuntu</id>
   <content type="html">&lt;h1 id=&quot;install-letscrypt-ssl-wildcard-certificate-on-ubuntu&quot;&gt;Install LetsCrypt SSL Wildcard Certificate on Ubuntu&lt;/h1&gt;

&lt;h4 id=&quot;installing-certbot-&quot;&gt;&lt;strong&gt;INSTALLING CERTBOT&lt;/strong&gt; &lt;a href=&quot;#bkmrk-installing-certbot&quot; id=&quot;bkmrk-installing-certbot&quot;&gt;&lt;/a&gt;&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:certbot/certbotsudo apt-get updatesudo apt-get install python-certbot-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;installing-nginx-&quot;&gt;&lt;strong&gt;INSTALLING NGINX&lt;/strong&gt; &lt;a href=&quot;#bkmrk-installing-nginx&quot; id=&quot;bkmrk-installing-nginx&quot;&gt;&lt;/a&gt;&lt;/h4&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;setup-dns-to-serve-all-the-subdomains-&quot;&gt;Setup DNS to serve all the subdomains &lt;a href=&quot;#bkmrk-setup-dns-to-serve-a&quot; id=&quot;bkmrk-setup-dns-to-serve-a&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Create a custom &lt;strong&gt;A&lt;/strong&gt; record, HOST &lt;strong&gt;*&lt;/strong&gt; POINTS TO: Your IP Address(Eg: 103.21.0.108)&lt;/li&gt;
  &lt;li&gt;Create a custom &lt;strong&gt;A&lt;/strong&gt; record, HOST &lt;strong&gt;@&lt;/strong&gt; POINTS TO: Your IP Address(Eg: 103.21.0.108)&lt;/li&gt;
  &lt;li&gt;Add a CNAME record, HOST &lt;strong&gt;www&lt;/strong&gt; POINTS TO &lt;strong&gt;@&lt;/strong&gt; this refers to your IP address.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;obtaining-wildcard-ssl-certificate-from-lets-encrypt-&quot;&gt;Obtaining wildcard ssl certificate from Let’s Encrypt &lt;a href=&quot;#bkmrk-obtaining-wildcard-s&quot; id=&quot;bkmrk-obtaining-wildcard-s&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;sudo certbot –server &lt;a href=&quot;https://acme-v02.api.letsencrypt.org/directory&quot;&gt;https://acme-v02.api.letsencrypt.org/directory&lt;/a&gt; -d *.&lt;strong&gt;example.com&lt;/strong&gt; –manual –preferred-challenges dns-01 certonly&lt;/p&gt;

&lt;p&gt;Note:- Replace &lt;strong&gt;example.com&lt;/strong&gt; with your domain name&lt;/p&gt;

&lt;p&gt;Deploy a &lt;strong&gt;DNS TXT&lt;/strong&gt; record provided by Let’s Encrypt certbot after running the above command&lt;/p&gt;

&lt;h3 id=&quot;configuring-nginx-to-serve-wildcard-subdomains-&quot;&gt;Configuring Nginx to serve wildcard subdomains &lt;a href=&quot;#bkmrk-configuring-nginx-to&quot; id=&quot;bkmrk-configuring-nginx-to&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Create a config file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo touch /etc/nginx/sites-available/example.com&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Open the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo vi /etc/nginx/sites-available/example.com&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Add the following code in the file&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;server {
  listen 80;
  listen [::]:80;
  server_name *.example.com;
  return 301 https://$host$request_uri;
}
server {
  listen 443 ssl;
  server_name *.example.com;
  ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem;
  ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem;
  include /etc/letsencrypt/options-ssl-nginx.conf;
  ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem;
  root /var/www/example.com;
  index index.html;
  location / {
    try_files $uri $uri/ =404;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;test-and-restart-nginx-&quot;&gt;Test and restart Nginx &lt;a href=&quot;#bkmrk-test-and-restart-ngi&quot; id=&quot;bkmrk-test-and-restart-ngi&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Test Nginx configuration using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo nginx -t&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;If it’s success reload Nginx using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo /etc/init.d/nginx reload&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nginx is now setup to handle wildcard subdomains.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;REFERENCES&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/@utkarsh\_verma/how-to-obtain-a-wildcard-ssl-certificate-from-lets-encrypt-and-setup-nginx-to-use-wildcard-cfb050c8b33f&quot;&gt;https://medium.com/@utkarsh_verma/how-to-obtain-a-wildcard-ssl-certificate-from-lets-encrypt-and-setup-nginx-to-use-wildcard-cfb050c8b33f&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</content>
 </entry>
 
 <entry>
   <title>patchelf Installation and usage</title>
   <link href="http://localhost:4000/linux/2022/04/14/patchelf/"/>
   <updated>2022-04-14T00:00:00+00:00</updated>
   <id>http://localhost:4000/linux/2022/04/14/patchelf</id>
   <content type="html">&lt;p&gt;Patch Elf is a A small utility to modify the dynamic linker and RPATH of ELF executables&lt;/p&gt;

&lt;p&gt;To build patchelf&lt;/p&gt;

&lt;h2 id=&quot;close-the-repoistory&quot;&gt;Close the repoistory&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/miko-ai/patchelf.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;pre-requisites&quot;&gt;Pre Requisites&lt;/h2&gt;
&lt;p&gt;Install gcc version 7&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt install software-properties-common
sudo add-apt-repository ppa:ubuntu-toolchain-r/test
sudo apt-get update
sudo apt-get install gcc-7 g++-7
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 70 --slave /usr/bin/g++ g++ /usr/bin/g++-7 --slave /usr/bin/gcov gcov /usr/bin/gcov-7
sudo apt-get install autoconf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;build&quot;&gt;Build&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
cd patchelf
./bootstrap.sh
./configure
make
make check
sudo make install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;patchelf-usage&quot;&gt;patchelf usage&lt;/h2&gt;

&lt;p&gt;lets assume that application binary app users glibc version 2.22 and you need to configure it to use glibc 2.25 without recompiling the binary&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;patchelf --set-interpreter /opt/glibc-2.25/lib/ld-linux-x86-64.so.2 --set-rpath /opt/glibc-2.25/lib:/lib/x86_64-linux-gnu/ app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://github.com/sgerrand/docker-glibc-builder/issues/9&lt;/li&gt;
  &lt;li&gt;https://linuxize.com/post/how-to-install-gcc-compiler-on-ubuntu-18-04/&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>GLib 2.25 Installation on Ubuntu 16.04</title>
   <link href="http://localhost:4000/linux/2022/04/14/glibc/"/>
   <updated>2022-04-14T00:00:00+00:00</updated>
   <id>http://localhost:4000/linux/2022/04/14/glibc</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this article we will look at installation of Glib 2.25 installation on Ubuntu 16.04&lt;/p&gt;

&lt;h2 id=&quot;pre-requisite-installation&quot;&gt;Pre Requisite installation&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get install autoconf&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;installation-steps&quot;&gt;Installation Steps&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir ~/glibc_install; cd ~/glibc_install

wget http://ftp.gnu.org/gnu/glibc/glibc-2.25.tar.gz

tar zxvf glibc-2.25.tar.gz

cd glibc-2.25

mkdir build

cd build

../configure --prefix=/opt/glibc-2.25

make -j4

sudo make install

export LD_LIBRARY_PATH=/opt/glibc-2.25/lib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If OS does not support the glibc version for example 2.25 that you require . Then installing glibc in default prefix may cause the system to become unusable . This is because  glibc consists of many pieces (200+ shared libraries) which all must match. One of the pieces is ld-linux.so.2, and it must match libc.so.6, If all libraries do not match it would actually cause the system to be unusable as glibc is a very critical library . Hence its preferable to install glibc in alternate prefix&lt;/p&gt;

&lt;p&gt;Errors&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ld.so.conf errors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkdir -p /opt/glibc-2.25/etc ;  touch /opt/glibc-2.25/etc/ld.so.conf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;echo these 2 lines in the .conf file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/usr/local/lib
/opt/lib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://github.com/sgerrand/docker-glibc-builder/issues/9&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Natural Language Understanding - Intent Detection with Keras and LSTM</title>
   <link href="http://localhost:4000/2018/01/31/lstm4/"/>
   <updated>2018-01-31T00:00:00+00:00</updated>
   <id>http://localhost:4000/2018/01/31/lstm4</id>
   <content type="html">&lt;p&gt;In this example we will look at the problem of natural language understanding.The aim is simply to understand the meaning of sentense. This is a unsolved problem in the context of open domain sentense understaing ie there exists no system that can understand the meaning of any open domain sentense.However many of the techniques that work well do so within a specific domain.&lt;/p&gt;

&lt;p&gt;An intent is a group of utterances with similar meaning&lt;/p&gt;

&lt;p&gt;Airline Travel Information System (ATIS) dataset is a standard data set used to demonstrate
and benchmark various NLU algorithms.ATIS consists of spoken queries on flight related information&lt;/p&gt;

&lt;p&gt;we will use modified version of the ATIS dataset used in the project &lt;a href=&quot;https://github.com/yvchen/JointSLU/tree/master/data&quot;&gt;https://github.com/yvchen/JointSLU&lt;/a&gt; . This database contains spoken queries,associated intents and entities in the statements.&lt;/p&gt;

&lt;p&gt;The dataset consists of sentenses labelled/encoded in Inside Outside Beginning (IOB) representation.&lt;/p&gt;

&lt;p&gt;An example utterance is&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Show&lt;/td&gt;
      &lt;td&gt;flights&lt;/td&gt;
      &lt;td&gt;from&lt;/td&gt;
      &lt;td&gt;Boston&lt;/td&gt;
      &lt;td&gt;to&lt;/td&gt;
      &lt;td&gt;New&lt;/td&gt;
      &lt;td&gt;York&lt;/td&gt;
      &lt;td&gt;today&lt;/td&gt;
      &lt;td&gt;EOS&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;O&lt;/td&gt;
      &lt;td&gt;O&lt;/td&gt;
      &lt;td&gt;O&lt;/td&gt;
      &lt;td&gt;B-dept&lt;/td&gt;
      &lt;td&gt;O&lt;/td&gt;
      &lt;td&gt;B-arr&lt;/td&gt;
      &lt;td&gt;I-arr&lt;/td&gt;
      &lt;td&gt;B-date&lt;/td&gt;
      &lt;td&gt;atis_flight&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The aim would be to first understand the intent of the statement,which in the above examples is to show flight information (atis_flight ) .The second operation is to understand that source city is boston,departure city is new york, and date and time is specified by today.Second operation is called as slot filling .&lt;/p&gt;

&lt;p&gt;First task is to read the ATIS dataset&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def processCSVData(filename):
    output=[]
    lables=[]
    with open(filename, 'r') as f:
        for rowx in f:
            r={}
            words = rowx.split()
            intent_name=words[len( words)-1]
            words=words[0:len( words)-1]
            aa=&quot;&quot;;
            cc=&quot;&quot;;
            state=-1
            for w in words:
                if w==&quot;EOS&quot;:
                    state=1;

                if state==0:
                    aa=aa+&quot; &quot;+w;
                if state==1:
                    cc=cc+&quot; &quot;+w
                if w==&quot;BOS&quot;:
                    state=0

            r['text']=aa
            r['tags'] =cc
            r['intent']=intent_name
            lables.append(intent_name)
            output.append(r)
    lables=np.unique(lables)
    return output,lables

output,lables=processCSVData(&quot;atis.txt&quot;)
#print output[0],lables
print &quot;number of samples&quot;,len(output)
print &quot;number of intent&quot;,len(lables)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The ATIS official split contains 4,978/893 sentences and intent 22 classes.&lt;/p&gt;

&lt;p&gt;Our first task would be to just perform intent detection. The input to the classifier is a sequence
of words and output is the intent associated with the statement.&lt;/p&gt;

&lt;p&gt;we will use LSTM for intent classification&lt;/p&gt;

&lt;p&gt;As discussed in the article we will be using approach of sequence classification in LSTM using pre trained word vector embeddings.&lt;/p&gt;

&lt;p&gt;Below is the code that reads the glove2vec vector embeddings and creates a neural network embedding layer
which uses the pre trained embedding vectors as initializations.The output of the function is a tokenizer that converts words to integers representing dictionary entires as well as embedding layer that can be used as first layer of a sequential model in NLP pipeline&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def getEmbeddingLayer(EMBEDDING_DIM):
    embeddings_index = {}
    f = open(os.path.join('data/', 'glove.6B.100d.txt'))
    count=0
    words=[]
    for line in f:
        values = line.split()
        word = values[0]
        words.append(word)
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
        count=count+1;
    f.close()

    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
    tokenizer.fit_on_texts(words)
    word_index = tokenizer.word_index

    print &quot;total words embeddings is &quot;,count,len(word_index)
    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            embedding_matrix[i] = embedding_vector


    embedding_layer = Embedding(input_dim=len(word_index) + 1,
                                output_dim=EMBEDDING_DIM,
                                weights=[embedding_matrix],
                                input_length=MAX_SEQUENCE_LENGTH,
                                trainable=True)

    return tokenizer,embedding_layer
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;below is the code to create embedding layer&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def create_embedded_model(embedding_layer,num_classes,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM):
    model = Sequential()
    model.add(embedding_layer)
    model.add(LSTM(128, return_sequences=False
                   , input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)))

    model.add(Dense(num_classes, activation='softmax'))

    print(model.summary())
    return model;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Training the same over the ATIS dataset with 20 epochs and batch size of 100 gives an accuracy of 99% .
This data set is too small to attain ant kind of benchmark and it would clearly be overfitting on the data
since dataset is very small for DNN applications .&lt;/p&gt;

&lt;p&gt;This article mearly demonstrates how LSTM networks can be used for intent detection task.&lt;/p&gt;

&lt;p&gt;The complete code can be found at &lt;a href=&quot;https://gist.github.com/pi19404/4a054e0ef1f0dc2fbd3661fb8be00d37&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;References&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://chsasank.github.io/spoken-language-understanding.html&quot;&gt;Keras Tutorial - Spoken Language Understanding&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;http://mrbot.ai/blog/natural-language-processing/understanding-intent-classification/&lt;/li&gt;
  &lt;li&gt;https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html&lt;/li&gt;
  &lt;li&gt;https://docs.google.com/viewer?url=http://ccc.inaoep.mx/~villasen/bib/dmello-roman07.pdf&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Using Pre Trained Word Vector Embeddings for Sequence Classification using LSTM</title>
   <link href="http://localhost:4000/2018/01/30/lstm3/"/>
   <updated>2018-01-30T00:00:00+00:00</updated>
   <id>http://localhost:4000/2018/01/30/lstm3</id>
   <content type="html">&lt;p&gt;In this article we will look at using pre trained word vector embedding for sequence classification
using LSTM&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using Pre-Trained Word Vector Embeddings&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the article &lt;a href=&quot;http://pi19404.github.io/pyVision/2017/05/13/spacy1/&quot;&gt;NLP spaCy Word and document vectors&lt;/a&gt; we saw how to get the word vector representation trained on common crawl corpus provided by spacy toolkit. We will use this pretrained word vector representation rather than training our own Embedding Layer&lt;/p&gt;

&lt;p&gt;We cannot use the dataset provided by keras directly so we download the raw database&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We will use the following script to extract the data&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#this function will convert text to lowercase and will disconnect punctuation and special symbols from words
function normalize_text {
  awk '{print tolower($0);}' &amp;lt; $1 | sed -e 's/\./ \. /g' -e 's/&amp;lt;br \/&amp;gt;/ /g' -e 's/&quot;/ &quot; /g' \
  -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\!/ \! /g' -e 's/\?/ \? /g' \
  -e 's/\;/ \; /g' -e 's/\:/ \: /g' &amp;gt; $1-norm
}

cd ..
mkdir nbsvm_run; cd nbsvm_run

wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
tar -xvf aclImdb_v1.tar.gz
rm aclImdb_v1.tar.gz

for j in train/pos train/neg test/pos test/neg; do
  for i in `ls aclImdb/$j`; do cat aclImdb/$j/$i &amp;gt;&amp;gt; temp; awk 'BEGIN{print;}' &amp;gt;&amp;gt; temp; done
  normalize_text temp
  mv temp-norm aclImdb/$j/norm.txt
  rm temp
done

mkdir data
mv aclImdb/train/pos/norm.txt data/train-pos.txt
mv aclImdb/train/neg/norm.txt data/train-neg.txt
mv aclImdb/test/pos/norm.txt data/test-pos.txt
mv aclImdb/test/neg/norm.txt data/test-neg.txt

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above script store postive examples in file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data/train-pos.txt&lt;/code&gt; and negative examples in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data/train-neg.txt&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;we will use spacy to get word vector embeddings&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
import imdb
import os
import spacy


def pad_vec_sequences(sequences, maxlen=40):
        new_sequences = []
        for sequence in sequences:

            orig_len, vec_len = np.shape(sequence)

            if orig_len &amp;lt; maxlen:
                new = np.zeros((maxlen, vec_len))
                for k in range(maxlen-orig_len,maxlen):
                    new[k:, :] = sequence[k-maxlen+orig_len]

            else:
                new = np.zeros((maxlen, vec_len))
                for k in range(0,maxlen):
                    new[k:,:] = sequence[k]

            new_sequences.append(new)

        return np.array(new_sequences)

def test():        
    nlp1 = spacy.load('en')

    f=&quot;data/train-pos.txt&quot;
    train_data=[]
    Xtrain=[]
    labels=[]
    for sentence in open(f).xreadlines():
        
        row={}

        t=[]
        try:
            sentence1 = ''.join([i if ord(i) &amp;lt; 128 else ' ' for i in sentence])
            r=unicode(sentence1)
            doc = nlp1(r)
            #print doc
            for token in doc:
                t.append(token.vector)

            row['text']=sentence;
            row['feature']=t;
            row['lable']=1;
            Xtrain.append(t)
            labels.append(1)
            train_data.append(row)
        except:
            print &quot;skip data row&quot;,sentence


    f = &quot;data/train-neg.txt&quot;
    for sentence in open(f).xreadlines():

        row={}

        t=[]
        try:
            sentence1=''.join([i if ord(i) &amp;lt; 128 else ' ' for i in sentence])
            r=unicode(sentence1)
            doc = nlp1(r)
            #print doc
            for token in doc:
                t.append(token.vector)

            row['text']=sentence;
            row['feature']=t;
            row['lable']=1;
            Xtrain.append(t)
            labels.append(0)
            train_data.append(row)
        except:
            print &quot;skip data row&quot;,sentence


    print(&quot;completed processing the data&quot;)
    max_words=50

    num_classes = np.max(labels) + 1

    X_train = pad_vec_sequences(Xtrain, maxlen=max_words, padding=&quot;post&quot;, truncating=&quot;post&quot;)
    y_train = keras.utils.to_categorical(labels, num_classes)
    print(&quot;Training data: &quot;)
    print(X_train.shape),(y_train.shape)

    X_train=np.reshape(X_train,(X_train.shape[0],max_words));

    print(&quot;input tensor&quot;)
    print(X_train.shape),y_train.shape

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However directly using the word vector embeddings does not often lead to good results,
since the embeddings are trained over the common crawl corpus but it may no be representative
of the current database.&lt;/p&gt;

&lt;p&gt;Another approach is to use pretrained word vector embeddings as initializations for Embedding Layer
Thus uses the glove2vec word vector embedding as initialization for the embedding layer of the LSTM
neural network.&lt;/p&gt;

&lt;p&gt;You can download the glove2vec word vector embedding data from &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;GloVe: Global Vectors for Word Representation&lt;/a&gt; . We will download the glove2vec trained over wikipedia 2014 database using the top 400K words. &lt;a href=&quot;http://nlp.stanford.edu/data/glove.6B.zip&quot;&gt;http://nlp.stanford.edu/data/glove.6B.zip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We will use the 100 dimensional feature represenation in this example.&lt;/p&gt;

&lt;p&gt;First we will read all the words from the glove2vec database and create a index of words
and corresponding word vector embedding&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    MAX_SEQUENCE_LENGTH = 50
    EMBEDDING_DIM = 100 
    embeddings_index = {}
    f = open(os.path.join('data/', 'glove.6B.100d.txt'))
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()

    print('Found %s word vectors.' % len(embeddings_index))

    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            embedding_matrix[i] = embedding_vector

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For the LSTM network the traning data will consists of sequence of word vector indices representing the movie review from the IMDB dataset and the output will be sentiment. We will use the same database as used in the article
&lt;a href=&quot;http://pi19404.github.io/pyVision/2018/01/30/lstm2/&quot;&gt;Sequence classification with LSTM&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    f=&quot;data/tpos.txt&quot;
    train_data=[]
    Xtrain=[]
    labels=[]
    print &quot;reading postive samples&quot;
    text=[]
    for sentence in open(f).xreadlines():
        #print &quot;AAAAAAAAAA&quot;,sentence
        row={}
        tt = [];
        t=[]
        try:
            sentence1 = ''.join([i if ord(i) &amp;lt; 128 else ' ' for i in sentence])

            str1 = &quot; &quot;.join(str(x) for x in sentence1);

            labels.append(1)

            text.append(str1)
        except:
            print &quot;pskip data row&quot;,sentence
            continue;

    f=&quot;data/tneg.txt&quot;
    print &quot;reading negative samples&quot;
    for sentence in open(f).xreadlines():
      
        row={}
        tt = [];
        t=[]
        try:
            sentence1 = ''.join([i if ord(i) &amp;lt; 128 else ' ' for i in sentence])

            str1 = &quot; &quot;.join(str(x) for x in sentence1);
            #Xtrain.append(t)
            labels.append(0)

            text.append(str1)
        except:
            print &quot;nskip data row&quot;,sentence
            continue;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;each element of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text&lt;/code&gt; contains an sequence of words. Now we use a Tokenizer to represent words by their word indexes.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical

MAX_NB_WORDS = 100000

tokenizer = Tokenizer(num_words=MAX_NB_WORDS)

#create the word indices
tokenizer.fit_on_texts(text)

word_index = tokenizer.word_index

#convert the training data to sequence of word indexes
sequences = tokenizer.texts_to_sequences(text)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This tells the tokenizer to consider only the most frequently occuring 100K words in the training dataset&lt;/p&gt;

&lt;p&gt;we pad the sequences to create a sequence of same length to be passed to the LSTM network&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;First we create the embedding layer of LSTM network and initialize the weights using the embedding matrix
created in the earlier step. We set trainable to true so that embeddings are updated based on the current training
data as the embedding matrix passed is used as initialization . If we set to trainable to false then the embeddings will not be updated and network will use the embeddings as per the embdding matrix itself.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    embedding_layer = Embedding(input_dim=len(word_index) + 1,
                                output_dim=EMBEDDING_DIM,
                                weights=[embedding_matrix],
                                input_length=MAX_SEQUENCE_LENGTH,
                                trainable=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we will create the network and train the same&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.layers import LSTM
    from keras.layers import Dense, Embedding

    model = Sequential()
    model.add(embedding_layer)


    model.add(LSTM(128, return_sequences=False
               , input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)))


    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    model.fit(data,y_train,epochs=20, batch_size=100, verbose=1)
    save_model(model,&quot;/tmp/model4&quot;)

    model.evaluate(X_test,y_test)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The approach of using exising word vector embeddings as initialization in embedding layer provides
the best results .&lt;/p&gt;

&lt;p&gt;This approach can be also considered as approach for transfer learning where the embeedings learnt over
a large corups are being use to learn embeddings over a much smaller corpus ,how the initializations boosts
learning and generalization ability of the classifier at hand. If we dont have enough training data to learn
good embeddings over current data,it may limit the generalization ability of the network.however by using this approach we are able to use existing existing embedding matrix and tune it specifically for the data at hand.&lt;/p&gt;

&lt;p&gt;The complete code for the same can be found at &lt;a href=&quot;https://gist.github.com/pi19404/1d2f5cb64380740781338949ec4aac8a&quot;&gt;code1&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Sequence classification with LSTM</title>
   <link href="http://localhost:4000/2018/01/30/lstm2/"/>
   <updated>2018-01-30T00:00:00+00:00</updated>
   <id>http://localhost:4000/2018/01/30/lstm2</id>
   <content type="html">&lt;p&gt;In this article, we will look at how to use LSTM recurrent neural network models for sequence classification problems using the Keras deep learning library&lt;/p&gt;

&lt;p&gt;A standard dataset used to demonstrate sequence classification is sentiment classficiation on IMDB movie review dataset.&lt;/p&gt;

&lt;p&gt;Each movie review is a sentense and sentiment associated with each movie review must be determined.The training dataset consists of movie review and associated sentiment.A sentense is modelled as a sequence of words and hence we look sequence classification problem.&lt;/p&gt;

&lt;p&gt;Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer “3” encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: “only consider the top 10,000 most common words, but eliminate the top 20 most common words”.&lt;/p&gt;

&lt;p&gt;A representation of review of imdb database is as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For this example we will only take first 50 words of the review . we will pad dataset to a maximum review length in words.&lt;/p&gt;

&lt;p&gt;The same review is represented as&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[    1    14    22    16    43   530   973  1622  1385    65   458  4468
    66  3941     4   173    36   256     5    25   100    43   838   112
    50   670 22665     9    35   480   284     5   150     4   172   112
   167 21631   336   385    39     4   172  4536  1111    17   546    38
    13   447]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np

from keras.datasets import imdb
from keras.preprocessing import sequence

import numpy as np
import keras
from keras.datasets import reuters
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.preprocessing.text import Tokenizer

#from matplotlib import pyplot
# load the dataset
(X_train, y_train), (X_test, y_test) = imdb.load_data()
print(&quot;Training data: &quot;)
print(X_train.shape)
print(&quot;Testing data: &quot;)
print(X_test.shape)

num_classes = np.max(y_train) + 1
print(num_classes, 'classes')

max_words = 50
X_train = sequence.pad_sequences(X_train, maxlen=max_words,padding=&quot;post&quot;,truncating=&quot;post&quot;)
X_test = sequence.pad_sequences(X_test, maxlen=max_words,padding=&quot;post&quot;,truncating=&quot;post&quot;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A sentense can be modelled as sequence of words indexes,however there is no contextual relation between index 1 and index 2 .&lt;/p&gt;

&lt;p&gt;We will use LSTM to model sequences,where input to LSTM is sequence of indexs representing words and output is sentiment associated with the sentense&lt;/p&gt;

&lt;p&gt;Input to LSTM is a 3D tensor with shape (batch_size, timesteps, input_dim). In out present case the batch_size will be the size of training data.Timesteps will be sequence length 50 and input dimensions will be 1 since the sequences as integer indexes&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

def save_model(model, filename):
    logging.info(&quot;saving  the model %s&quot; % (filename))
    model_json = model.to_json()
    with open(filename + '.model', &quot;w&quot;) as json_file:
        json_file.write(model_json)
        json_file.close();
    model.save_weights(filename + &quot;.weights&quot;)

def load_model(filename):
    logging.info(&quot;loading the model %s&quot; % (filename))
    json_file = open(filename + '.model', 'r')
    loaded_model_json = json_file.read()
    json_file.close()
    loaded_model = model_from_json(loaded_model_json)
    loaded_model.load_weights(filename + &quot;.weights&quot;)

    return loaded_model;
		
model=Sequential()

model.add(LSTM(50,return_sequences=False,input_shape=(X_train.shape[1],X_train.shape[2])))
model.add(Dense(num_classes, activation='softmax'))
print(model.summary())
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train,y_train,epochs=1000, batch_size=100, verbose=1)

save_model(model,&quot;/tmp/model1&quot;)

model.evaluate(X_test,y_test)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We obtain a training accuracy of 58% on the training data after 75 iterations&lt;/p&gt;

&lt;p&gt;In this approach words are simply represented as word indexes and sequence of words are used to train LSTM . The word indexes are themselves do not represent any information are in general orthogonal . Word indexes may be represented as one hot vector encoding,but for large dictionary sizes such a representation will be too large and occury lot of memory and computational resources.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Word Embeddings&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A word embedding is a class of approaches for representing words and documents using a dense vector representation. The position of word in N dimensional space is 
referred to as its embedding. The word embeddings are learned from text and is based on the words that surround the word when it is used ie context in which words are used.&lt;/p&gt;

&lt;p&gt;If we represent words as index of vocubulary we end up with large dimensional sparse vector using techniques like one hot encoding.The idea of word vectors can also be viewed
as method to cast a high dimensional sparse representation to a low dimensional dense representation.&lt;/p&gt;

&lt;p&gt;Keras offers an Embedding layer that can be used for neural networks on text data.&lt;/p&gt;

&lt;p&gt;The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).&lt;/p&gt;

&lt;p&gt;The parameters of Keras Embedding Layer is&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;input_dim is the vocabulary size&lt;/li&gt;
  &lt;li&gt;output_dim is the dimension of the output embedding&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

def save_model(model, filename):
    logging.info(&quot;saving  the model %s&quot; % (filename))
    model_json = model.to_json()
    with open(filename + '.model', &quot;w&quot;) as json_file:
        json_file.write(model_json)
        json_file.close();
    model.save_weights(filename + &quot;.weights&quot;)

def load_model(filename):
    logging.info(&quot;loading the model %s&quot; % (filename))
    json_file = open(filename + '.model', 'r')
    loaded_model_json = json_file.read()
    json_file.close()
    loaded_model = model_from_json(loaded_model_json)
    loaded_model.load_weights(filename + &quot;.weights&quot;)

    return loaded_model;
		
X_train=np.reshape(X_train,(X_train.shape[0],max_words));
X_test=np.reshape(X_test,(X_test.shape[0],max_words));


print(&quot;input tensor&quot;)
print(X_train.shape),y_train.shape

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dense, Embedding

model=Sequential()

model.add(Embedding(vocab_size,300,input_length=max_words))
model.add(LSTM(128,return_sequences=False,input_shape=(300,max_words)))
model.add(Dense(num_classes, activation='softmax'))
print(model.summary())
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train,y_train,epochs=1000, batch_size=100, verbose=1)

save_model(model,&quot;/tmp/model1&quot;)

model.evaluate(X_test,y_test)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see that by adding embedding layer at 1 iterations we obtained an accuracy of  73% . Thus by simply adding a embedding layer that encapsulates context of words to arrive at word vector embedding provides a significantly higher accuracy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/&lt;/li&gt;
  &lt;li&gt;http://pi19404.github.io/pyVision/2017/05/13/spacy1/&lt;/li&gt;
  &lt;li&gt;https://keras.io/preprocessing/sequence/&lt;/li&gt;
  &lt;li&gt;https://sandipanweb.wordpress.com/2017/03/30/sentiment-analysis-using-linear-model-classifier-with-hinge-loss-and-l1-penalty-with-language-model-features-and-stochastic-gradient-descent-in-python/&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Java AES encryption and decryption</title>
   <link href="http://localhost:4000/2017/05/24/javaaes/"/>
   <updated>2017-05-24T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/24/javaaes</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;In cryptography, encryption is the process of encoding a message or information in such a way that only authorized parties can access it&lt;/p&gt;

&lt;p&gt;Encryption does not of itself prevent interference, but denies the intelligible content to a would-be interceptor&lt;/p&gt;

&lt;p&gt;In an encryption scheme, the intended information or message, referred to as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;plaintext&lt;/code&gt;, is encrypted using an encryption algorithm, generating &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ciphertext&lt;/code&gt; that can only be read if decrypted.&lt;/p&gt;

&lt;p&gt;For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is in principle possible to decrypt the message without possessing the key, but, for a well-designed encryption scheme, considerable computational resources and skills are required.&lt;/p&gt;

&lt;p&gt;An authorized recipient can easily decrypt the message with the key provided by the originator to recipients but not to unauthorized users.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encryption Schemes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Efficient encryption schemes usually operate on fixed-size messages called blocks. Such schemes are called block ciphers.&lt;/p&gt;

&lt;p&gt;Well-known examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DES (Data Encryption Standard).&lt;/li&gt;
  &lt;li&gt;3DES (Triple DES)&lt;/li&gt;
  &lt;li&gt;AES (Advanced Encryption Standard)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;AES&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Advanced Encryption Standard, or AES, is a symmetric block cipher.&lt;/p&gt;

&lt;p&gt;It chosen by the U.S. government to protect classified information and is implemented in software and hardware throughout the world to encrypt sensitive data&lt;/p&gt;

&lt;p&gt;In present day cryptography, AES is widely adopted and supported in both hardware and software.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
import javax.crypto.KeyGenerator;
import javax.crypto.SecretKey;

//Generate a key for encryption of specified length
    public static SecretKey getSecretEncryptionKey(int length) throws Exception{
        KeyGenerator generator = KeyGenerator.getInstance(&quot;AES&quot;);
        System.out.println(generator.getProvider().toString());
        generator.init(length); // The AES key size in number of bits
        SecretKey secKey = generator.generateKey();
        return secKey;
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the key is generated we can encrypt any plain text using the secret key&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    public String encryptText(String plainText,SecretKey SEC_KEY,String initVector) throws Exception{

        Cipher aesCipher = Cipher.getInstance(&quot;AES/CBC/PKCS5Padding&quot;);
       IvParameterSpec iv = new IvParameterSpec(initVector.getBytes(&quot;UTF-8&quot;));
        aesCipher.init(Cipher.ENCRYPT_MODE, SEC_KEY,iv);
        byte[] byteCipherText = aesCipher.doFinal(plainText.getBytes());
        String encryptedData = Base64.getEncoder().encodeToString(byteCipherText);

        return encryptedData;
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Any entity with the access to the secret key would be able to decrypt the encoded text message&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    public String decryptText(String encryptedData,SecretKey SEC_KEY,String initVector) throws Exception {

        byte[] byteCipherText = Base64.getDecoder().decode(encryptedData);
				IvParameterSpec iv = new IvParameterSpec(initVector.getBytes(&quot;UTF-8&quot;));
        Cipher aesCipher = Cipher.getInstance(&quot;AES/CBC/PKCS5Padding&quot;);
        aesCipher.init(Cipher.DECRYPT_MODE, SEC_KEY,iv);
        byte[] bytePlainText = aesCipher.doFinal(byteCipherText);
        return new String(bytePlainText);
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following functions allow us to encrypt a AES 128 bit key and encode and decode a string using
CBC block cipher algorithm and use PKCS5Padding scheme&lt;/p&gt;

&lt;p&gt;One of best-known, good block cipher modes is cipher block chaining (CBC).With it, every ciphertext block depends on all previous ciphertext blocks, which avoids repetition problems like we observed with ECB.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;           String plaintext=&quot;12345asdasdadsadasd&quot;;					 
            String initVector = &quot;RandomInitVector&quot;; // 16 bytes IV
            SecretKey k=getSecretEncryptionKey(128);
            String e=encryptText(plaintext, k,initVector);
            String d=decryptText(e, k,initVector);
            System.out.println(&quot;AES encryption Testing -&amp;gt; text &quot;+plaintext+&quot;\n Encrypted String : &quot;+e+&quot;\n Descrypted String &quot;+d);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h1 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;http://www.cs.cornell.edu/courses/cs5430/2015sp/notes/crypto.php&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>NLP Introduction to LSTM using Keras</title>
   <link href="http://localhost:4000/2017/05/19/lstmkeras/"/>
   <updated>2017-05-19T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/19/lstmkeras</id>
   <content type="html">&lt;h1 id=&quot;long-short-term-memory-network&quot;&gt;&lt;strong&gt;Long Short-Term Memory Network&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;The Long Short-Term Memory network, or LSTM network, is a recurrent neural network.&lt;/p&gt;

&lt;p&gt;RNN are networks with loops in them, allowing information to persist.Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies&lt;/p&gt;

&lt;p&gt;LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to lear&lt;/p&gt;

&lt;p&gt;We can use LSTM for host of NLP classification problem.&lt;/p&gt;

&lt;h2 id=&quot;input-training-data&quot;&gt;&lt;strong&gt;Input Training data&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The input training data is of the form&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{ u'text': u'11 dollar to rupee conversion', u'intent': u'&quot;conversion&quot;'}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;word-embedding&quot;&gt;&lt;strong&gt;Word Embedding&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The input to LSTM network is a sequence of tokens of the sentense and the output is associated class lable.
The LSTM network will model how various words belonging to a class occur in a statement/document.&lt;/p&gt;

&lt;p&gt;We will be using spaCy NLP package.The built in word embedding function  provides a word vector of length 300.&lt;/p&gt;

&lt;p&gt;For details about using spaCy and computing word vectors refer to the article http://34.194.184.172/emotix/index.php/2017/05/13/nlp-spacy-word-and-document-vectors/&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    def word2vec(self,tokens):
        output=[]
        for d in tokens:
            doc = self.nlp1(d['text'])
            t=[]
						l=[]
            for token in doc:
				  t.append(token.vector)								
			      l.append(token.lemma)
			d['tokens']=l
            d['features']=t;
            output.append(d)
        return output
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below is the code on how to call the above function&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vectors=nlp.word2vec(data)
print(&quot;number of training samples are&quot;, len(vectors))
print(&quot;dimension each token of training data &quot;, len(vectors[0]['features'][0]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above function accepts the training data.For each sentence in the training data the function computes the document vector for each token and stores the list of features in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;features&lt;/code&gt; tag of the output &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;json&lt;/code&gt; data.&lt;/p&gt;

&lt;h2 id=&quot;pre-processing--the-sequential-data&quot;&gt;&lt;strong&gt;Pre-Processing  the sequential data&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Since we will be analyzing short sentences and not long paragraphs of text we will choose the maximum number of tokens in the sentence to be 40&lt;/p&gt;

&lt;p&gt;If the tokens in a sentence are less than 40 we will be zero padding or trimming it so that all sequence are of the same length.This step pre-processes the sequential data so that training data consists of vectors of the same dimension.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    def pad_vec_sequences(self,sequences, maxlen=40):
        new_sequences = []
        for sequence in sequences:
            orig_len, vec_len = np.shape(sequence)

            if orig_len &amp;lt; maxlen:
                new = np.zeros((maxlen, vec_len))
                new[maxlen - orig_len:, :] = sequence
            else:
                new = sequence[orig_len - maxlen:, :]
            new_sequences.append(new)

        return np.array(new_sequences)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above function is used as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;maxlen=40
Xtrain=[]
labels=[]
for d in vectors:
   Xtrain.append(d['features'])
   labels.append(d['intent'])
Xtrain = self.pad_vec_sequences(Xtrain, maxlen)						
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above function pads or truncates the input training data so that each sample of training data is of size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(maxlen,dimension)&lt;/code&gt;
Thus inputs to the neural network are of fixed dimension vector of size (40,300) representing the sentence/document being analyzed.&lt;/p&gt;

&lt;p&gt;We will prepare the training data in a form suitable to be used with keras&lt;/p&gt;

&lt;p&gt;First we will process the output labels so that textual categories are covnverted to one hot encoded vector form&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from sklearn import preprocessing
from keras.utils import np_utils, generic_utils
				
self.label_encoder = preprocessing.LabelEncoder()
#converts text categories to integers
self.label_encoder.fit(labels)
y=self.label_encoder.transform(labels)
        
#converts integers to one hot encoded vectors
y=np_utils.to_categorical(y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our training data consists of 3 unique classes so the output vectors are encoded as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
[1  0  0]
[0  1  0]
[0  0  1]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;training-and-cross-validation-data-split&quot;&gt;&lt;strong&gt;Training and Cross Validation Data Split&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;No we will split our training data into training and cross validation data&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.70, random_state = 42)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This splits the training data such that 70% data is used for cross validation and 30% data is training data.&lt;/p&gt;

&lt;p&gt;If the input training data X has 282 samples then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X_train&lt;/code&gt; will have 84 and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X_test&lt;/code&gt; will have 198 samples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Keras Pre Processing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The training data required for keras is of the form &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[samples, time steps, features]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In our case time steps is length of sequential data being analyed ie maximum number of words being analyzed in the sequence.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; X_train = numpy.reshape(X_train, (X_train.shape[0],X_train.shape[1],X_train.shape[2]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Keras Configuration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Keras is a high-level neural networks API, written in Python and capable of running on top of either TensorFlow or Theano&lt;/p&gt;

&lt;p&gt;We will be using tensorflow as backend to Keras&lt;/p&gt;

&lt;p&gt;The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = Sequential()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Stacking layers is as easy as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.add():&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In the present example we want to configure Keras to use LSTM networks.Hence first layer we want to stack is LSTM&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; model.add(LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2])))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The LSTM network will accepts input vectors of shape &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(40,300)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The output of the LSTM network is class lable.Hence number of outputs of the last layer is equal to the number of unique classes.In the present example the number of output classes are 3.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model.add(Dense(nclasses, activation='softmax'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We are specyfying that we will be using softmax activation function for the output layers&lt;/p&gt;

&lt;p&gt;And then we can choose the how we want to model the hidden layers between the input and output layers&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
        model = Sequential()
        model.add(LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2])))
        model.add(Dense(20))
        model.add(Dense(nclasses, activation='softmax'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the layers of model are added we configure the parameters for the learning process&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We choose &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;categorical_crossentropy&lt;/code&gt; as loss function when we want to perform categorical classification task.We choose the optimizer as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;adam optimizer&lt;/code&gt; which is a slightly enhanced version of the stochastic gradient descent.For any categorical classification problem the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;metrics&lt;/code&gt; parameter is set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accuracy&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training the model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To train the model&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; model.fit(X_train,y_train,epochs=10, batch_size=1, verbose=2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we can choose the number of epochs and batch size so that training data can be iterated over in batches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Evaluating the model&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)
print loss_and_metrics
model.save('my_model.h5')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Batch Size&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Keras implementation of LSTMs resets the state of the network after each batch.&lt;/p&gt;

&lt;p&gt;This suggests that if we had a batch size large enough to hold all input patterns and if all the input patterns were ordered sequentially, that the LSTM could use the context of the sequence within the batch to better learn the sequence.&lt;/p&gt;

&lt;p&gt;Keras shuffles the training dataset before each training epoch. To ensure the training data patterns remain sequential, we can disable this shuffling.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://keras.io/getting-started/sequential-model-guide/&lt;/li&gt;
  &lt;li&gt;https://keras.io/&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>NLP Basics of using Stanford corenlp server</title>
   <link href="http://localhost:4000/2017/05/19/corenlp1/"/>
   <updated>2017-05-19T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/19/corenlp1</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://stanfordnlp.github.io/CoreNLP/index.html&quot;&gt;Stanford CoreNLP&lt;/a&gt; provides a set of natural language analysis tools written in Java&lt;/p&gt;

&lt;p&gt;CoreNLP source code can be downloaded from https://github.com/stanfordnlp/CoreNLP&lt;/p&gt;

&lt;p&gt;CoreNLP 3.7.0 package can be downloaded from link https://stanfordnlp.github.io/CoreNLP/index.html#download
Also download the model files from the page&lt;/p&gt;

&lt;p&gt;CoreNLP can be hosted as a service by running the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;java -mx4g -cp stanford-corenlp-3.7.0-models.jar:stanford-corenlp-3.7.0.jar:slf4j-api.jar:slf4j-simple.jar edu.stanford.nlp.pipeline.StanfordCoreNLPServer 9000 10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This starts the coreNLP server at port 9000&lt;/p&gt;

&lt;p&gt;Accessing the port 9000 from the browser will give a interface similar to one available on stanford coreNLP
server website http://corenlp.run/&lt;/p&gt;

&lt;p&gt;We can call the stanford CoreNLP server using client API from client application.&lt;/p&gt;

&lt;p&gt;In this article we will be using python wrapper for coreNLP server&lt;/p&gt;

&lt;p&gt;download and install the python package https://github.com/smilli/py-corenlp&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pycorenlp
from pycorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost:9000')

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;CoreNLP works by using Annotators to build Annotations over a stream of text using a CoreNLP pipeline&lt;/p&gt;

&lt;p&gt;There are a lot of different annotators available,&lt;/p&gt;

&lt;p&gt;tokenize - Tokenizes the words
ssplit - Splits a sequence of tokens into sentences.
pos - Labels tokens with their POS tag.
lemma- computes the LEMMA for the token
ner - perform entity recognition for the token
sentiment - sentiment analysis annotator&lt;/p&gt;

&lt;p&gt;Now we send the request to NLP server with inputs as text NLP pipeline properties to be executed on the server&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                    output = nlp.annotate(line, properties={
                    'annotators': 'tokenize,ssplit,pos,lemma,ner,sentiment',
                    'outputFormat': 'json'
                    })
										
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example  the input &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;no i dont want to tell you my name&lt;/code&gt; has the output&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{u'sentences': [{u'tokens': [{u'index': 1, u'word': u'no', u'after': u' ', u'pos': u'DT', u'characterOffsetEnd': 2, u'characterOffsetBegin': 0, u'originalText': u'no', u'before': u''}, {u'index': 2, u'word': u'i', u'after': u' ', u'pos': u'FW', u'characterOffsetEnd': 4, u'characterOffsetBegin': 3, u'originalText': u'i', u'before': u' '}, {u'index': 3, u'word': u'dont', u'after': u' ', u'pos': u'FW', u'characterOffsetEnd': 9, u'characterOffsetBegin': 5, u'originalText': u'dont', u'before': u' '}, {u'index': 4, u'word': u'want', u'after': u' ', u'pos': u'VBP', u'characterOffsetEnd': 14, u'characterOffsetBegin': 10, u'originalText': u'want', u'before': u' '}, {u'index': 5, u'word': u'to', u'after': u' ', u'pos': u'TO', u'characterOffsetEnd': 17, u'characterOffsetBegin': 15, u'originalText': u'to', u'before': u' '}, {u'index': 6, u'word': u'tell', u'after': u' ', u'pos': u'VB', u'characterOffsetEnd': 22, u'characterOffsetBegin': 18, u'originalText': u'tell', u'before': u' '}, {u'index': 7, u'word': u'you', u'after': u' ', u'pos': u'PRP', u'characterOffsetEnd': 26, u'characterOffsetBegin': 23, u'originalText': u'you', u'before': u' '}, {u'index': 8, u'word': u'my', u'after': u' ', u'pos': u'PRP$', u'characterOffsetEnd': 29, u'characterOffsetBegin': 27, u'originalText': u'my', u'before': u' '}, {u'index': 9, u'word': u'name', u'after': u'', u'pos': u'NN', u'characterOffsetEnd': 34, u'characterOffsetBegin': 30, u'originalText': u'name', u'before': u' '}], u'index': 0, u'sentiment': u'Negative', u'sentimentValue': u'1', u'enhancedDependencies': [{u'dep': u'ROOT', u'dependent': 3, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'dont'}, {u'dep': u'neg', u'dependent': 1, u'governorGloss': u'dont', u'governor': 3, u'dependentGloss': u'no'}, {u'dep': u'compound', u'dependent': 2, u'governorGloss': u'dont', u'governor': 3, u'dependentGloss': u'i'}, {u'dep': u'acl:relcl', u'dependent': 4, u'governorGloss': u'dont', u'governor': 3, u'dependentGloss': u'want'}, {u'dep': u'mark', u'dependent': 5, u'governorGloss': u'tell', u'governor': 6, u'dependentGloss': u'to'}, {u'dep': u'xcomp', u'dependent': 6, u'governorGloss': u'want', u'governor': 4, u'dependentGloss': u'tell'}, {u'dep': u'iobj', u'dependent': 7, u'governorGloss': u'tell', u'governor': 6, u'dependentGloss': u'you'}, {u'dep': u'nmod:poss', u'dependent': 8, u'governorGloss': u'name', u'governor': 9, u'dependentGloss': u'my'}, {u'dep': u'dobj', u'dependent': 9, u'governorGloss': u'tell', u'governor': 6, u'dependentGloss': u'name'}], u'basicDependencies': [{u'dep': u'ROOT', u'dependent': 3, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'dont'}, {u'dep': u'neg', u'dependent': 1, u'governorGloss': u'dont', u'governor': 3, u'dependentGloss': u'no'}, {u'dep': u'compound', u'dependent': 2, u'governorGloss': u'dont', u'governor': 3, u'dependentGloss': u'i'}, {u'dep': u'acl:relcl', u'dependent': 4, u'governorGloss': u'dont', u'governor': 3, u'dependentGloss': u'want'}, {u'dep': u'mark', u'dependent': 5, u'governorGloss': u'tell', u'governor': 6, u'dependentGloss': u'to'}, {u'dep': u'xcomp', u'dependent': 6, u'governorGloss': u'want', u'governor': 4, u'dependentGloss': u'tell'}, {u'dep': u'iobj', u'dependent': 7, u'governorGloss': u'tell', u'governor': 6, u'dependentGloss': u'you'}, {u'dep': u'nmod:poss', u'dependent': 8, u'governorGloss': u'name', u'governor': 9, u'dependentGloss': u'my'}, {u'dep': u'dobj', u'dependent': 9, u'governorGloss': u'tell', u'governor': 6, u'dependentGloss': u'name'}], u'parse': u'(ROOT\n  (FRAG\n    (NP\n      (NP (DT no) (FW i) (FW dont))\n      (SBAR\n        (S\n          (VP (VBP want)\n            (S\n              (VP (TO to)\n                (VP (VB tell)\n                  (NP (PRP you))\n                  (NP (PRP$ my) (NN name)))))))))))', u'enhancedPlusPlusDependencies': [{u'dep': u'ROOT', u'dependent': 3, u'governorGloss': u'ROOT', u'governor': 0, u'dependentGloss': u'dont'}, {u'dep': u'neg', u'dependent': 1, u'governorGloss': u'dont', u'governor': 3, u'dependentGloss': u'no'}, {u'dep': u'compound', u'dependent': 2, u'governorGloss': u'dont', u'governor': 3, u'dependentGloss': u'i'}, {u'dep': u'acl:relcl', u'dependent': 4, u'governorGloss': u'dont', u'governor': 3, u'dependentGloss': u'want'}, {u'dep': u'mark', u'dependent': 5, u'governorGloss': u'tell', u'governor': 6, u'dependentGloss': u'to'}, {u'dep': u'xcomp', u'dependent': 6, u'governorGloss': u'want', u'governor': 4, u'dependentGloss': u'tell'}, {u'dep': u'iobj', u'dependent': 7, u'governorGloss': u'tell', u'governor': 6, u'dependentGloss': u'you'}, {u'dep': u'nmod:poss', u'dependent': 8, u'governorGloss': u'name', u'governor': 9, u'dependentGloss': u'my'}, {u'dep': u'dobj', u'dependent': 9, u'governorGloss': u'tell', u'governor': 6, u'dependentGloss': u'name'}]}]}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see that the sentiment Value is negative and sentiment value is 1&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://github.com/stanfordnlp/CoreNLP&lt;/li&gt;
  &lt;li&gt;https://stanfordnlp.github.io/CoreNLP/index.html&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>NLP Installing TensorFlow on ubuntu 16.04</title>
   <link href="http://localhost:4000/2017/05/16/tensorflow1/"/>
   <updated>2017-05-16T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/16/tensorflow1</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt; is An open-source software library for Machine Intelligence.
In this article will be installing tensorflow on ubuntu 16.04 with GPU support.&lt;/p&gt;

&lt;h1 id=&quot;cuda-installation-&quot;&gt;**CUDA Installation **&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Verify You Have a CUDA-Capable GPU&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To verify that your GPU is CUDA-capable, at the command line, enter:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pi@pi-dektop:~$ lspci | grep -i nvidia
01:00.0 VGA compatible controller: NVIDIA Corporation GK208 [GeForce GT 730] (rev a1)
01:00.1 Audio device: NVIDIA Corporation GK208 HDMI/DP Audio Controller (rev a1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If your graphics card is from NVIDIA and it is listed in http://developer.nvidia.com/cuda-gpus, your GPU is CUDA-capable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Verify You Have a Supported Version of Linux&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The CUDA Development Tools are only supported on some specific distributions of Linux. These are listed in the CUDA Toolkit release notes.&lt;/p&gt;

&lt;p&gt;To determine which distribution and release number you’re running, type the following at the command line:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ uname -m &amp;amp;&amp;amp; cat /etc/*release

x86_64
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=&quot;Ubuntu 16.04.1 LTS&quot;
NAME=&quot;Ubuntu&quot;
VERSION=&quot;16.04.1 LTS (Xenial Xerus)&quot;
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=&quot;Ubuntu 16.04.1 LTS&quot;
VERSION_ID=&quot;16.04&quot;
HOME_URL=&quot;http://www.ubuntu.com/&quot;
SUPPORT_URL=&quot;http://help.ubuntu.com/&quot;
BUG_REPORT_URL=&quot;http://bugs.launchpad.net/ubuntu/&quot;
UBUNTU_CODENAME=xenial
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The x86_64 line indicates you are running on a 64-bit system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Verify the System Has gcc Installed&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The gcc compiler is required for development using the CUDA Toolkit. It is not required for running CUDA applications. It is generally installed as part of the Linux installation, and in most cases the version of gcc installed with a supported version of Linux will work correctly.&lt;/p&gt;

&lt;p&gt;To verify the version of gcc installed on your system, type the following on the command line:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pi@pi-dektop:~$  gcc --version
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If an error message displays, you need to install the development tools from your Linux distribution or obtain a version of gcc and its accompanying toolchain from the Web.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Verify the System has the Correct Kernel Headers and Development Packages Installed&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The CUDA Driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation&lt;/p&gt;

&lt;p&gt;The version of the kernel your system is running can be found by running the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pi@pi-dektop:~$ uname -r
4.4.0-75-generic
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The kernel headers and development packages for the currently running kernel can be installed with:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install linux-headers-$(uname -r)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Download CUDA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CUDA can be downloaded from https://developer.nvidia.com/cuda-downloads&lt;/p&gt;

&lt;p&gt;Select the relevant operating system,architecture type,OS distribution and version to download the appropriate file&lt;/p&gt;

&lt;p&gt;we will be installing cuda toolking using standalone installed which downloads the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.run&lt;/code&gt; file by choosing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runfile(local)&lt;/code&gt; option on the download screen.Optionally you can choose other options like to download a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.deb&lt;/code&gt; local or network package for installation.&lt;/p&gt;

&lt;p&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo sh cuda_8.0.61_375.26_linux.run&lt;/code&gt; and follow the instruction to install the latest nvidia drivers&lt;/p&gt;

&lt;p&gt;You have have to enter console mode and stop the desktop environment to install nvidia drivers&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hit CTRL+ALT+F1 and login using your credentials.&lt;/li&gt;
  &lt;li&gt;kill your current X server session by typing sudo service lightdm stop or sudo lightdm stop&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo sh cuda_8.0.61_375.26_linux.run&lt;/code&gt; and follow installation instructions&lt;/li&gt;
  &lt;li&gt;sudo reboot&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You may have to reboot the PC after installation.&lt;/p&gt;

&lt;p&gt;Upon restart if you face any issue run the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install nvidia-current
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Download NVIDIA Drivers&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install -y nvidia-367 nvidia-prime
sudo update-alternatives --config x86_64-linux-gnu_gl_conf
LD_PRELOAD=/usr/lib/nvidia-375/libnvidia-ml.so nvidia-smi
sudo apt-get install libcupti-dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;tensorflow-installation&quot;&gt;&lt;strong&gt;TENSORFLOW INSTALLATION&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;virtualenv&quot;&gt;&lt;strong&gt;Virtualenv&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Virtualenv is a virtual Python environment isolated from other Python development, incapable of interfering with or being affected by other Python programs on the same machine. During the virtualenv installation process, you will install not only TensorFlow but also all the packages that TensorFlow requires&lt;/p&gt;

&lt;p&gt;To start working with TensorFlow, you simply need to “activate” the virtual environment. All in all, virtualenv provides a safe and reliable mechanism for installing and running TensorFlow.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Install pip and virtualenv by issuing the following command:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$sudo apt-get install python-pip python-dev python-virtualenv 
 $sudo pip install virtualenv virtualenvwrapper
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create a virtualenv environment by issuing the following command:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;virtualenv --system-site-packages targetDirectory 
mkvirtualenv tensorflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Activate the virtualenv environment by issuing one of the following commands:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;workon tensorflow
source /opt/python/virtualenv/bin/activate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that you must activate the virtualenv environment each time you use TensorFlow&lt;/p&gt;

&lt;p&gt;When you are done using TensorFlow, you may deactivate the environment by invoking the deactivate&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;deactivate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Install tensorflow&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;we will install tensorflow and supporting libraries required for running machine learning algorithms&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install --upgrade tensorflow-gpu
sudo pip install keras
sudo pip install scikit-learn gensim pandas ijson nltk sklearn spacy
python -m spacy download en

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Install NVIDIA Machine learning softwares&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;NVIDIA requirements to run TensorFlow with GPU support&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CUDA® Toolkit 8.0&lt;/li&gt;
  &lt;li&gt;The NVIDIA drivers associated with CUDA Toolkit 8.0&lt;/li&gt;
  &lt;li&gt;cuDNN v5.1&lt;/li&gt;
  &lt;li&gt;libcupti-dev library, which is the NVIDIA CUDA Profile Tools Interface&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Download the  NVIDIA &lt;a href=&quot;https://developer.nvidia.com/rdp/cudnn-download&quot;&gt;cuDNN&lt;/a&gt; is a GPU-accelerated library of primitives for deep neural networks.&lt;/p&gt;

&lt;p&gt;We will be using the cuDNN Version 5 which is compatible with cuda8.0
Select the option &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cuDNN v5 Library for Linux  Ubuntu16.04 (Deb)&lt;/code&gt; and follow the instructions to install the file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gunzip cudnn-8.0-linux-x64-v5.0-ga.tgz
tar -xvf cudnn-8.0-linux-x64-v5.0-ga.tar
cp -rf cuda /usr/local/cuda/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Check Tensorflow installation&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/lib/nvidia-381/
python -c &quot;import tensorflow; print(tensorflow.__version__)&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you get output then tensorflow has been installed properly&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://www.tensorflow.org/install/&lt;/li&gt;
  &lt;li&gt;http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#axzz4hDncLYTs&lt;/li&gt;
  &lt;li&gt;http://exponential.io/blog/2015/02/10/install-virtualenv-and-virtualenvwrapper-on-ubuntu/&lt;/li&gt;
  &lt;li&gt;http://exponential.io/blog/2015/02/10/configure-pycharm-to-use-virtualenv/&lt;/li&gt;
  &lt;li&gt;https://keras.io/#installation&lt;/li&gt;
  &lt;li&gt;https://developer.nvidia.com/cuda-downloads&lt;/li&gt;
  &lt;li&gt;https://www.tensorflow.org/install/install_linux&lt;/li&gt;
  &lt;li&gt;http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-nouveau&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>NLP Dictionary,Bag Of Words and TFIDF using Gensim</title>
   <link href="http://localhost:4000/2017/05/15/gensim2/"/>
   <updated>2017-05-15T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/15/gensim2</id>
   <content type="html">&lt;p&gt;In Natural language processing one of the most common questions is how to convert a sentense to some kind of numeric representation for machine learning algorithms.&lt;/p&gt;

&lt;p&gt;One of the elemenatry ways of doing this to represent a sentense by its mathematical represetation is by measuring the relative frequency count of words occuring in sentense&lt;/p&gt;

&lt;p&gt;Given a dictionary we can associate an index with every word occuring in the text document.&lt;/p&gt;

&lt;p&gt;A dictionary can be constructued from the training corpus or we can use a pre defined dictionary containing a word list&lt;/p&gt;

&lt;p&gt;We will be using &lt;a href=&quot;https://radimrehurek.com/gensim/index.html&quot;&gt;Gensim&lt;/a&gt; NLP software for Topic modelling in this article.&lt;/p&gt;

&lt;p&gt;we will be using the websters dictionary for building the dictionary for NLP purposes
Download the dictionary.txt file from https://github.com/pi19404/dictionary github repository&lt;/p&gt;

&lt;p&gt;The code to create the dictionary file is as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    def saveDictionary(self,source,dest):
        # Set up input and output files
        dict_file = self.cwd + '/'+source;
        dest_file=self.cwd+&quot;/&quot; + dest;

        #read the input text file
        f = open(dict_file, 'r')
        lines = f.readlines()
        f.close()

        #tokenize the data
        tokenize_data = [[word for word in line.lower().split()] for line in lines]

         #create and save the dictionary file
        dictionary = corpora.Dictionary(tokenize_data)
        dictionary.save(dest_file)
				
				
    def loadDictionary(self,filename):
        dict_file = self.cwd + '/'+filename;
        dictionary = corpora.Dictionary.load(dict_file)
        return dictionary;
				
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;saveDictionary&lt;/code&gt; accepts as a input a text file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source&lt;/code&gt; and creates a dictionary file as output&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dest&lt;/code&gt; .&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loadDictionary&lt;/code&gt; can load a dictionary from the file saved by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;saveDictionary&lt;/code&gt; function&lt;/p&gt;

&lt;p&gt;Next to represent a sentense we use a bag of vector model of representation.Where every word in a sentense is associated with a dictionary index and  frequency count of occurence of the word in sentense.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    def bowfeature(self,sentense,dictionary):
        corpus = dictionary.doc2bow(sentense)
        return corpus
				
tokens=&quot;do you know to go to market&quot;
tokenize_data = [word for word in tokens.lower().split()]

feature1=bowfeature(tokenize_data,dict)				
print feature1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above function accepts as input a sentense and dictionary object and returns the bag of words representation of a sentense&lt;/p&gt;

&lt;p&gt;For example The text &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do you know to go to market&lt;/code&gt; has the bow feature representation as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(29, 1), (116, 2), (928, 1), (1688, 1), (3685, 1), (23187, 1)]&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;do -&amp;gt; [(1688, 1)]
you -&amp;gt; [(29, 1)]
know -&amp;gt; [(3685, 1)]
to -&amp;gt; [(116, 1)]
go-&amp;gt;[(928, 1)]
to-&amp;gt;[(116, 1)]
market-&amp;gt;[(23187, 1)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each sentense may contain variable word lengths and its mathetical representation consists of sequence of words and its associated relative frequency of words occuring in the sentense wrt to entire courpus being analyze.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tf-idf&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus&lt;/p&gt;

&lt;p&gt;Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; 
the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears&lt;/p&gt;

&lt;p&gt;The TF term computes the relative frequency count of words occuring the document/sentence being analyzed and computes how important is the word in a document.The IDF term computes the imporance of word wrt to the entire corpus.The words which occur very frequently in the corpus will be assigned lower values than word which occur rarely.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function computetfidf(self,features)
		tfidf = models.TfidfModel(features)
		return tfidf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let us consider the following two statemets as toy example&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;([u'do', u'you', u'know', u'to', u'cook'], [(29, 1), (116, 1), (1688, 1), (3685, 1), (83291, 1)])
([u'do', u'know', u'to', u'go', u'to', u'market'], [(116, 2), (928, 1), (1688, 1), (3685, 1), (23187, 1)])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[(116, 2), (928, 1), (1688, 1), (3685, 1), (23187, 1)] [(928, 0.7071067811865475), (23187, 0.7071067811865475)]
[(29, 1), (116, 1), (1688, 1), (3685, 1), (83291, 1)] [(29, 0.7071067811865475), (83291, 0.7071067811865475)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The TF term for sentences are&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[(116, 2/6), (928, 1/6), (1688, 1/6), (3685, 1/6), (23187, 1/6)]
 [(29, 1/5), (116, 1/5), (1688, 1/5), (3685, 1/5), (83291, 1/5)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The IDF term is&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[(116, 0), (928, 1), (1688, 0), (3685, 0), (23187, 1)]
 [(29, 1), (116, 0), (1688, 0), (3685, 0), (832911)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now multiplying above two we get&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[(116, 0), (928, 1), (1688, 0), (3685, 0), (23187,1)]
 [(29,1), (116, 0), (1688, 0), (3685, 0), (83291,1)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can normalize the tfidf values for each document&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[(116, 0), (928, 1/sqrt(2)), (1688, 0), (3685, 0), (23187,1/sqrt(2))]
 [(29,1/sqrt(2)), (116, 0), (1688, 0), (3685, 0), (83291,1/sqrt(2))]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thus we can see that TFIDF has zero values for terms which are repeating the corpus while has non zero values
for terms that are less frequenct in the corpus.&lt;/p&gt;

&lt;p&gt;Assigning low values to stop words is automatically done by the TFIDF process therby eliminating the need
to do stop word removal as pre processing stage in the NLP pipeline.&lt;/p&gt;

&lt;p&gt;Thus given a set of sentenses we have obtained a mathematical representation of sentenses.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://radimrehurek.com/gensim/index.html&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>NLP Latent semantic indexing using Gensim</title>
   <link href="http://localhost:4000/2017/05/15/gensim1/"/>
   <updated>2017-05-15T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/15/gensim1</id>
   <content type="html">&lt;p&gt;In the article http://34.194.184.172/emotix/index.php/2017/05/15/nlp-dictionarybag-of-words-and-tfidf-using-gensim/
we saw how to compute the TFIDF vectors of document enabling us to reprepsent a sentense/document using a mathematical
representation.&lt;/p&gt;

&lt;p&gt;However the each sentense/document is reprepsented by unequal length feature vectors.&lt;/p&gt;

&lt;p&gt;To perform task like find which two documents are similar or find documents similar to given document.A typical ways is to compute the term document matrix.Where the rows consists of the terms while the columns consist of the documents.Thus by analyzing which terms are occuring in which documents we can compute similarity between document pairs.&lt;/p&gt;

&lt;p&gt;For performing tasks like finding the topic of document,recommendation we need a discriminative classifier.&lt;/p&gt;

&lt;p&gt;Whenever we want to compute similarity we need to compute the input document/sentence against all the sentence/document in term document matrix.The we can have a weighted count of documents the input document is similar to and assign the input document to the class which has registered maximum values of similarity measure.This simple approach may be satisfactory for a host of application especially if we want to run it only once and offline .&lt;/p&gt;

&lt;p&gt;Similarity over entire term-document matrix is not feasible method of discriminative classification especially if we want to run the algorithm in real time .&lt;/p&gt;

&lt;p&gt;Latent Semantic Analysis arose from the problem of how to find relevant documents from search words.
 LSA assumes that words that are close in meaning will occur will belong to the same topic.&lt;/p&gt;

&lt;p&gt;This enables us to&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It generates latent topics/models which are represented by fixed dimensional vector&lt;/li&gt;
  &lt;li&gt;It enables us to represent documents as linear combination of the topic vectors.&lt;/li&gt;
  &lt;li&gt;It converts a variable length feature vector representing a document to fixed length feature vector&lt;/li&gt;
  &lt;li&gt;It converts a large length feature vector to a short length feature vector and therby reduces the computational load&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In simple terms ,LSA uses  SVD to perform dimensionality reduction on the tf-idf vectors.It groups document into latent topics/classes and expresses and mathematicall represents the document as a linear combination of vectors representing the topics/classes&lt;/p&gt;

&lt;p&gt;The term-document matrix is converted to topic-document matrix.Some words/terms or dimensions of the matrix are combined
and depend on more than one term.The the rank lowering is expected to merge the dimensions associated with terms that have similar meanings.&lt;/p&gt;

&lt;p&gt;The words belonging to similar documents are expected to be merged to a single dimension representing topic/concept represented by the words.&lt;/p&gt;

&lt;p&gt;To construct the topic models or vectors the LSI algorithm accepts as input the tfidf vectors and dictionary used to construct the tfidf vectors.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
tfidf = models.TfidfModel(bow,normalize=False)
corpus_tfidf=tfidf[bow]

NO_OF_TOPICS=10;
lsi = models.LsiModel(corpus_tfidf, id2word=dict, num_topics=NO_OF_TOPICS)
corpus_lsi = lsi[corpus_tfidf]

for i in lsi.print_topics():
    print i
		
for doc in corpus_lsi:
     print(doc)
		 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example the statement has a tfidf vector as&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[(116, 0.08954685018891612), (292, 0.35664692448848984), (23575, 0.468033500575449), (76631, 0.6858442297536098), (396448, 0.4187441554540362)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The topic vectors are expressed as linear combination of word&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(0, u'-0.491*&quot;2&quot; + -0.377*&quot;a&quot; + -0.366*&quot;into&quot; + -0.241*&quot;to&quot; + -0.235*&quot;ounces&quot; + -0.227*&quot;pounds&quot; + -0.199*&quot;1&quot; + -0.186*&quot;pound&quot; + -0.174*&quot;gram&quot; + -0.172*&quot;years&quot;')
(1, u'-0.408*&quot;the&quot; + -0.383*&quot;weather&quot; + -0.366*&quot;what&quot; + -0.318*&quot;in&quot; + -0.298*&quot;is&quot; + -0.261*&quot;\'s&quot; + -0.211*&quot;distance&quot; + -0.164*&quot;from&quot; + 0.145*&quot;into&quot; + -0.144*&quot;how&quot;')
(2, u'0.440*&quot;1&quot; + -0.298*&quot;2&quot; + -0.293*&quot;into&quot; + 0.285*&quot;pound&quot; + 0.281*&quot;to&quot; + 0.229*&quot;us&quot; + 0.220*&quot;liquid&quot; + 0.208*&quot;a&quot; + 0.208*&quot;gallon&quot; + -0.201*&quot;ounces&quot;')
(3, u'-0.403*&quot;from&quot; + -0.311*&quot;far&quot; + -0.301*&quot;distance&quot; + 0.291*&quot;weather&quot; + 0.260*&quot;\'s&quot; + -0.250*&quot;is&quot; + -0.249*&quot;how&quot; + 0.229*&quot;in&quot; + -0.218*&quot;road&quot; + -0.218*&quot;by&quot;')
(4, u'0.440*&quot;us&quot; + 0.416*&quot;liquid&quot; + 0.386*&quot;gallon&quot; + -0.336*&quot;a&quot; + -0.297*&quot;pound&quot; + -0.205*&quot;gram&quot; + 0.184*&quot;into&quot; + 0.183*&quot;imperial&quot; + -0.129*&quot;tonne&quot; + -0.127*&quot;milligram&quot;')
(5, u'-0.418*&quot;indian&quot; + -0.363*&quot;rupees&quot; + -0.340*&quot;dollar&quot; + -0.285*&quot;convert&quot; + 0.228*&quot;distance&quot; + -0.205*&quot;rupee&quot; + -0.204*&quot;in&quot; + 0.170*&quot;a&quot; + -0.166*&quot;you&quot; + -0.166*&quot;do&quot;')
(6, u'0.632*&quot;ounce&quot; + 0.505*&quot;an&quot; + -0.349*&quot;pounds&quot; + 0.247*&quot;into&quot; + -0.180*&quot;a&quot; + -0.141*&quot;2&quot; + -0.125*&quot;gram&quot; + -0.125*&quot;to&quot; + 0.098*&quot;years&quot; + 0.085*&quot;1&quot;')
(7, u'-0.403*&quot;distance&quot; + 0.375*&quot;far&quot; + -0.340*&quot;between&quot; + 0.241*&quot;road&quot; + 0.241*&quot;by&quot; + -0.227*&quot;and&quot; + 0.221*&quot;from&quot; + 0.214*&quot;how&quot; + 0.182*&quot;weather&quot; + 0.154*&quot;goa&quot;')
(8, u'-0.724*&quot;years&quot; + 0.433*&quot;pounds&quot; + 0.298*&quot;ounces&quot; + 0.173*&quot;grams&quot; + -0.159*&quot;a&quot; + 0.154*&quot;ounce&quot; + -0.148*&quot;minutes&quot; + 0.111*&quot;an&quot; + -0.107*&quot;weeks&quot; + -0.079*&quot;seconds&quot;')
(9, u'0.673*&quot;pound&quot; + -0.543*&quot;gram&quot; + 0.222*&quot;ounces&quot; + 0.209*&quot;into&quot; + -0.190*&quot;ounce&quot; + -0.169*&quot;an&quot; + -0.128*&quot;grams&quot; + -0.128*&quot;2&quot; + -0.106*&quot;a&quot; + -0.093*&quot;in&quot;')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This show the 10 highest weighted words contributing to the topic vector&lt;/p&gt;

&lt;p&gt;After Performing LSI over the corpus containing the sentenses&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[(0, -0.051051416577990574), (1, -0.036492033868053303), (2, 0.074519246143707196), (3, -0.018580909401619147), (4, 0.016247308095280522), (5, -0.24649553328692864), (6, -0.01305561872828372), (7, -0.1020150188870306), (8, 0.00079849557800111158), (9, -0.035308315739288908)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thus we converted the set of documents to fixed length feature vector containing 10 vector.&lt;/p&gt;

&lt;p&gt;Now since each sentence/document is expressed as fixed length vectors a host of machine learning algorithms open up which can enable us to perform discriminative classification.&lt;/p&gt;

&lt;p&gt;As seen above the LSI gives us a method to transformation to convert any document to a fixed length feature representation.
Once LSI model is created and we can use it to transform another set of documents not related to the training dataset used to generate the LSI model.&lt;/p&gt;

&lt;p&gt;We can train the LSI model on a very large corpus collected over several years consisting of serveral millions of documents.This model will not be updated frequenctly due to computation and time complexities.&lt;/p&gt;

&lt;p&gt;While the training database for classification task or recommendation task may be a smaller database and which might be updated regularily or when new content becomes available.&lt;/p&gt;

&lt;p&gt;LSI captures the hidden context in the articles,any new documents belonging to the domain will exhibit similar characteristics .
Incremental addition of few thousand documents will not affect the LSI model computed over a few million documents.&lt;/p&gt;

&lt;p&gt;However we cannot train the LSI model on medical corpus and expect it to perform well on news corpus.&lt;/p&gt;

&lt;p&gt;Transformation of a document to fixed length feature vector using LSI can be considered as pre-processing step for other Machine learning algorithms.LSI process can be abstracted as a method to convert any text to fixed length vector representation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://en.wikipedia.org/wiki/Latent_semantic_analysis#Rank_lowering&lt;/li&gt;
  &lt;li&gt;https://radimrehurek.com/gensim/tut1.html&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>NLP spaCy Word and document vectors</title>
   <link href="http://localhost:4000/2017/05/13/spacy1/"/>
   <updated>2017-05-13T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/13/spacy1</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://spacy.io/&quot;&gt;spaCy&lt;/a&gt; is a library for advanced natural language processing in Python and Cython.&lt;/p&gt;

&lt;p&gt;A elementary step in NLP applications is to convert textual to mathematical reperations which can be processed
by various NLP alorithms.&lt;/p&gt;

&lt;p&gt;For automatic natural language processing, it’s often more effective to use dictionaries that define concepts in terms of their usage statistics&lt;/p&gt;

&lt;p&gt;The word2vec family of models are the most popular way of creating these dictionaries. Given a large sample of text, word2vec gives you a dictionary where each definition is just a row of, say, 300 floating-point numbers. To find out whether two entries in the dictionary are similar, you ask how similar their definitions are — a well-defined mathematical operation.&lt;/p&gt;

&lt;p&gt;Modern NLP techniques represent words by a fixed dimensional feature vector.The GloVe common crawl vectors have become a de facto standard for practical NLP. The default English model used by spacy uses vectors for one million vocabulary entries, using the 300-dimensional vectors trained on the Common Crawl corpus using the GloVe algorithm.&lt;/p&gt;

&lt;h1 id=&quot;word-vectors&quot;&gt;&lt;strong&gt;Word Vectors&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;below program gives the word vectors for each word/token in the sentence&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import spacy
nlp = spacy.load('en')

sentence = &quot;programming books are more better than others and&quot;
doc = nlp(sentence)
for word in doc:
    print (word.text, word.lemma, word.lemma_, word.tag, word.tag_, word.pos, word.pos_,word.has_vector,word.vector)
		
		
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By default, Token.vector returns the vector for its underlying words or tokens&lt;/p&gt;

&lt;h1 id=&quot;document-vectors&quot;&gt;&lt;strong&gt;Document Vectors&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Spacy also provides document vector by calculating the average of all the word/token vectors that occur in a sentense.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import spacy
nlp1 = spacy.load('en')

sentence = u&quot;programming books are more better than others and&quot;

doc = nlp1.tokenizer(sentence)

print(&quot;length of sentence vector is &quot;,len(doc.vector))
print(&quot;sentence vector is &quot;,doc.vector[0],doc.vector[1],doc.vector[2])
print(&quot;number of tokens are &quot;,len(doc))

v=np.array(300);

vec = []
for token in doc:
    if token.has_vector:
        vec.append(token.vector)

v1=np.sum(vec, axis=0) / len(vec)


print(v1[0],v1[1],v1[2])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we can see that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;doc.vector[0],doc.vector[1],doc.vector[2]&lt;/code&gt; is equal to value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(v1[0],v1[1],v1[2]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Although this is not the best method to compute the document vector ,it provides good performance in a lot of applications.&lt;/p&gt;

&lt;p&gt;By representing a document vector by a sum of word vectors we loose the local information of word vectors .The document vector captures the global information at the cost of local information.&lt;/p&gt;

&lt;p&gt;We will look at better ways to compute the document vectors in the future articles.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;https://spacy.io/&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>RASA NLU Trainer Frontend application</title>
   <link href="http://localhost:4000/2017/05/12/rasa1/"/>
   <updated>2017-05-12T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/12/rasa1</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://rasa.ai/&quot;&gt;rasa NLU&lt;/a&gt; (Natural Language Understanding) is a tool for intent classification and entity extraction&lt;/p&gt;

&lt;p&gt;The RASA NLU Trainer helps in creating training data files in a format suitable for intent and entity extraction&lt;/p&gt;

&lt;p&gt;To install the RASA NLU trainer,run the following commands&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$git clone https://github.com/golastmile/rasa-nlu-trainer.git
$npm i -g rasa-nlu-trainer
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To start the server&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$rasa-nlu-trainer 
server listening at http://localhost:36252/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will start the application at a random port on the server. you can access the editor in the browser which can be used to create the data file which is required for training&lt;/p&gt;

&lt;p&gt;Files can be saved on server and used for training RSA NLU or other equivalent systems&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://rasa.ai/&quot;&gt;rasa NLU&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;https://github.com/golastmile/rasa-nlu-trainer#installation&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Natural Language Processing Pre Processing Stemming,Lemmatization,Stop Words</title>
   <link href="http://localhost:4000/2017/05/12/nlp3/"/>
   <updated>2017-05-12T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/12/nlp3</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;The fundamental purpose  of Natural Language Processing is to do some form of analysis, or processing, where the machine can understand, at least to some level, what the text means, says, or implies.&lt;/p&gt;

&lt;p&gt;One of the Basic Steps in Nautral Language Processing pipeline is pre-processing the text.&lt;/p&gt;

&lt;p&gt;This article describes some pre-processing steps that are commonly used in Information Retrieval (IR), Natural Language Processing (NLP) and text analytics applications.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Stop words removal&lt;/li&gt;
  &lt;li&gt;Stemming&lt;/li&gt;
  &lt;li&gt;Lemmatisation&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;stop-word-removal&quot;&gt;&lt;strong&gt;Stop Word Removal&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Simply put stop words are words which we want to remove before performing any processing on the text.The nature of stop word are that these are high frequency words that occur in our text aren’t giving any additional information but bias the NLP task because they occur very frequently.&lt;/p&gt;

&lt;p&gt;For example if we want to find out the most important word in a piece of text,if you use a simple technique to infer what the text means if to count of words occuring in the text and this actually forms the basis of most advanced text analytics .
However if analyze the text you would find that the words like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;the&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;it&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;not&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;and&lt;/code&gt; would be at the top of the list.And this would be the case with any piece of text . This analysis would also give you these high frequency but irrelevant words as important words in your textual analysis and it would not enable you to differentiate once document from another since all the documents would contain these words at the top of the list.&lt;/p&gt;

&lt;p&gt;The stop word removal is a common sense approach to remove words that are irrelevant to the Natural language tasks being performed before starting with the actual NLP processing.&lt;/p&gt;

&lt;p&gt;There is no universal stop word list that can apply to all applications.Since NLP is often domain dependent ,the stop word list needs to be customized for you specific application and domain.&lt;/p&gt;

&lt;p&gt;Few of stop words defined in NLTK stop word list are as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from nltk.corpus import stopwords
stopWords = set(stopwords.words('english'))
print(&quot;number of stop words in NLTK &quot;+str(len(stopWords)))
print &quot; &quot;.join(map(unicode, stopWords))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;number of stop words in NLTK 153
all:just:being:over:both:through:yourselves:its:before:o:hadn:herself:ll:had:should:to:only:won:under:ours:has:do:them:his:very:they:not:during:now:him:nor:d:did:didn:this:she:each:further:where:few:because:doing:some:hasn:are:our:ourselves:out:what:for:while:re:does:above:between:mustn:t:be:we:who:were:here:shouldn:hers:by:on:about:couldn:of:against:s:isn:or:own:into:yourself:down:mightn:wasn:your:from:her:their:aren:there:been:whom:too:wouldn:themselves:weren:was:until:more:himself:that:but:don:with:than:those:he:me:myself:ma:these:up:will:below:ain:can:theirs:my:and:ve:then:is:am:it:doesn:an:as:itself:at:have:in:any:if:again:no:when:same:how:other:which:you:shan:needn:haven:after:most:such:why:a:off:i:m:yours:so:y:the:having:once
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Maybe for the application at hand you would want to words like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;won&lt;/code&gt; or you would want to have words like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;no,nor,not,of,off,on,once,only,or,other,ought,our,ours&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Following link contains a more exhaustive list of stop words&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://www.link-assistant.com/seo-stop-words.html&lt;/li&gt;
  &lt;li&gt;https://meta.wikimedia.org/wiki/Stop_word_list/consolidated_stop_word_list&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This may be applicable for search engines but for tasks like document or text classification or intent detection words like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;what,why&lt;/code&gt;when&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt; make important key words.For example to identifying and answering questions like &lt;/code&gt;who is rahul gandhi&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt; and &lt;/code&gt;what is a palindrome&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt; to understand these sentenses if we remove the stop words &lt;/code&gt;who&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt; and &lt;/code&gt;what` we may not be able to differentiate the statements.&lt;/p&gt;

&lt;p&gt;There are ways to include these words in stop word list and still achieve the task at hand.But main intent of above example is that you may customize the pre defined stop word lists in various NLU toolkits according to the NLP task and domain.&lt;/p&gt;

&lt;h1 id=&quot;stemming-and-lemmatization&quot;&gt;&lt;strong&gt;Stemming and Lemmatization&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Stemming&lt;/code&gt; is the process of reducing a word into its stem, i.e. its root form. The root form is not necessarily a word by itself, but it can be used to generate words by concatenating the right suffix&lt;/p&gt;

&lt;p&gt;For example,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the words fish, fishes and fishing all stem into fish&lt;/li&gt;
  &lt;li&gt;banks and banking become bank&lt;/li&gt;
  &lt;li&gt;investing and invested become invest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Lemmatization&lt;/code&gt; usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lemma&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The most common algorithm for stemming English, and one that has repeatedly been shown to be empirically very effective, is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Porter's algorithm&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize

ps = PorterStemmer()
sentence = &quot;programming books are more better than others&quot;
words = word_tokenize(sentence)

for word in words:
    print(word + &quot;:&quot; + ps.stem(word))
		
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output of the above program is as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;programming:program
books:book
are:are
more:more
better:better
than:than
others:other
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Lemmatization does full morphological analysis to accurately identify the lemma for each word.
Doing full morphological analysis produces at most very modest benefits for NLP.Both the forms of normalization tends not improve the NLP processing by a huge degree.It may help is some tasks but&lt;/p&gt;

&lt;p&gt;The NLTK Lemmatization method is based on WordNet’s built-in morphy function&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

sentence = &quot;programming books are more better than others&quot;
words = word_tokenize(sentence)

for word in words:
    print(word + &quot;:&quot; + wordnet_lemmatizer.lemmatize(word))
		
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output of the above program is&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;books:book
are:are
more:more
better:better
than:than
others:others
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If again we were to analyze the text by computing the relative frequency count of words.Without the stemming process the words fish,fishes,fishing would be treated as different words or banks and banking would be treated as different words though by grouping it together would provide a better analysis .&lt;/p&gt;

&lt;p&gt;By stemming them, it groups the frequencies of different inflection to just one term — in this case, invest&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;http://www.ranks.nl/stopwords&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Apache Thrift Basics with java application</title>
   <link href="http://localhost:4000/2017/05/08/aj1/"/>
   <updated>2017-05-08T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/08/aj1</id>
   <content type="html">&lt;p&gt;Apache Thrift is a RPC framework founded by facebook and now it is an Apache project. Thrift lets you define data types and service interfaces in a language neutral definition file. That definition file is used as the input for the compiler to generate code for building RPC clients and servers that communicate over different programming languages.&lt;/p&gt;

&lt;h1 id=&quot;installation&quot;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Pre Requisite software&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Run the following command to install the pre-requisite softwares&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install libboost-dev libboost-test-dev libboost-program-options-dev libboost-filesystem-dev libboost-thread-dev libevent-dev automake libtool flex bison pkg-config g++ libssl-dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Download Software&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Thrift software can be downloaded from  http://thrift.apache.org/download&lt;/li&gt;
  &lt;li&gt;Copy the downloaded file into the desired directory and untar the file&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tar -xvf thrift-0.10.0.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For an Ubuntu linux distribution you just need to go to the thrift directory and type:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure
make all
sudo ln -s /usr/local/lib/libthriftc.so /usr/lib/libthriftc.so
sudo ln -s /usr/local/lib/libthriftc.so.0 /usr/lib/libthriftc.so.0
sudo ln -s /usr/local/lib/libthrift-0.10.0.so /usr/lib/libthrift-0.10.0.so
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This installs the thrift code generation and other tools.&lt;/p&gt;

&lt;p&gt;To verify the install&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;thrift -version
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can use apache Thrift in the maven project by including the below pom dependency&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.apache.thrift&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;libthrift&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;0.10.0&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Writing a .thrift file&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In a .thrift file you can define the services that your server will implement and that they will be called by any clients. The Thrift compiler will read this file and generate source code to be used from the servers and clients you will write.&lt;/p&gt;

&lt;p&gt;The Thrift interface definition language (IDL) allows for the definition of Thrift Types. A Thrift IDL file is processed by the Thrift code generator to produce code for the various target languages to support the defined structs and services in the IDL file.&lt;/p&gt;

&lt;p&gt;Services are defined using Thrift types. Definition of a service is semantically equivalent to defining an interface (or a pure virtual abstract class) in object oriented programming. The Thrift compiler generates fully functional client and server stubs that implement the interface.&lt;/p&gt;

&lt;p&gt;A service consists of a set of named functions, each with a list of parameters and a return type.&lt;/p&gt;

&lt;p&gt;A simple .thrift file with which we define the UserLogin RPC is as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;namespace java com.pyvision

struct userLoginRequest
{
    1:string name,
    2:string password,
    3:string devicetoken
}

struct userLoginResponse
{
    1:string name,
    2:string loginToken
}


service loginRPC
{
        userLoginResponse multiply(1:userLoginResponse n1)
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This defines a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loginRPC&lt;/code&gt; RPC with input argument as userLoginRequest and output as userLoginResponse&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Generating the java file&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To generate source code from thrift file run the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;thrift --gen java Login.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will generate the java files for the thrift code&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Server Code&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; public class loginRPCImpl implements loginRPC.Iface 
     {

     @Override
     public userLoginResponse send(userLoginRequest request) throws TException {
            System.out.println(&quot;received RPC &quot;+request.getName());
            try {
                Thread.sleep(3000);
            } catch (InterruptedException ex) {
                Logger.getLogger(loginRPCImpl.class.getName()).log(Level.SEVERE, null, ex);
            }
          userLoginResponse r = new userLoginResponse();
          r.setName(request.getName());
          r.setLoginToken(&quot;1234&quot;);
          System.out.println(&quot;sending back reply RPC &quot;+request.getName());
          return r;
    }

    };
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loginRPC.Iface&lt;/code&gt; is abstract class for server side RPC which needs to be implemented&lt;/p&gt;

&lt;p&gt;We initialize the server as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                 loginRPCImpl rpc=new loginRPCImpl();
                
                loginRPC.Processor&amp;lt;loginRPC.Iface&amp;gt; processor = new loginRPC.Processor&amp;lt;loginRPC.Iface&amp;gt;(rpc);
								
								TServer server = new TThreadPoolServer(new TThreadPoolServer.Args(serverTransport).processor(processor));
								
								server.serve();
								
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The client side code is as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TTransport transport;
loginRPC.Client client;
transport = new TSocket(&quot;localhost&quot;, 7914);
transport.open();

TProtocol protocol = new TBinaryProtocol(transport);
client = new loginRPC.Client(protocol);    

userLoginRequest user=new userLoginRequest();
user.setDevicetoken(&quot;device1&quot;);
user.setName(&quot;user&quot;+count);
user.setPassword(&quot;1234&quot;);
						
userLoginResponse rr=client.send(user);

transport.close();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;**Apache Thrift RPC Architecture **&lt;/p&gt;

&lt;p&gt;In thrift architecture&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Protocol layer : provides serialization and de-serialization support.&lt;/li&gt;
  &lt;li&gt;Transport layer : provides is responsible for IPC communication&lt;/li&gt;
  &lt;li&gt;Processor : Reads data from the input, processes the data throught the Handler specified by the user and then writes the data to the output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The protocol and transport layer are part of the runtime library. This means that it is possible to define a service and change the protocol and transport without recompiling the code.&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;https://dzone.com/articles/apache-thrift-java-quickstart&lt;/li&gt;
  &lt;li&gt;http://thrift.apache.org/docs/install/debian&lt;/li&gt;
  &lt;li&gt;http://thrift.apache.org/static/files/thrift-20070401.pdf&lt;/li&gt;
  &lt;li&gt;http://jnb.ociweb.com/jnb/jnbJun2009.html&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Apache Avro RPC Framework using Netty Server</title>
   <link href="http://localhost:4000/2017/05/05/a1/"/>
   <updated>2017-05-05T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/05/a1</id>
   <content type="html">&lt;p&gt;Apache Avro is more than a data serialization framework. It provides support for RPC Mechanisms&lt;/p&gt;

&lt;p&gt;Avro protocols describe RPC interfaces. Like schemas, they are defined with JSON text.&lt;/p&gt;

&lt;p&gt;Let us consider example of UserLoginRPC&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
   &quot;namespace&quot;: &quot;com.pyvision&quot;,
   &quot;protocol&quot;: &quot;loginRPC&quot;,
   &quot;version&quot; : &quot;1.6.2&quot;,
   &quot;types&quot;: [
       {   &quot;name&quot;: &quot;userLoginRequest&quot;,&quot;type&quot;: &quot;record&quot;,
            &quot;fields&quot;: [
            {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;},
            {&quot;name&quot;: &quot;password&quot;, &quot;type&quot;: &quot;string&quot;},
            {&quot;name&quot;: &quot;devicetoken&quot;, &quot;type&quot;: &quot;string&quot;}
            ]
       },
       {
            &quot;name&quot;: &quot;userLoginResponse&quot;,&quot;type&quot;: &quot;record&quot;,   
            &quot;fields&quot;: [
               {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;},      
               {&quot;name&quot;: &quot;loginToken&quot;, &quot;type&quot;: &quot;string&quot;}
            ]
       }       
   ],
 &quot;messages&quot;: {
     &quot;send&quot;: {
         &quot;request&quot;: [{&quot;name&quot;: &quot;request&quot;, &quot;type&quot;: &quot;userLoginRequest&quot;}],
         &quot;response&quot;: &quot;userLoginResponse&quot;
     } 
 }
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;protocol - the name of the RPC&lt;/li&gt;
  &lt;li&gt;types - list of schema definitions of named types&lt;/li&gt;
  &lt;li&gt;messages - keys are message name and value is the message object&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The message object contains the attribute.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;request - containing the input to the RPC&lt;/li&gt;
  &lt;li&gt;response - contains the output to the RPC&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the above example the message name is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;send&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Generating the java files&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To generate the java files from the above schema files run the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;java -jar avro-tools-1.8.1.jar compile protocol Login.json .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will generate the java files&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
com/pyvision/loginRPC.java
com/pyvision/userLoginResponse.java
com/pyvision/userLoginRequest.java

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This contains the schema definitions for the input request and output response
and also the RPC file.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using the java File&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Netty is a NIO client server framework which enables quick and easy development of network applications such as protocol servers and clients.&lt;/p&gt;

&lt;p&gt;Avro provides support for using Netty framework to build networked applications&lt;/p&gt;

&lt;p&gt;The Avro Netty server listens on a port for events sent over by Avro RPC.Using Netty we need
to configure a port for each type of RPC.&lt;/p&gt;

&lt;p&gt;The code to create the server is&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private static void createServer() throws IOException {
        server = new NettyServer(new SpecificResponder(loginRPC.class, new loginRPCImpl()), new InetSocketAddress(65111));
        }
				
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The code to create a client&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
private static void connectClient() throws IOException
{
              client = new NettyTransceiver(new InetSocketAddress(65111));
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The code for creating RPC handler&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
public static class loginRPCImpl implements loginRPC {
    public userLoginResponse send(userLoginRequest request) throws AvroRemoteException {
      System.out.println(&quot;received RPC&quot;);
      userLoginResponse r = new userLoginResponse();
      r.setName(request.getName());
      r.setLoginToken(&quot;1234&quot;);
      return r;
    }

  }

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above code describes the&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;send&lt;/code&gt; RPC function. The loginRPC can contain many RPC functions defined in the protocol file.&lt;/p&gt;

&lt;p&gt;The code for calling the RPC is as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private static void sendRPC() throws AvroRemoteException, IOException
        {
            loginRPC proxy = (loginRPC) SpecificRequestor.getClient(loginRPC.class, client);
            System.out.println(&quot;AA&quot;+&quot; sending the rpc&quot;);
            userLoginRequest m=new userLoginRequest();
            m.setName(&quot;pi&quot;);
            m.setPassword(&quot;123&quot;);
            m.setDevicetoken(&quot;123&quot;);
            proxy.send(m);
            
        }
				
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above code should enable you to call the server RPC from the client and get a suitable response back.&lt;/p&gt;

&lt;p&gt;Multiple client threads are not supported by Netty.However they are supported by Jetty/HTTP mechanism
You cannot create two threads to call the server in the client application. They will cause issue while calling the server.&lt;/p&gt;

&lt;p&gt;In the later articles we will look at using the Avro RPC with Jetty framework that supports multiple threads&lt;/p&gt;

&lt;p&gt;This structure enables us to create login service as a microservice and expose the functionality as a RPC
which can be called by other frontend or backend componenets.&lt;/p&gt;

&lt;p&gt;RPC also send the data in encoded binary format but not encrypted.We will also look at methods to provide encryption security over RPC.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;http://avro.apache.org/docs/current/spec.html#Protocol+Declaration&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Apache Avro Data serialization Framework</title>
   <link href="http://localhost:4000/2017/05/04/a1/"/>
   <updated>2017-05-04T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/04/a1</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Data serialization is a mechanism to translate or serialize data into binary or textual form that can be transported over the network or store on some persisten storage .Serialization operation is performed by the sender/transmitter of the data&lt;/p&gt;

&lt;p&gt;Deserialization is the process of converting serialized binary data which is received over the network or from persistant storage to its original form . Deserialization of the data is performed by he Receiver of the data.&lt;/p&gt;

&lt;p&gt;Apache Avro is a language agnostic remote procedure call and data serialization framework.Avro is a preferred tool to serialize data in Hadoop.&lt;/p&gt;

&lt;h1 id=&quot;download&quot;&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Apache Avro jar file can be downloaded from the following link
https://avro.apache.org/releases.html&lt;/p&gt;

&lt;p&gt;You can select and download the library for any of the languages provided. In this tutorial, we use Java. Hence download the jar files &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;avro-1.8.1.jar&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;avro-tools-1.8.1.jar&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To work with Avro in Linux environment, download the following jar files −&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;avro-1.8.1.jar
avro-tools-1.8.1.jar
log4j-api-2.0-beta9.jar
og4j-core-2.0.beta9.jar.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also get the Avro library into your project using Maven&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      &amp;lt;dependency&amp;gt;
         &amp;lt;groupId&amp;gt;org.apache.avro&amp;lt;/groupId&amp;gt;
         &amp;lt;artifactId&amp;gt;avro&amp;lt;/artifactId&amp;gt;
         &amp;lt;version&amp;gt;1.8.1&amp;lt;/version&amp;gt;
      &amp;lt;/dependency&amp;gt;
	
      &amp;lt;dependency&amp;gt;
         &amp;lt;groupId&amp;gt;org.apache.avro&amp;lt;/groupId&amp;gt;
         &amp;lt;artifactId&amp;gt;avro-tools&amp;lt;/artifactId&amp;gt;
         &amp;lt;version&amp;gt;1.8.1&amp;lt;/version&amp;gt;
      &amp;lt;/dependency&amp;gt;
			
      &amp;lt;dependency&amp;gt;
         &amp;lt;groupId&amp;gt;org.apache.logging.log4j&amp;lt;/groupId&amp;gt;
         &amp;lt;artifactId&amp;gt;log4j-api&amp;lt;/artifactId&amp;gt;
         &amp;lt;version&amp;gt;2.7&amp;lt;/version&amp;gt;
      &amp;lt;/dependency&amp;gt;
	
      &amp;lt;dependency&amp;gt;
         &amp;lt;groupId&amp;gt;org.apache.logging.log4j&amp;lt;/groupId&amp;gt;
         &amp;lt;artifactId&amp;gt;log4j-core&amp;lt;/artifactId&amp;gt;
         &amp;lt;version&amp;gt;2.7&amp;lt;/version&amp;gt;
      &amp;lt;/dependency&amp;gt;
			
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;avro-schema&quot;&gt;&lt;strong&gt;Avro Schema&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Avro accepts schemas as input. Using these schemas, you can store serialized values in binary format using less space.&lt;/p&gt;

&lt;p&gt;Avro schema supports the following primitive data types&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Data type&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;null&lt;/td&gt;
      &lt;td&gt;Null is a type having no value&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;int&lt;/td&gt;
      &lt;td&gt;32-bit signed integer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;long&lt;/td&gt;
      &lt;td&gt;64-bit signed integer.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;float&lt;/td&gt;
      &lt;td&gt;single precision (32-bit) IEEE 754 floating-point number.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;double&lt;/td&gt;
      &lt;td&gt;double precision (64-bit) IEEE 754 floating-point number.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;bytes&lt;/td&gt;
      &lt;td&gt;sequence of 8-bit unsigned bytes.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;string&lt;/td&gt;
      &lt;td&gt;Unicode character sequence.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It also supports six complex data types like Records, Enums, Arrays, Maps, Unions, and Fixed&lt;/p&gt;

&lt;h2 id=&quot;record-&quot;&gt;**Record **&lt;/h2&gt;

&lt;p&gt;The Record datatype  contains 4 attributes&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;type - Describes document type
namespace − Describes the name of the namespace in which the object resides
name − Describes the schema name
fields − This is an attribute array which contains the following
         name − Describes the name of field
         type − Describes data type of field
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example of a schema for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;type record&lt;/code&gt; is as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
   &quot;type&quot; : &quot;record&quot;,
   &quot;namespace&quot; : &quot;Testing&quot;,
   &quot;name&quot; : &quot;Employee&quot;,
   &quot;fields&quot; : [
      { &quot;name&quot; : &quot;Name&quot; , &quot;type&quot; : &quot;string&quot; },
      { &quot;name&quot; : &quot;Age&quot; , &quot;type&quot; : &quot;int&quot; }
   ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;enum&quot;&gt;&lt;strong&gt;Enum&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;An enumeration is a list of items in a collection&lt;/p&gt;

&lt;p&gt;name − The value of this field holds the name of the enumeration.&lt;/p&gt;

&lt;p&gt;namespace − The value of this field contains the string that qualifies the name of the Enumeration.&lt;/p&gt;

&lt;p&gt;symbols − The value of this field holds the enum’s symbols as an array of names.&lt;/p&gt;

&lt;p&gt;Example schema for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enum type&lt;/code&gt; is  as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
   &quot;type&quot; : &quot;enum&quot;,
   &quot;name&quot; : &quot;Numbers&quot;, &quot;namespace&quot;: &quot;data&quot;, &quot;symbols&quot; : [ &quot;ONE&quot;, &quot;TWO&quot;, &quot;THREE&quot;, &quot;FOUR&quot; ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Arrays&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This data type defines an array field having a single attribute items. This items attribute specifies the type of items in the array.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{ &quot; type &quot; : &quot; array &quot;, &quot; items &quot; : &quot; int &quot; }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Maps&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The map data type is an array of key-value pairs. The values attribute holds the data type of the content of map. Avro map values are implicitly taken as strings&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{&quot;type&quot; : &quot;map&quot;, &quot;values&quot; : &quot;int&quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Unions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A union datatype is used whenever the field has one or more datatypes.&lt;/p&gt;

&lt;p&gt;They are represented as JSON arrays. For example, if a field that could be either an int or null, then the union is represented as [“int”, “null”].&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{ 
   &quot;type&quot; : &quot;record&quot;, 
   &quot;namespace&quot; : &quot;tutorialspoint&quot;, 
   &quot;name&quot; : &quot;empdetails &quot;, 
   &quot;fields&quot; : 
   [ 
      { &quot;name&quot; : &quot;experience&quot;, &quot;type&quot;: [&quot;int&quot;, &quot;null&quot;] }, { &quot;name&quot; : &quot;age&quot;, &quot;type&quot;: &quot;int&quot; } 
   ] 
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;generating-java-classes&quot;&gt;&lt;strong&gt;Generating Java Classes&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;One can read an Avro schema into the program either by generating a class corresponding to a schema or by using the parsers library&lt;/p&gt;

&lt;p&gt;For example let us consider the RPC for login function&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Defining the Schema&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The login data will require username,password and devicetoken to be passed to the server by the client.&lt;/p&gt;

&lt;p&gt;Let us define the schema file called “Login.json”&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
   &quot;namespace&quot;: &quot;com.pyvision&quot;,
   &quot;type&quot;: &quot;record&quot;,
   &quot;name&quot;: &quot;userLogin&quot;,
   &quot;fields&quot;: [
      {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;},
      {&quot;name&quot;: &quot;password&quot;, &quot;type&quot;: &quot;string&quot;},
      {&quot;name&quot;: &quot;devicetoken&quot;, &quot;type&quot;: &quot;string&quot;}
   ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Compiling the Schema&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After creating the Avro schema, we need to compile it using Avro tools.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;java -jar &amp;lt;path/to/avro-tools-x.x.x.jar&amp;gt; compile schema &amp;lt;path/to/schema-file&amp;gt; &amp;lt;destination-folder&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;java -jar avro-tools-1.8.1.jar compile schema Login.json .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will generate the java file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;com/pyvision/userLogin.java&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The namespace is the packagename and the java file generated is the name of schema record type&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Use Generated Java file&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;copy the directory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;com/pyvsion&lt;/code&gt; to your java maven project.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    userLogin user=new userLogin();
     user.setDevicetoken(&quot;device1&quot;);
     user.setName(&quot;user1&quot;);
     user.setPassword(&quot;1234&quot;);
				
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To converts Java objects into in-memory binary serialized format.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
DatumWriter&amp;lt;userLogin&amp;gt; empDatumWriter = new SpecificDatumWriter&amp;lt;userLogin&amp;gt;(userLogin.class);
DataFileWriter&amp;lt;userLogin&amp;gt; empFileWriter = new DataFileWriter&amp;lt;userLogin&amp;gt;(empDatumWriter);
empFileWriter.create(user.getSchema(),new File(&quot;userLogin.avro&quot;));
empFileWriter.append(user);
empFileWriter.close();

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To deserialize a class&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
DatumReader&amp;lt;userLogin&amp;gt; empDatumReader = new SpecificDatumReader&amp;lt;userLogin&amp;gt;(userLogin.class);
DataFileReader&amp;lt;userLogin&amp;gt; dataFileReader = new DataFileReader(new File(&quot;/tmp/userLogin.avro&quot;), empDatumReader);

userLogin em=null;
while(dataFileReader.hasNext()){

em=dataFileReader.next(em);
System.out.println(em);

}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The deserialized data is as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{&quot;name&quot;: &quot;user1&quot;, &quot;password&quot;: &quot;1234&quot;, &quot;devicetoken&quot;: &quot;device1&quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One can read an Avro schema into a program either by generating a class corresponding to a schema or by using the parsers library.In Avro, data is always stored with its corresponding schema. Therefore, we can always read a schema without code generation.&lt;/p&gt;

&lt;p&gt;Avro has support for following languages C,C++,java,php,perl,python,ruby,scala,Go,haskell&lt;/p&gt;

&lt;p&gt;Thus we can serialize and deserialize any data across applications developed in any of the above languages.&lt;/p&gt;

&lt;p&gt;As far as serialization goes it is similar to Thrift and Protocol Buffer.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;https://www.tutorialspoint.com/avro/avro_schemas.htm&lt;/li&gt;
  &lt;li&gt;https://www.tutorialspoint.com/avro/avro_schemas.htm&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Install Mantis Issue Tracker on Ubuntu 16.04</title>
   <link href="http://localhost:4000/2017/05/02/a1/"/>
   <updated>2017-05-02T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/05/02/a1</id>
   <content type="html">&lt;p&gt;MantisBT is an open source issue tracker.We will look at installation of Mantis on ubuntu 16.04 OS.&lt;/p&gt;

&lt;p&gt;You can download mantis from &lt;a href=&quot;https://www.mantisbt.org/download.php&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This download contains a zip file containg the php code for mantis&lt;/p&gt;

&lt;p&gt;Extract the zip file and Copy the directory to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/www/html/mantis&lt;/code&gt; directory&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo unzip mantisbt-2.4.0.zip
mv mantisbt-2.4.0 /var/www/html/mantis
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Pre Requisites&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;mantis requires the mbstring module of php .&lt;/p&gt;

&lt;p&gt;Run the following command to install and enable the same&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install libapache2-mod-php5
sudo apt-get install php5 libapache2-mod-php5
sudo php5enmod mbstring
sudo apt-get install php5-mysql
sudo apt-get install php5-xcache
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Access the url from the webbrowser to initiate the setup&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://localhost/mantis/admin/install.php
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Enter the server and database details and click on “Install/Upgrade” button to finish installation
This will do the necessary installation.&lt;/p&gt;

&lt;p&gt;To access mantis login page access the url&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://localhost/mantisbt/login_page.php
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The defaul admin username and password is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;administrator&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; respectively&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;http://www.mantisbt.org/docs/master/en-US/Admin_Guide/html-single/#admin.install.preinstall&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>asciinema Terminal Recording Installation and usage</title>
   <link href="http://localhost:4000/2017/04/24/a1/"/>
   <updated>2017-04-24T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/04/24/a1</id>
   <content type="html">&lt;p&gt;asciinema is a lightweight, purely text-based approach to terminal recording.&lt;/p&gt;

&lt;p&gt;To install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asciinema&lt;/code&gt; on ubuntu 16.04 run the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-add-repository ppa:zanchey/asciinema
sudo apt-get update
sudo apt-get install asciinema
sudo locale-gen en_US en_US.UTF-8
sudo dpkg-reconfigure locales
export LC_ALL=&quot;en_US.UTF-8&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Start Recording&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By running asciinema rec [filename] you start a new recording session&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@b3c403d0dff1:/# asciinema rec test1
~ Asciicast recording started.
~ Hit Ctrl-D or type &quot;exit&quot; to finish.
# ls
bin  boot  dev	etc  home  lib	lib32  lib64  media  mnt  opt  proc  root  run	sbin  srv  sys	test1  tmp  usr  var
# ps
  PID TTY          TIME CMD
  484 pts/0    00:00:00 sh
  485 pts/0    00:00:00 sh
  487 pts/0    00:00:00 ps
# exit
~ Asciicast recording finished.

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Playback Recording&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;you can playback the recording&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@b3c403d0dff1:/# asciinema play test1
# ls
bin  boot  dev	etc  home  lib	lib32  lib64  media  mnt  opt  proc  root  run	sbin  srv  sys	test1  tmp  usr  var
# ps
  PID TTY          TIME CMD
  484 pts/0    00:00:00 sh
  485 pts/0    00:00:00 sh
  487 pts/0    00:00:00 ps
# exit
root@b3c403d0dff1:/# 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see that play command replayed the command and also the corresponding output from the test1 file&lt;/p&gt;

&lt;p&gt;The output of recording is a json file which keeps on storing the command and corresponding output&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Creating an account&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To public the results on the cloud&lt;/p&gt;

&lt;p&gt;Create a account at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://asciinema.org/login/new&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To associate or aunthentica you account on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asciinema.org&lt;/code&gt; run the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@b3c403d0dff1:/# asciinema auth
Open the following URL in a browser to register your API token and assign any recorded asciicasts to your profile:
https://asciinema.org/connect/6e2e4cr4-hla2-1236e-863e-aje76d5e8b8f

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To upload the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asciicast&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@b3c403d0dff1:/# asciinema upload test1
https://asciinema.org/a/7bkwd4gyh0keo2c01uqej9syo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now you can see the session recording reflected on you online account&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://34.194.184.172/emotix/wp-content/uploads/2017/04/ggg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can view the recoding at&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://asciinema.org/a/7bkwd4gyh0keo2c01uqej9syo&quot;&gt;https://asciinema.org/a/7bkwd4gyh0keo2c01uqej9syo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;rererences&quot;&gt;&lt;strong&gt;Rererences&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://asciinema.org/&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Partitioning and Mounting disk on Ubuntu 16.04 using fdisk and gparted</title>
   <link href="http://localhost:4000/2017/04/22/a3/"/>
   <updated>2017-04-22T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/04/22/a3</id>
   <content type="html">&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fdisk&lt;/code&gt; command is a text-based utility for viewing and managing hard disk partitions on Linux. It’s one of the most powerful tools you can use to manage partitions.&lt;/p&gt;

&lt;p&gt;Many drives come with a single partition already set up, but all storage devices are just treated as a mass of unallocated, free space when they contain no partitions. To actually set up a file system and save any files to the drive, the drive needs a partition.&lt;/p&gt;

&lt;p&gt;Partitions are necessary because you can’t just start writing files to a blank drive. You must first create at least one container with a file system. After creating a partition, the partition is formatted with a file system — like the NTFS file system on Windows drives, FAT32 file system for removable drives, HFS+ file system on Mac computers, or the ext4 file system on Linux. Files are then written to that file system on the partition.&lt;/p&gt;

&lt;p&gt;The sudo fdisk -l commands lists the partitions on your system.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo fdisk -l

Disk /dev/xvda: 100 GiB, 107374182400 bytes, 209715200 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x974bb19a

Device     Boot  Start       End   Sectors  Size Id Type
/dev/xvda1 *      2048    526335    524288  256M 83 Linux
/dev/xvda2      526336 209715166 209188831 99.8G 83 Linux


Disk /dev/xvdc: 100 GiB, 107374182400 bytes, 209715200 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This shows that the disk &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/dev/xvda1&lt;/code&gt; has already been partitioned and contains 2 partitions
one a boot partition &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/dev/xvda1&lt;/code&gt; of 256MB and another linux parition &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/dev/xvda2&lt;/code&gt; of 99.8G&lt;/p&gt;

&lt;p&gt;The disk &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/dev/xvdc&lt;/code&gt; does not contain any partitions&lt;/p&gt;

&lt;p&gt;To create a partition we need to enter command mode
we do this by typing the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@server2:/home/pi# sudo fdisk /dev/xvdc

Welcome to fdisk (util-linux 2.27.1).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0xeb288d63.

Command (m for help): 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As seen we can see that since the device is not partitioned ,it does not contain any partition table&lt;/p&gt;

&lt;p&gt;Use the option &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt; to print the current partition table&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Command (m for help): p
Disk /dev/xvdc: 100 GiB, 107374182400 bytes, 209715200 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xeb288d63

Command (m for help): 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When partitioning, you’ll need to be aware of the difference between primary, extended, and logical partitions. A disk with a traditional partition table can only have up to four partitions. Extended and logical partitions are a way to get around this limitation.&lt;/p&gt;

&lt;p&gt;Each disk can have up to four primary partitions or three primary partitions and an extended partition. If you need four partitions or less, you can just create them as primary partitions.&lt;/p&gt;

&lt;p&gt;However, let’s say you want six partitions on a single drive. You’d have to create three primary partitions as well as an extended partition. The extended partition effectively functions as a container that allows you to create a larger amount of logical partitions. 
So, if you needed six partitions, you’d create three primary partitions, an extended partition, and then three logical partitions inside the extended partition.  You could also just create a single primary partition, an extended partition, and five logical partitions — you just can’t have more than four primary partitions at a time.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n command&lt;/code&gt; can be used to create a new partition.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You can create a logical or primary partition (l for logical or p for primary). A disk can only have four primary partitions.&lt;/li&gt;
  &lt;li&gt;Next, specify the sector of the disk you want the partition to start at. Press Enter to accept the default sector, which is the first free sector on the disk.&lt;/li&gt;
  &lt;li&gt;Last, specify the last sector of the partition on the disk. If you want to use up all available space after the initial sector, just press Enter&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Command (m for help): p
Disk /dev/xvdc: 100 GiB, 107374182400 bytes, 209715200 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xeb288d63

Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p): 

Using default response p.
Partition number (1-4, default 1): 
First sector (2048-209715199, default 2048): 
Last sector, +sectors or +size{K,M,G,T,P} (2048-209715199, default 209715199): 

Created a new partition 1 of type 'Linux' and of size 100 GiB.

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we can see that by running the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt; command again that linux partition of 100G space have been created
on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/dev/xvdc&lt;/code&gt; device&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Command (m for help): p
Disk /dev/xvdc: 100 GiB, 107374182400 bytes, 209715200 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xeb288d63

Device     Boot Start       End   Sectors  Size Id Type
/dev/xvdc1       2048 209715199 209713152  100G 83 Linux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Use w to write the changes you’ve made to disk.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Command (m for help): w
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Formatting a Partition&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You must format new partitions with a file system before you can use them. You can do this with the appropriate mkfs command.&lt;/p&gt;

&lt;p&gt;we will be formatting the partition as ext4 file system.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@server2:/home/pi# sudo mkfs.ext4 /dev/xvdc1 
mke2fs 1.42.13 (17-May-2015)
Creating filesystem with 26214144 4k blocks and 6553600 inodes
Filesystem UUID: 6aef433c-583d-4fd4-9e2b-33084414a4bd
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000, 23887872

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done

Writing superblocks and filesystem accounting information: done  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can also configure partitions using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gparted&lt;/code&gt; which is a graphical utility&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Launch the utility&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://34.194.184.172/emotix/wp-content/uploads/2017/04/f21-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create Parition Table&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;create parition Table by selecting option “Device-&amp;gt;New Partition Table” and selecting the default options&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://34.194.184.172/emotix/wp-content/uploads/2017/04/f23.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;**Create and Format the partition **&lt;/p&gt;

&lt;p&gt;Right Click on the unallocated partition and Select New to enter the menu to create and format the new partition&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://34.194.184.172/emotix/wp-content/uploads/2017/04/f27.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Enter the parition size ,whether its a primary or extended partition and filesystem type&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://34.194.184.172/emotix/wp-content/uploads/2017/04/f28.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Upon selecting the Add options,It will create the respective parition and also format it as per selected options&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://34.194.184.172/emotix/wp-content/uploads/2017/04/f29.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mount the parition&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;we can now mount the partition and start using it.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mount /dev/xvdc1 /tmp/storage1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If the computer is rebooted then we need to manually run the mount command&lt;/p&gt;

&lt;p&gt;To configure the system to auto mount the system we need to edit the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/fstab&lt;/code&gt; file
however icorrectly editing the files can lead to destructive changes,hence it is
better to use a graphical or commandline utility which edits the file&lt;/p&gt;

&lt;p&gt;we will be using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gnome-disks&lt;/code&gt; utility&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install gnome-disk-utility`
sudo gnome-disks
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Launch the utility&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://34.194.184.172/emotix/wp-content/uploads/2017/04/f12.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edit and save the mount options&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://34.194.184.172/emotix/wp-content/uploads/2017/04/f14.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://34.194.184.172/emotix/wp-content/uploads/2017/04/f16.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once the configuration is done we can see the following entry added in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/fstab&lt;/code&gt; file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/dev/disk/by-uuid/6aef433c-583d-4fd4-9e2b-33084414a4bd /opt/miko/storage/disk1 auto nosuid,nodev,nofail,x-gvfs-show 0 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now you can restart the compute and see that all the partitions are auto mounted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://www.howtogeek.com/106873/how-to-use-fdisk-to-manage-partitions-on-linux/&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Mounting NFS Share on ubuntu 16.04 Client</title>
   <link href="http://localhost:4000/2017/04/22/a1/"/>
   <updated>2017-04-22T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/04/22/a1</id>
   <content type="html">&lt;p&gt;We will look at the process of mounting the network file storage on ubuntu16.04 system client&lt;/p&gt;

&lt;p&gt;On the client server, we need to install a package called nfs-common, which provides NFS functionality without including unneeded server components. Again, we will refresh the local package index prior to installation to ensure that we have up-to-date information:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install nfs-common
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Creating the Mount Points on the Client&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to make the remote shares available on the client, we need to mount the host directory on an empty client directory.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir -p /tmp/disk2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Mounting the nfs share&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo mount nfs_host:/mount /nfs/home
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we can run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df -h&lt;/code&gt; to verify that the mount is accessible&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pi@server2:~$ df -h
Filesystem                                          Size   Used  Avail    Use% Mounted on
udev                                                     16G     0       16G       0%     /dev
tmpfs                                                   3.2G  305M  2.9G    10%  /run
/dev/xvda2                                         97G   36G    62G       37%  /
tmpfs                                                    16G     0       16G       0%   /dev/shm
tmpfs                                                   5.0M     0      5.0M     0%   /run/lock
tmpfs                                                    16G     0       16G       0%   /sys/fs/cgroup
/dev/xvda1                                         240M   83M 145M   37% /boot
tmpfs                                                   3.2G     0        3.2G      0%  /run/user/1000
/dev/xvdc1                                         99G     60M   94G       1%  /opt/miko/storage/disk1
nfs_host:/mount  500G  128K  500G   1% /opt/miko/storage/disk2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://wiki.qnap.com/wiki/Mounting_an_NFS_share_on_Ubuntu&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Glassfish Admin Password Management and Command Login without password</title>
   <link href="http://localhost:4000/2017/04/21/a5/"/>
   <updated>2017-04-21T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/04/21/a5</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;we can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asadmin&lt;/code&gt; utility to perform administrative tasks for Oracle GlassFish Server from the command line or from a script.&lt;/p&gt;

&lt;p&gt;Path to the asadmin Utility&lt;/p&gt;

&lt;p&gt;The asadmin utility is found in the glassfish installation directory&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;${glassfish_install}/bin/asadmin
asadmin&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Upon running the command you enter the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asadmin&amp;gt;&lt;/code&gt; console which enables to perform various administrative tasks&lt;/p&gt;

&lt;p&gt;Inorder to run any command the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;adadmin&lt;/code&gt; utility will ask admin username and password&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;adadmin&amp;gt;asadmin list-applications
Enter admin user name&amp;gt;  admin
Enter admin password for user &quot;admin&quot;&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The commands are executed once the username and password is validated&lt;/p&gt;

&lt;h2 id=&quot;change-the-admin-password&quot;&gt;&lt;strong&gt;Change The admin Password&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;To change the admin password run the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;asadmin&amp;gt; change-admin-password --user admin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;asadmin-login-without-password&quot;&gt;&lt;strong&gt;asadmin login without password&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;If we want automated scripts to handle various administrative task we need that password need not be entered from stdin&lt;/p&gt;

&lt;p&gt;Instead of typing the password at the command line, you can access the password for a command from a file such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;passwords.tx&lt;/code&gt;t.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--passwordfile&lt;/code&gt; option of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asadmin&lt;/code&gt; utility takes the name of the file that contains the passwords.&lt;/p&gt;

&lt;p&gt;The entry for a password in the file must have the AS_ADMIN_ prefix followed by the password name in uppercase letters.&lt;/p&gt;

&lt;p&gt;Example password file is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;AS_ADMIN_ prefix=1234
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example to run deploy command via scripts we can execute&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./asadmin deploy $war --user admin --passwordfile  /opt/password.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;http://ufasoli.blogspot.in/2013/07/glassfish-asadmin-without-password.html&lt;/li&gt;
  &lt;li&gt;http://docs.oracle.com/cd/E19776-01/820-4495/ghytn/index.html&lt;/li&gt;
  &lt;li&gt;https://docs.oracle.com/cd/E19798-01/821-1751/giobi/index.html&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Glassfish application deployment from commandline</title>
   <link href="http://localhost:4000/2017/04/21/a4/"/>
   <updated>2017-04-21T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/04/21/a4</id>
   <content type="html">&lt;p&gt;**Application Deployment **&lt;/p&gt;

&lt;p&gt;To deploy application from commandline we need to run the following commands&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;filename=/tmp/a.war
basename=`basename $war`
basename=&quot;${filename%.*}&quot;
context=/test
${GLASSFISH}/bin/asadmin --user admin --passwordfile  /opt/password.txt deploy --contextroot $context --name $bname $filename 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/opt/password.txt&lt;/code&gt; is the password file for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;admin&lt;/code&gt; user. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$filename&lt;/code&gt; is variable containing the war file&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Application Undeployment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To undeploy the application we can run the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;filename=/tmp/a.war
basename=`basename $war`
basename=&quot;${filename%.*}&quot;
context=/test
${GLASSFISH}/bin/asadmin --user admin --passwordfile  /opt/password.txt undeploy $basename 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Application Redeploy&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;filename=/tmp/a.war
basename=`basename $war`
basename=&quot;${filename%.*}&quot;
context=/test
adadmin --user admin --passwordfile  /opt/password.txt redeploy --contextroot $context --name $basename --properties keepSessions=false --force ${filename} 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://docs.oracle.com/cd/E19798-01/821-1750/gijmb/index.html
-https://blogs.oracle.com/alexismp/entry/glassfish_tip_have_your_application&lt;/li&gt;
  &lt;li&gt;https://docs.oracle.com/cd/E19798-01/821-1750/gijmq/index.html&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>How to Install and Configure Ansible on Ubuntu 16.04</title>
   <link href="http://localhost:4000/2017/04/20/a5/"/>
   <updated>2017-04-20T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/04/20/a5</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Configuration management systems are designed to make controlling large numbers of servers in easy for administrators and operations teams. They allow you to control many systems from one central location using automated way.&lt;/p&gt;

&lt;p&gt;Ansible works by configuring client machines from an computer with Ansible components installed and configured.
Any server that has an SSH port exposed can configured by Ansible.Ansible can interact with clients through either command line tools or through its configuration scripts called Playbooks.&lt;/p&gt;

&lt;h2 id=&quot;step-1--installing-ansible&quot;&gt;&lt;strong&gt;Step 1 — Installing Ansible&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Run the following commands to run ansible&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-add-repository ppa:ansible/ansible
sudo apt-get update
sudo apt-get install ansible
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;-step-2---configuring-ansible-hosts&quot;&gt;** Step 2 - Configuring Ansible Hosts**&lt;/h2&gt;

&lt;p&gt;ansible keeps track of the servers it controlls through a host/inventory files.The host/inventory file can be found at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/ansible/hosts&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Syntax of the host file is as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[group_name]
alias ansible_ssh_host=your_server_ip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The group_name is an organizational tag that lets you refer to any servers listed under it with one word. The alias is just a name to refer to that server.&lt;/p&gt;

&lt;p&gt;For example&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[dev]
beta_server ansible_ssh_host=192.0.2.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If the ssh is not running on the default port&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[dev]
beta_server ansible_port=5555 ansible_host=192.0.2.50
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can also specify ssh user&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[dev]
beta_server ansible_port=5555 ansible_host=192.0.2.50 ansible_user=root
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To add a lot of hosts  we can also specify patterns&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[webservers]
www[01:50].example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hosts can be in multiple groups and groups can configure parameters for all of their members.&lt;/p&gt;

&lt;p&gt;There are a host of other options like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ansible_ssh_private_key_file&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ansible_ssh_pass&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ansible_become&lt;/code&gt; which can be found at
http://docs.ansible.com/ansible/intro_inventory.html#hosts-and-groups&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Host Variables&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;host_vars&lt;/code&gt; directory can be used to store host variables that can later be used in configuration scripts &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ansible playbooks&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To create a host file for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;beta_server&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir /etc/ansible/host_vars
sudo vi /etc/ansible/host_vars/beta_server
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;you can place the configuration parameters in this file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
ansible_ssh_user:  root
ansible_port: 5555 
ansible_host: 192.0.2.50
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Similariy we can set configuration parameters for group by using group variables&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo mkdir /etc/ansible/group_vars
sudo nano /etc/ansible/group_vars/dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since ansible connects to client machine via ssh . It requires information regarding ssh user and password on the client machine&lt;/p&gt;

&lt;p&gt;We can configure ansible to user a specific user for all the servers in the dev group&lt;/p&gt;

&lt;p&gt;The contents of the file are&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
ansible_ssh_user:  root
ansible_port: 5555 
ansible_host: 192.0.2.50
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Instead of creating files,you can create directories named after group or hosts and ansible will read all the files within the directory&lt;/p&gt;

&lt;p&gt;Now let us assume you have ssh private key which enables you to take ssh sessions on respective server&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh -i access_key.pem user@hostname
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For steps to create the private key refer to the following link&lt;/p&gt;

&lt;p&gt;Let us configure the host variables for the beta server by creating &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sh_config&lt;/code&gt; file in the directory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/ansible/host_vars/dev2&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---

ansible_ssh_private_key_file: /etc/ansible/a.pem
ansible_host: 172.17.0.2
ansible_port: 22
ansible_ssh_user: root
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Following is the entry in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/ansible/hosts&lt;/code&gt; file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[dev-servers]
dev2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To test the communication with the client run the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@pi-dektop:/home/pi/ansible# ansible -m ping dev2
dev2 | SUCCESS =&amp;gt; {
    &quot;changed&quot;: false, 
    &quot;ping&quot;: &quot;pong&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-ansible-on-ubuntu-16-04&lt;/li&gt;
  &lt;li&gt;http://docs.ansible.com/ansible/intro_inventory.html#hosts-and-groups&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Docker Installation and setup on ubuntu OS</title>
   <link href="http://localhost:4000/2017/04/19/a5/"/>
   <updated>2017-04-19T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/04/19/a5</id>
   <content type="html">&lt;h1 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Docker is the world’s leading software container platform&lt;/p&gt;

&lt;p&gt;An image is a lightweight, stand-alone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, environment variables, and config files.&lt;/p&gt;

&lt;p&gt;A container is a runtime instance of an image – what the image becomes in memory when actually executed. It runs completely isolated from the host environment by default, only accessing host files and ports if configured to do so.&lt;/p&gt;

&lt;p&gt;Using containers has a lot of advantages especially from DevOps point of view&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using containers, everything required to make a piece of software run is packaged into isolated containers. Which makes replication of creation of run time environment for application very easy&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The purpose of containers is to provide efficient and lightweight self contained system which gurantees that the software will run regardless of where ever it is deployed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The container creation process also gurantees that several created containers are exactly identical.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Images can be created from containers which stores the container state in its enterity&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The purpose of containers is not to bundle full operating system though you can do the same within a container.Which can be user to replicate the container instances&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The containers run applications natively on the hosts machine kernel each one running in a discrete process taking no more resources than any other executables.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;install-docker&quot;&gt;&lt;strong&gt;Install Docker&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Unistall old version&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Uninstall older versions of Docker were called docker or docker-engine.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get remove docker docker-engine
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The contents of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/lib/docker/&lt;/code&gt;, including images, containers, volumes, and networks, are preserved. The Docker CE package is now called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-ce&lt;/code&gt;, and the Docker EE package is now called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-ee&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recommended extra packages for Trusty 14.04.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Install the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linux-image-extra-*&lt;/code&gt; packages, which allow Docker to use the aufs storage drivers.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get update

$ sudo apt-get install \
    linux-image-extra-$(uname -r) \
    linux-image-extra-virtual
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Install Pre Requisites softwares&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Add Docker’s official GPG key:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and verify that the key fingerprint is&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;by running the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-key fingerprint 0EBFCD88
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Adding the apt repository for docker&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;
		
sudo apt-get update

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Install docker&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install docker-ce=&amp;lt;VERSION&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;**Verify the docker installation **&lt;/p&gt;

&lt;p&gt;Verify that Docker CE or Docker EE is installed correctly by running the hello-world image.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo docker run hello-world
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This command downloads a test image and runs it in a container. When the container runs, it prints an informational message and exits.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Uninstall docker&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To uninstall docker run the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get purge docker-ce
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Images, containers, volumes, or customized configuration files on your host are not automatically removed. To delete all images, containers, and volumes:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo rm -rf /var/lib/docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Manage Docker as a non-root user&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The docker daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user root and other users can only access it using sudo. The docker daemon always runs as the root user.&lt;/p&gt;

&lt;p&gt;If you don’t want to use sudo when you use the docker command, create a Unix group called docker and add users to it. When the docker daemon starts, it makes the ownership of the Unix socket read/writable by the docker group.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create Docker Group&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To create the docker group and add your user:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo groupadd docker
sudo usermod -aG docker $USER
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Verify that you can run docker commands without sudo.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ docker run hello-world
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This command downloads a test image and runs it in a container. When the container runs, it prints an informational message and exits.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Docker Boot configuration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Most current Linux distributions (RHEL, CentOS, Fedora, Ubuntu 16.04 and higher) use systemd to manage which services start when the system boots. Ubuntu 14.10 and below use upstart.&lt;/p&gt;

&lt;p&gt;To enable or disable docker start at boot for systemd run the following commands respectively&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo systemctl enable docker
$ sudo systemctl disable docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Docker is automatically configured to start on boot using upstart&lt;/p&gt;

&lt;p&gt;To disable the behavior run the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo manual | sudo tee /etc/init/docker.override
sudo chkconfig docker on
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Docker service&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To start and stop the docker daemon run the following commands&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo service docker stop
sudo service docker start

or 

sudo systemctl start docker
sudo systemctl stop docker

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;https://www.docker.com/what-docker&lt;/li&gt;
  &lt;li&gt;https://docs.docker.com/get-started/#a-brief-explanation-of-containers&lt;/li&gt;
  &lt;li&gt;https://docs.docker.com/engine/installation/linux/ubuntu/#uninstall-docker&lt;/li&gt;
  &lt;li&gt;https://docs.docker.com/engine/installation/linux/linux-postinstall/#your-kernel-does-not-support-cgroup-swap-limit-capabilities&lt;/li&gt;
  &lt;li&gt;https://docs.docker.com/get-started/#setup&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Generating a pem file for auto login to ubuntu 14.04 server</title>
   <link href="http://localhost:4000/2017/04/18/a6/"/>
   <updated>2017-04-18T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/04/18/a6</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Generate the key&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ssh-keygen -t rsa -b 2048 -v 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and when asked to enter file in which to save the key, type my-certificate and when asked to enter passphrase, press Enter (empty passphrase) and confirm by Enter.&lt;/p&gt;

&lt;p&gt;This will generate my-certificate and my-certificate.pub files in the current folder&lt;/p&gt;

&lt;p&gt;rename the my-certificate to my-certificate.pem&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mv my-certificate my-certificate.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Upload the public certificate to to server:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-copy-id -i ~/my-certificate.pub username@ip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;this will ask you for the password to copy the public key file contents to /home/username/.ssh/authorized_keys&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test the connection&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;you can test the connection by running the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo ssh -i my-certificate.pem username@ip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Replace the username and ip as per your configuration&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;http://www.beginninglinux.com/home/server-administration/openssh-keys-certificates-authentication-pem-pub-crt&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Sudo Permissions to a script on ubuntu 14.04</title>
   <link href="http://localhost:4000/2017/04/18/a5/"/>
   <updated>2017-04-18T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/04/18/a5</id>
   <content type="html">&lt;p&gt;The sudoers file located at: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/sudoers&lt;/code&gt;, contains the rules that users must follow when using the sudo command.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/sudoers&lt;/code&gt; file controls who can run what commands as what users on what machines and can also control special things such as whether you need a password for particular commands. The file is composed of aliases (basically variables) and user specifications (which control who can run what).&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;visudo&lt;/code&gt; is utility to edit the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/sudoers&lt;/code&gt; file which enable us to configure permissions&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;User Specifications&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;User Specifications are where the sudoers file sets who can run what as who.&lt;/p&gt;

&lt;p&gt;Format for user specification is as follows&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;user list&amp;gt; &amp;lt;host list&amp;gt; = &amp;lt;operator list&amp;gt; &amp;lt;tag list&amp;gt; &amp;lt;command list&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let is consider the following configuration&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pi ALL=(root) NOPASSWD: ALL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This allows user &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pi&lt;/code&gt; to run any command as root user and it will not prompt for password from the user&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;beta-dev ALL=(root) /opt/test.ksh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This allows beta-dev users run the ` /opt/test.ksh` script giving root access for script execution&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;beta-dev ALL=(root) NOPASSWD /opt/test.ksh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This allows beta-dev users run the ` /opt/test.ksh` script giving root access for script execution
and this will not ask for a password from the user&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://www.garron.me/en/linux/visudo-command-sudoers-file-sudo-default-editor.html&lt;/li&gt;
  &lt;li&gt;https://help.ubuntu.com/community/Sudoers&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Adding Swap Space on Ubuntu 14.04</title>
   <link href="http://localhost:4000/linux/2016/12/23/Swapubuntu/"/>
   <updated>2016-12-23T00:00:00+00:00</updated>
   <id>http://localhost:4000/linux/2016/12/23/Swapubuntu</id>
   <content type="html">&lt;p&gt;Swap space in Linux can be used when a system requires more memory than it has been physically allocated. When swap space is enabled, Linux systems can swap infrequently used memory pages from physical memory to swap space (either a dedicated partition or a swap file in an existing file system) and free up that space for memory pages that require high speed access.&lt;/p&gt;

&lt;p&gt;To see if your instance is using swap space, you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;swapon -s&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;If you don’t see a swap volume listed with this command, you may need to enable swap space for the device. Check your available disks using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lsblk&lt;/code&gt; command.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@ip-172-31-55-140:/home/ubuntu# lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk 
└─xvda1 202:1    0   8G  0 part /
xvdf    202:80   0   4G  0 disk 
root@ip-172-31-55-140:/home/ubuntu# 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, the swap volume xvdf is available to the instance, but it is not enabled (notice that the MOUNTPOINT field is empty). You can enable the swap volume with the swapon command.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkswap /dev/xvdf
sudo swapon /dev/xvda3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You will also need to edit your /etc/fstab file so that this swap space is automatically enabled at every system boot.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/dev/xvdf      none    swap    sw  0       0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-store-swap-volumes.html&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Introduction to Apache Spark Lightweight Web Framework</title>
   <link href="http://localhost:4000/2016/12/23/ApacheSpak/"/>
   <updated>2016-12-23T00:00:00+00:00</updated>
   <id>http://localhost:4000/2016/12/23/ApacheSpak</id>
   <content type="html">&lt;p&gt;Introduction
Apache Spark is a Lightweight Web Framework .Spark Framework is a true micro Java web framework. Its total size is less than a megabyte, and to keep it lean and clean we decided to cut support for Java 7 in Spark 2. If you are stuck with Java 7 for whatever reason, you unfortunately have to have to use Spark 1.&lt;/p&gt;

&lt;p&gt;To get started create a maven project in your IDE&lt;/p&gt;

&lt;p&gt;Add the following dependency to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pom.xml&lt;/code&gt; file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;com.sparkjava&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spark-core&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;2.3&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Hello World&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import static spark.Spark.*;

public class HelloWorld {
    public static void main(String[] args) {
        get(&quot;/hello&quot;, (req, res) -&amp;gt; &quot;Hello World&quot;);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This sets up a route for hello and corresponding response of “Hello Word”&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://localhost:4567/hello
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In a typical RESTful application we expect to receive POST requests with json objects as part of the payload.&lt;/p&gt;

&lt;p&gt;As far as the communication protocol goes the data is just a text.&lt;/p&gt;

&lt;p&gt;Our job will be to check the code is well-formed JSON, that it corresponds to the expected structure, that the values are in the valid ranges, etc.&lt;/p&gt;

&lt;p&gt;The payload or the data format should be independent of the communication protocol.
In all the apis the data will be in form of a json.This leads to losse coupling of data and commnication protocol.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;http://sparkjava.com/download.html&lt;/li&gt;
  &lt;li&gt;https://sparktutorials.github.io/2015/04/02/hello-tutorials.html&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Tomcat7 installation on Ubuntu 14.04</title>
   <link href="http://localhost:4000/2016/12/22/Tomcatinstallation/"/>
   <updated>2016-12-22T00:00:00+00:00</updated>
   <id>http://localhost:4000/2016/12/22/Tomcatinstallation</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;tomcat7 installation&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install tomcat7
sudo apt-get install tomcat7-docs tomcat7-admin tomcat7-examples
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;**To start the service run **&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;service tomcat7 start
service tomcat7 stop
service tomcat7 restart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;tomcat7 is available by default on port 8080&lt;/p&gt;

&lt;p&gt;Tomcat is installed with CATALINA_HOME in /usr/share/tomcat7 and CATALINA_BASE in /var/lib/tomcat7, following the rules from /usr/share/doc/tomcat7-common/RUNNING.txt.gz.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tomcat-users.xml&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Tomcat configuration files are found in the directory: CATALINA_HOME/conf (where CATALINA_HOME environment variable is the Tomcat installation directory). The main configuration file is server.xml. tomcat-users.xml is one of the configuration files&lt;/p&gt;

&lt;p&gt;manager-gui roles needs to be added and assigned to a user to get access to tomcat admin panel&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;tomcat-users&amp;gt;
    &amp;lt;role rolename=&quot;manager-gui&quot;/&amp;gt;
    &amp;lt;role rolename=&quot;admin-gui&quot;/&amp;gt;
    &amp;lt;role rolename=&quot;tomcat&quot;/&amp;gt;
    &amp;lt;user password=&quot;password&quot; roles=&quot;manager-gui,admin-gui,admin&quot; username=&quot;admin&quot;/&amp;gt;
    &amp;lt;user password=&quot;tomcat&quot; roles=&quot;manager-gui,admin-gui,manager-script,admin&quot; username=&quot;tomcat&quot;/&amp;gt;
    &amp;lt;user password=&quot;tomcat7&quot; roles=&quot;manager-script,admin&quot; username=&quot;tomcat7&quot;/&amp;gt;
&amp;lt;/tomcat-users&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;**Create a softlink to conf directory **&lt;/p&gt;

&lt;p&gt;Some softwares exepct the conf directory to be present in the CATALINA_HOME but is present in CATALINA_BASE .In which case we need to create a softlink for the same&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ln -s /var/lib/tomcat7/conf /usr/share/tomcat7/conf
ln -s /var/lib/tomcat7/logs /usr/share/tomcat7/logs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Glassfish Installation on Ubuntu 14.04</title>
   <link href="http://localhost:4000/2016/12/22/GlassfishInstallation/"/>
   <updated>2016-12-22T00:00:00+00:00</updated>
   <id>http://localhost:4000/2016/12/22/GlassfishInstallation</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Download glassfish&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget http://download.java.net/glassfish/4.1/release/glassfish-4.1.zip
unzip glassfish-4.1.zip
rm -f glassfish-4.1.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create the following startup script&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#
# description: Startup script for Glassfish Application Server
# processname: glassfish
 
GLASSFISH_HOME=/opt/glassfish4/glassfish
export GLASSFISH_HOME
GLASSFISH_USER=glassfish
export GLASSFISH_USER
 
start() {
echo -n &quot;Starting Glassfish: &quot;
su $GLASSFISH_USER -c &quot;$GLASSFISH_HOME/bin/asadmin start-domain domain1&quot;
sleep 2
echo &quot;done&quot;
}
 
stop() {
echo -n &quot;Stopping Glassfish: &quot;
su $GLASSFISH_USER -c &quot;$GLASSFISH_HOME/bin/asadmin stop-domain domain1&quot;
echo &quot;done&quot;
}
 
case &quot;$1&quot; in
start)
start
;;
stop)
stop
;;
restart)
stop
start
;;
*)
echo $&quot;Usage: glassfish {start|stop|restart}&quot;
exit
esac
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Make it a executable&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;chmod 755 /etc/init.d/glassfish
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Configure the user&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;useradd glassfish
chown -R glassfish:glassfish /opt/glassfish4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Glassfish administration&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo /etc/init.d/glassfish start
### Stop GlassFish ###
sudo /etc/init.d/glassfish stop
### restart GlassFish ###
sudo /etc/init.d/glassfish restart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Accessing Glassfish&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;GlassFish will be available on HTTP port 8080 by default also port 4848 by administration. Open your favorite browser and navigate to http://yourdomain.com:8080 or http://server-ip:4848 and complete the required the steps to finish the installation. If you are using a firewall, please open port 8080 and 4848 to enable access to the control panel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Turn on system administration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To turn the remote administration on and access the GlassFish admin console via web browser, execute the following commands:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /opt/glassfish4/glassfish/bin
./asadmin --user admin
asadmin&amp;gt; change-admin-password
./asadmin enable-secure-admin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Default password is blank.You can reset the password as required&lt;/p&gt;

&lt;p&gt;** To deploy and undeploy a file from command line **&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /opt/glassfish4/glassfish/bin
./asadmin deploy war-name

./asadmin undeploy war-name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;http://idroot.net/tutorials/how-to-install-glassfish-on-ubuntu-14-04/&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Importing Mysql Database on ubuntu 14.04</title>
   <link href="http://localhost:4000/2016/12/20/mysqlimport/"/>
   <updated>2016-12-20T00:00:00+00:00</updated>
   <id>http://localhost:4000/2016/12/20/mysqlimport</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Create a database&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE DATABASE &amp;lt;DATABASENAME&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Importing a database&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mysql -u &amp;lt;username&amp;gt; -p&amp;lt;password&amp;gt; database_name &amp;lt; file.sql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>OrientDB Graph Database Installation on Ubuntu 14.04</title>
   <link href="http://localhost:4000/2016/12/13/OrientdbInstall/"/>
   <updated>2016-12-13T00:00:00+00:00</updated>
   <id>http://localhost:4000/2016/12/13/OrientdbInstall</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Pre-Requisites&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Java needs to be installed before processing with orientdb installation&lt;/p&gt;

&lt;p&gt;Run the following command to install openjdk&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install openjdk-8-jre
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;OrientDB Installation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Download orientDB from the following page http://orientdb.com/download/&lt;/p&gt;

&lt;p&gt;The orientdb database executables are contained within the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tar.gz&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Uncompress the file by running&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gunzip orientdb-community-2.2.13.tar.gz
tar -xvf orientdb-community-2.2.13.tar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will uncompress all the files in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;orientdb-community-2.2.13&lt;/code&gt; directory&lt;/p&gt;

&lt;p&gt;By default orientdb is only accessible to external ip address.&lt;/p&gt;

&lt;p&gt;To enable access to orientdb on localhost changes need to be made in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ORIENTDB_HOME/config/orientdb-server-config.xml&lt;/code&gt; file&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Starting the Server&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;to start the server run the script file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ORIENTDB_HOME/bin/server.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2016-12-13 09:55:54:382 INFO  Listening binary connections on 0.0.0.0:2424 (protocol v.36, socket=default) [OServerNetworkListener]
2016-12-13 09:55:54:389 INFO  Listening http connections on 0.0.0.0:2480 (protocol v.10, socket=default) [OServerNetworkListener]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;you can see that orientdb listeners are up on ports 2424 and 2480 .&lt;/p&gt;

&lt;p&gt;Port 2480 is a web application which gives access to orientdb via browser&lt;/p&gt;

&lt;p&gt;Port 2424 supports binary protocol which is used by API’s to communicate with orientdb.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Running Scripts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To run scripts execute the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ORIENDB_HOME/bin/console.sh myscript.osql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example script is given below&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;connect remote:localhost:2424/emotix admin admin
create class GraphNode IF NOT EXISTS extends V ;
create property GraphNode.name IF NOT EXISTS string;
create property GraphNode.type IF NOT EXISTS string;
create property GraphNode.description IF NOT EXISTS string;

CREATE INDEX GraphNode.name UNIQUE;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;To export and Import Databases&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For importing the database run the following script using console&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;IMPORT DATABASE /home/ubuntu/test/emtest -preserveClusterIDs=true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For exporting the database run the following script using the console&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;EXPORT DATABASE /temp/petshop.export
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;** To load a CSV file to orient db database **&lt;/p&gt;

&lt;p&gt;The orientdb ETL module is used to load data from various data sources transform it a format
suitable for orientdb and create suitable objects in orientdb database and store the information&lt;/p&gt;

&lt;p&gt;For example if we have a csv file for a entity type person containing name and description files&lt;/p&gt;

&lt;p&gt;we need to create a csv file in the following format&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;name,short_description
test , test is a person
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The we create a configuration file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;person.json&lt;/code&gt; as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;source&quot;: { &quot;file&quot;: { &quot;path&quot;: &quot;/home/pi/Documents/entity_person.csv&quot; } },
  &quot;extractor&quot;: { &quot;csv&quot;: {} },
  &quot;transformers&quot;: [
    { &quot;vertex&quot;: { &quot;class&quot;: &quot;entity_person&quot; } }
  ],
  &quot;loader&quot;: {
    &quot;orientdb&quot;: {
       &quot;dbURL&quot;: &quot;remote:localhost/test2&quot;,
	&quot;serverUser&quot;: &quot;root&quot;,
	&quot;serverPassword&quot;: &quot;reloded23&quot;,
       &quot;dbType&quot;: &quot;graph&quot;,
       &quot;classes&quot;: [
         {&quot;name&quot;: &quot;entity_person&quot;, &quot;extends&quot;: &quot;V&quot;}
       ], &quot;indexes&quot;: [
         {&quot;class&quot;:&quot;entity_person&quot;, &quot;fields&quot;:[&quot;name:STRING&quot;], &quot;type&quot;:&quot;UNIQUE_HASH_INDEX&quot; }
       ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which tell the ETL scripts to take the csv file from specificed location and reate a vertex of entity class
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enity_person&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It also specfies details of loader details on which database to load the data and access details for the same&lt;/p&gt;

&lt;p&gt;Finally loader specifies the vertex properties and indexes&lt;/p&gt;

&lt;p&gt;To load the data run the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
./oetl.sh person.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;http://orientdb.com/download/&lt;/li&gt;
  &lt;li&gt;http://orientdb.com/docs/2.2/Export-and-Import.html&lt;/li&gt;
  &lt;li&gt;http://orientdb.com/docs/2.2/Import-the-Database-of-Beers.html&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Install Docker on Ubuntu 14.04</title>
   <link href="http://localhost:4000/software%20installation/2016/07/26/dockerinstall/"/>
   <updated>2016-07-26T00:00:00+00:00</updated>
   <id>http://localhost:4000/software%20installation/2016/07/26/dockerinstall</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Install Docker&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Docker requires a 64-bit installation regardless of your Ubuntu version. Additionally, your kernel must be 3.10 at minimum. The latest 3.10 minor version or a newer maintained version are also acceptable.&lt;/p&gt;

&lt;p&gt;To check your current kernel version, open a terminal and use uname -r to display your kernel version:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;uname -r
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Update your apt sources&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Docker’s APT repository contains Docker 1.7.1 and higher. To set APT to use packages from the new repository:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install apt-transport-https ca-certificates
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Add the new GPG key.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Apt sources&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Add the line&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;deb https://apt.dockerproject.org/repo ubuntu-trusty main
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;to  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/apt/sources.list.d/docker.list&lt;/code&gt; file&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prerequisites by Ubuntu Version&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Install the following pre-requisites&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install linux-image-extra-$(uname -r)
sudo apt-get install apparmor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Install Docker&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Run the below command to install docker&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install docker-engine
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Start the docker daemon&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo service docker start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;verify the docker is installed properly&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo docker run hello-world
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Uninstallation&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; sudo apt-get purge docker-engine
 sudo apt-get autoremove --purge docker-engine
 rm -rf /var/lib/docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://docs.docker.com/engine/installation/linux/ubuntulinux/&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Ansible Installation on Ubuntu 14.04 and Installation of Netbeans IDE</title>
   <link href="http://localhost:4000/software%20installation/2016/07/26/ansibleinstall/"/>
   <updated>2016-07-26T00:00:00+00:00</updated>
   <id>http://localhost:4000/software%20installation/2016/07/26/ansibleinstall</id>
   <content type="html">&lt;h2 id=&quot;ansible-installation&quot;&gt;&lt;strong&gt;Ansible installation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In this guide, we will discuss how to install Ansible on an Ubuntu 14.04 machine and go over some basics of how to use the software.&lt;/p&gt;

&lt;p&gt;Ansible by default manages machines over the SSH protocol.&lt;/p&gt;

&lt;p&gt;Once Ansible is installed, it will not add a database, and there will be no daemons to start or keep running. You only need to install it on one machine (which could easily be a laptop) and it can manage an entire fleet of remote machines from that central point. When Ansible manages remote machines, it does not leave software installed or running on them, so there’s no real question about how to upgrade Ansible when moving to a new version.&lt;/p&gt;

&lt;p&gt;Currently Ansible can be run from any machine with Python 2.6 or 2.7 installed (Windows isn’t supported for the control machine).&lt;/p&gt;

&lt;p&gt;Ansible also uses the following Python modules that need to be installed&lt;/p&gt;

&lt;p&gt;To configure the PPA on your machine and install ansible run these commands:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$sudo apt-get update
$sudo apt-get install openssh-server
$sudo apt-get install software-properties-common
$sudo apt-add-repository ppa:ansible/ansible
$ sudo apt-get update
$ sudo apt-get install ansible

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;adding-ansible-clients&quot;&gt;&lt;strong&gt;Adding Ansible clients&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Now that you’ve installed Ansible, it’s time to get started with some basics.&lt;/p&gt;

&lt;p&gt;Edit (or create) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/ansible/hosts&lt;/code&gt; and put one or more remote systems in it. Your public SSH key should be located in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;authorized_keys&lt;/code&gt; on those systems:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;10.2.1.98
aserver.example.org
bserver.example.org
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To set up SSH agent to avoid retyping passwords, you can do:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-agent bash
ssh-add ~/.ssh/id_rsa
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We need to add the servers ssh public key on the clients authorized hosts&lt;/p&gt;

&lt;p&gt;To generate the ssh public/private key pair&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ssh-keygen
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Follow instructions to generate the keys&lt;/p&gt;

&lt;p&gt;Copy the servers public key  and enter in on the hosts
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;authorized_keys&lt;/code&gt; file on clients machine&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#on the host
cat ~/.ssh/id_rsa.pub
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCUUKR4c6AEubtkiVToENFyxMiOUQ+P2yjcJlo3167l5MFX7d4wfW0nhI1lteSdBVmVJL0ZRfNzD5EpzSCSLummXw5m0O4jNtEVweVNA1h57ogqELZ8p9JE0hsA2lLAPIFUmC3uBG1oK18o8rtaxNCdC/h575db9CeGB/lkL8PsnRLZzX/522BdNNzpgOQGPSArvl/ChHbZ2NgBHxXlRkxUQeFYbwPCamNa4BztDhvvJbjpyQQj5ULj7oiwE6BmNSVrsSO3QoI2I7RHKpfQNZwPTMlRE1V1h9r7UUcd/E96TMoRZXhDu8IzvWY8zNoyC4ErmCtKOkO8ocd/Xp+CkUEP root@gui-ubuntu

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add/append the text to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;authorized_keys&lt;/code&gt; file found in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~root/.ssh &lt;/code&gt; directory&lt;/p&gt;

&lt;p&gt;Now you should be able to log into the client systems via ssh without requiring any passwords&lt;/p&gt;

&lt;h2 id=&quot;running-first-ansible-command&quot;&gt;&lt;strong&gt;Running First Ansible command&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;This command should ping all the hosts configured in the  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/ansible/hosts&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ansible all -m ping
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we can also execute commands on the client machines by specifically mentioning client machine hostname/ipaddress (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost&lt;/code&gt;)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ansible localhost -a &quot;/bin/echo hello&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;ansible-playbooks&quot;&gt;&lt;strong&gt;Ansible Playbooks&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Since we can execute any command on the remote system,any predefined installation/configuration steps can be executed across several machines in an ordered and repeatable manner.&lt;/p&gt;

&lt;p&gt;Playbooks are Ansible’s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process.&lt;/p&gt;

&lt;p&gt;At a basic level, playbooks can be used to manage configurations of and deployments to remote machines.Playbooks are designed to be human-readable and are developed in a basic text language.&lt;/p&gt;

&lt;p&gt;Playbooks are expressed in YAML format (see YAML Syntax) and have a minimum of syntax, which intentionally tries to not be a programming language or script, but rather a model of a configuration or a process.&lt;/p&gt;

&lt;p&gt;We will look at using existing playbooks&lt;/p&gt;

&lt;h2 id=&quot;ansible-galaxy&quot;&gt;&lt;strong&gt;Ansible Galaxy&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Ansible galaxy hosts &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ansible roles&lt;/code&gt; for configurations for most softwares&lt;/p&gt;

&lt;p&gt;we will look at installing oracle JDK using ansible playbook&lt;/p&gt;

&lt;p&gt;https://galaxy.ansible.com/tersmitten/oracle-java/ provides details on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ansible roles&lt;/code&gt; for oracle JDK&lt;/p&gt;

&lt;p&gt;To fetch the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ansible roles &lt;/code&gt; on the host ansible machine&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ansible-galaxy install tersmitten.oracle-java
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example playbook file&lt;/p&gt;

&lt;p&gt;we write this sample playbook which which instructs ansible to install the role &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tersmitten.oracle-java&lt;/code&gt; on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost&lt;/code&gt; machine&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- hosts: servers
  roles:
     - { role: tersmitten.oracle-java }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;to run the playbook we run&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ansible-playbook orable_java.playbook
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will by default install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;oracle jdk7&lt;/code&gt; on the client machine&lt;/p&gt;

&lt;p&gt;Similarly by installing the roles and configuring playbook any software can be installed on client machine&lt;/p&gt;

&lt;p&gt;To install netbeans IDE&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ansible-galaxy install jeqo.netbeans
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Playbook file for the same&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- hosts: localhost 
  roles:
     - { role: jeqo.netbeans}
  vars :
    netbeans_version : 8.1 
    netbeans_base: &quot;\{\{ /opt/softwares/netbeans }}&quot;
    netbeans_home: &quot;/netbeans-&quot;


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can also pass variables via playbook if the roles are written to support them&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;http://docs.ansible.com/ansible/intro_getting_started.html&lt;/li&gt;
  &lt;li&gt;http://docs.ansible.com/ansible/intro_getting_started.html&lt;/li&gt;
  &lt;li&gt;http://docs.ansible.com/ansible/playbooks_intro.html&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Install and Configure Salt Master and Minion Servers on Ubuntu 14.04</title>
   <link href="http://localhost:4000/software%20installation/2016/07/25/saltInstallation/"/>
   <updated>2016-07-25T00:00:00+00:00</updated>
   <id>http://localhost:4000/software%20installation/2016/07/25/saltInstallation</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SaltStack&lt;/code&gt; is a powerful, flexible, high performing configuration management and remote execution system. It can be used manage your infrastructure from a centralized location while minimizing manual maintenance steps.&lt;/p&gt;

&lt;p&gt;If you are setting up your environment for the first time, you should install a Salt master on a dedicated management server or VM, and then install a Salt minion on each system that you want to manage using Salt.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Install the Master Daemon&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Packages for Ubuntu are also published in the saltstack PPA. If you have the add-apt-repository utility, you can add the repository and import the key in one step:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install python-software-properties
sudo apt-get install software-properties-common

echo deb http://repo.saltstack.com/apt/ubuntu/14.04/amd64/latest trusty main | sudo tee /etc/apt/sources.list.d/saltstack.list 
wget -O - https://repo.saltstack.com/apt/ubuntu/14.04/amd64/latest/SALTSTACK-GPG-KEY.pub | sudo apt-key add -
sudo apt-get update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Install the salt-minion, salt-master, or other Salt components:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install salt-master
sudo apt-get install salt-minion
sudo apt-get install salt-ssh
sudo apt-get install salt-syndic
sudo apt-get install salt-cloud
sudo apt-get install salt-api
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;docker-salt-master-configuration&quot;&gt;&lt;strong&gt;Docker Salt master Configuration&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;We will use a docker image for salt-master.&lt;/p&gt;

&lt;p&gt;Download the salt-master docker image from https://hub.docker.com/r/bbinet/salt-master/&lt;/p&gt;

&lt;p&gt;Run the below command to start the salt-master docker image&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -d --name klugadmin-salt-master \
    -v /home/klugadmin/pi/docker/salt-master/config:/config \
    -v /home/klugadmin/pi/docker/salt-master/data:/data \
    -p 4505:4505 \
    -p 4506:4506 \
    -p 443:443 \
    bbinet/salt-master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To login to salt master shell in docker container run the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo docker exec -i -t klugadmin-salt-server /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Get the Salt Master Public Key Fingerprint&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before we begin, we should grab the Salt master’s key fingerprint. We can add this to our minion configuration for increased security.&lt;/p&gt;

&lt;p&gt;On your Salt master server, type:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo salt-key -F master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output should look something like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@ecf1f39f73a3:/# salt-key -F master
Local Keys:
master.pem:  f3:67:2a:06:a2:70:21:f9:3f:c0:8c:0f:cc:e6:5e:41
master.pub:  7b:33:b2:4e:a0:f4:cd:b8:b4:4f:de:02:a7:26:d5:bc
Accepted Keys:
klugserver:  c8:de:11:56:a7:03:76:58:fd:43:ed:3e:61:d4:48:7a
root@ecf1f39f73a3:/#
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The value of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;master.pub key&lt;/code&gt;, located under the “Local Keys” section is the fingerprint we are looking for. Copy this value to use in our Minion configuration.&lt;/p&gt;

&lt;h2 id=&quot;salt-minion-installation-and-configuration&quot;&gt;&lt;strong&gt;Salt-Minion Installation and configuration&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;We will install the salt-minion on each of the systems that needs to be managed&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modify the Minion Configuration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Back on your new Salt minion, open the minion configuration file with sudo privileges:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo vi/etc/salt/minion
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We need to specify the location where the Salt master can be found. This can either be a resolvable DNS domain name or an IP address:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/etc/salt/minion
master: ip_of_salt_master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, set the master_finger option to the fingerprint value you copied from the Salt master a moment ago:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/etc/salt/minion
master_finger: '7b:33:b2:4e:a0:f4:cd:b8:b4:4f:de:02:a7:26:d5:bc'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Save and close the file when you are finished.&lt;/p&gt;

&lt;p&gt;Now, restart the Salt minion daemon to implement your new configuration changes:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo restart salt-minion
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The new minion should contact the Salt master service at the provided address.&lt;/p&gt;

&lt;p&gt;It will then send its key for the master to accept. In order to securely verify the key, need to check the key fingerprint on the new minion server.&lt;/p&gt;

&lt;p&gt;To do this, type:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo salt-call key.finger --local
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;You should see output that looks like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;klugadmin@klugserver:~$ sudo salt-call key.finger --local
local:
    c8:de:11:56:a7:03:76:58:fd:43:ed:3e:61:d4:48:7a
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You will need to verify that the key fingerprint that the master server received matches this value.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Accept the Minion Key on the Salt Master&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Back on your Salt master server, we need to accept the key.&lt;/p&gt;

&lt;p&gt;First, verify that we have an unaccepted key waiting on the master:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo salt-key --list all
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You should see a new key in the “Unaccepted Keys” section that is associated with your new minion:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Output
Accepted Keys:
klugserver
Denied Keys:
Unaccepted Keys:
saltminion
Rejected Keys:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check the fingerprint of the new key. Modify the highlighted portion below with the minion ID that you see in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Unaccepted Keys&lt;/code&gt; section:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo salt-key -f klugserver
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output should look something like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Output
Unaccepted Keys:
klugserver:  c8:de:11:56:a7:03:76:58:fd:43:ed:3e:61:d4:48:7a
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If this matches the value you received from the minion when issuing the salt-call command, you can safely accept the key by typing:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo salt-key -a klugserver
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key should now be added to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Accepted Keys&lt;/code&gt; section:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo salt-key --list all
Output
Accepted Keys:
klugserver
Denied Keys:
Unaccepted Keys:
Rejected Keys:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test that you can send commands to your new minion by typing:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@ecf1f39f73a3:/# salt '*' test.ping
klugserver:
    True
root@ecf1f39f73a3:/#
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You should receive back answers from the minion daemons you’ve configured:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@ecf1f39f73a3:/# salt '*' test.ping
klugserver:
    True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You should now have a Salt master server configured to control your infrastructure. We’ve also walked through the process of setting up a new minion server. You can follow this same procedure for additional Salt minions. These are the basic skills you need to set up new infrastructure for Salt management.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://docs.saltstack.com/en/latest/topics/installation/index.html&lt;/li&gt;
  &lt;li&gt;https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-salt-master-and-minion-servers-on-ubuntu-14-04&lt;/li&gt;
  &lt;li&gt;https://www.openstack.org/summit/tokyo-2015/videos/presentation/chef-vs-puppet-vs-ansible-vs-salt-whats-best-for-deploying-and-managing-openstack&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Redis Mass Insertion of Data</title>
   <link href="http://localhost:4000/2016/06/24/redisinsert/"/>
   <updated>2016-06-24T00:00:00+00:00</updated>
   <id>http://localhost:4000/2016/06/24/redisinsert</id>
   <content type="html">&lt;h1 id=&quot;redis-mass-insert&quot;&gt;&lt;strong&gt;Redis Mass Insert&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Sometimes Redis instances need to be loaded with a big amount of preexisting or user generated data in a short amount of time, so that millions of keys will be created as fast as possible&lt;/p&gt;

&lt;p&gt;Using a normal Redis client to perform mass insertion is not a good idea for a few reasons: the naive approach of sending one command after the other is slow because you have to pay for the round trip time for every command&lt;/p&gt;

&lt;p&gt;Redis clients communicate with the Redis server using a protocol called RESP (REdis Serialization Protocol).
Mass insertion can be performed using redis protocol .The Redis protocol is extremely simple to generate and parse&lt;/p&gt;

&lt;p&gt;Every redis command is represented in the following way:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Redis command format&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
*&amp;lt;args&amp;gt;&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
$&amp;lt;len&amp;gt;&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
&amp;lt;arg0&amp;gt;&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
&amp;lt;arg1&amp;gt;&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
...
&amp;lt;argN&amp;gt;&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Where &lt;cr&gt; means &quot;\r&quot; (or ASCII character 13) and &lt;lf&gt; means &quot;\n&quot; (or ASCII character 10).&lt;/lf&gt;&lt;/cr&gt;&lt;/p&gt;

&lt;p&gt;For instance the command SET key value is represented by the following protocol:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
*3&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
$3&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
SET&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
$3&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
key&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
$5&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
value&amp;lt;cr&amp;gt;&amp;lt;lf&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or represented as a quoted string:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;*3\r\n$3\r\nSET\r\n$3\r\nkey\r\n$5\r\nvalue\r\n&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Python code to generate redis protocol&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The python code to convert SET command to redis protocol string&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def gen_redis_proto(cmd,key,value):
    proto = &quot;&quot;

    proto=&quot;*3&quot;+&quot;\r\n&quot;

    #for arg in args1:
    arg=cmd
    proto =proto+&quot;$&quot;+str(len(arg))+&quot;\r\n&quot;
    proto =proto+arg+&quot;\r\n&quot;

    arg=key
    proto =proto+&quot;$&quot;+str(len(arg))+&quot;\r\n&quot;
    proto =proto+arg+&quot;\r\n&quot;

    arg=value
    proto =proto+&quot;$&quot;+str(len(arg))+&quot;\r\n&quot;
    proto =proto+arg+&quot;\r\n&quot;

    return proto
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;proto=gen_redis_proto(&quot;SET&quot;,&quot;key,&quot;value&quot;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The string is written to the text file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;*3
$3
SET
$3
key
$5
value
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Loading the data to redis database&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Loading the data to redis using command line utility&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat test.txt | redis-cli --pipe
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://redis.io/topics/mass-insert&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Artifactory and Maven repository Setup</title>
   <link href="http://localhost:4000/software%20installation/2016/05/10/aristofactormaven/"/>
   <updated>2016-05-10T00:00:00+00:00</updated>
   <id>http://localhost:4000/software%20installation/2016/05/10/aristofactormaven</id>
   <content type="html">&lt;h2 id=&quot;artifactory-and-maven-repository-setup&quot;&gt;&lt;strong&gt;Artifactory and Maven repository Setup&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In this article we will look at maven repository management using Artifactory repository manager.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Maven is a popular build tool available for java developers.&lt;/p&gt;

&lt;p&gt;The purpose of maven repository is to serve as an internal private repository of all software libraries used within an organization.&lt;/p&gt;

&lt;p&gt;The maven stores all the software libraries in a common remote store called a repository  which helps to provide a  single central reference repository of all dependent software libraries rather than several independent local libraries and reduce the duplication of dependent software libraries (jars) required to build an application .&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Types of maven repository&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Local repository&lt;/strong&gt; – exists on developers machine and is maintained by the developer. It is in sync with the maven repositories defined in the ‘settings.xml’ in their ‘~home/.m2’ folder.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Private remote internal repository&lt;/strong&gt; – This the repository which we will setup. We will change the maven &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pom.xml&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;settings.xml&lt;/code&gt; to use this repository&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Public remote external repository&lt;/strong&gt; – This is the public external repository at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ibiblio.org&lt;/code&gt;. By default, maven synchronizes with this repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Creating a maven repository&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This post will demonstrate how to create your own Maven2 repository and put your piece of work there.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JFrog Artifactory&lt;/code&gt; is one of the leading open source Maven repository managers&lt;/p&gt;

&lt;p&gt;This can be downloaded from 
https://www.jfrog.com/open-source/#os-arti&lt;/p&gt;

&lt;p&gt;The downloaded zip files contains the web server and can be run without any other pre requisites.&lt;/p&gt;

&lt;p&gt;However you can take the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;artifactor.war&lt;/code&gt; from the webapps directory and deploy it only your web server .&lt;/p&gt;

&lt;p&gt;The application will be deployed with default context root of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/artifactory&lt;/code&gt; and in our case the URL is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://10.0.0.15:8090/artifactory/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The default user name and password for artifactory is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;admin:password&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;artifactory manager&lt;/code&gt; will provide us settings that we need to use to access the repository as target to upload/deploy user defined library as well as to access or source of dependent libraries during build.&lt;/p&gt;

&lt;p&gt;Maven is configured using a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;settings.xml&lt;/code&gt; file located under your Maven home directory.&lt;/p&gt;

&lt;p&gt;This will typically be&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/home/{user.name}/.m2/settings.xml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To work with Artifactory you need to configure Maven to perform the following two steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deploy artifacts to repositories through Artifactory&lt;/li&gt;
  &lt;li&gt;Resolve artifacts through Artifactory&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Resolve artifacts through Artifactory&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To configure Maven to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resolve artifacts&lt;/code&gt; through Artifactory you need to modify the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;settings.xml&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To make it easy for you to configure Maven to work with Artifactory, Artifactory can automatically generate a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;settings.xml&lt;/code&gt; file which you can save under your Maven home directory.&lt;/p&gt;

&lt;p&gt;The definitions in the generated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;settings.xml&lt;/code&gt; file override the default central and snapshot repositories of Maven.&lt;/p&gt;

&lt;p&gt;In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Artifact Repository Browser&lt;/code&gt; of the Artifacts module, select &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Set Me Up&lt;/code&gt;. In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Set Me Up&lt;/code&gt; dialog, set Maven in the Tool field and click “Generate Maven Settings”. You can now specify the repositories you want to configure for Maven.&lt;/p&gt;

&lt;p&gt;Insert the contents into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;settings.xml&lt;/code&gt; file on the build machine&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/YquSPQX.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The password inserted in maven in encrypted
to find the encrypted password we need to first create master password&lt;/p&gt;

&lt;p&gt;authorized users have an additional settings-security.xml file in their ~/.m2 folder in which master password is placed&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mvn --encrypt-master-password &amp;lt;password&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Store the output of the above command  in the ~/.m2/settings-security.xml&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;settingsSecurity&amp;gt;
  &amp;lt;master&amp;gt;{jSMOWnoPFgsHVpMvz5VrIt5kRbzGpI8u+9EF1iFQyJQ=}&amp;lt;/master&amp;gt;
&amp;lt;/settingsSecurity&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To create user password&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mvn --encrypt-password &amp;lt;password&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will be used by unauthorized users to access the maven repository&lt;/p&gt;

&lt;p&gt;Paste the password into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;password&lt;/code&gt; section of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;settings.xml&lt;/code&gt; file.Choose the appropriate user created in Artifactory .&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deploy 3rd party libraries&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To deploy third party libraries that you need for development environment In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Artifacts module Tree Browser&lt;/code&gt; select the repository you want to deploy  and select on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deploy&lt;/code&gt; .&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ogypEg1.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Provide the path to the required jar files and&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/OxApSqj.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once the jar file is uploaded/deployed to the repository enter the required details like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;groupId&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;artifactId&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;version&lt;/code&gt; etc&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/ZoWSOZP.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally click on deploy to upload the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jar&lt;/code&gt; file to the repository.&lt;/p&gt;

&lt;p&gt;Once the download is complete you can browse the repository tree and navigate to the uploaded jar file.In present case the jar files are uploaded to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libs-release-local-repository&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In the dependency declaration section.You can find maven commands to be added to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pom.xml&lt;/code&gt; inorder to add the dependency to the project.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/OWyIUKV.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Remember that you can not deploy build artifacts to remote or virtual repositories, so you should not use them in a deployment element.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deploy as part of build process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To deploy build artifacts through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Artifactory&lt;/code&gt; you must add a deployment element with the URL of a target local repository to which you want to deploy your artifacts.&lt;/p&gt;

&lt;p&gt;To make this easier, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Artifactory&lt;/code&gt; displays a code snippet that you can use as your deployment element. In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Artifacts module Tree Browser&lt;/code&gt; select the repository you want to deploy to and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;click Set Me UP&lt;/code&gt;. The code snippet is displayed under Deploy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/hNMhcVp.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Copy the entire section and add to the project &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pom.xml&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Once you build the project,it will be automatically deployed onto the maven repository and accessible to other users as well.&lt;/p&gt;

&lt;p&gt;If not go to the project directory and run the commmand&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mvn deploy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://maven.apache.org/guides/mini/guide-encryption.html&lt;/li&gt;
  &lt;li&gt;https://devcenter.heroku.com/articles/using-a-custom-maven-settings-xml&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Samba File Server Setup</title>
   <link href="http://localhost:4000/software%20installation/2016/04/24/Samba1/"/>
   <updated>2016-04-24T00:00:00+00:00</updated>
   <id>http://localhost:4000/software%20installation/2016/04/24/Samba1</id>
   <content type="html">&lt;h2 id=&quot;samba-file-server&quot;&gt;&lt;strong&gt;Samba File Server&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;One of the most common ways to network Ubuntu and Windows computers is to configure Samba as a File Server. This section covers setting up a Samba server to share files with Windows clients.&lt;/p&gt;

&lt;p&gt;The server will be configured to share files with any client on the network without prompting for a password&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The first step is to install the samba package. From a terminal prompt enter:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install samba

sudo apt-get install libpam-smbpass samba-dsdb-modules samba-vfs-modules python-samba samba-common-bin samba-common samba-libs nautilus-share libgnomevfs2-extra

sudo apt-get install cifs-utils samba-common system-config-samba winbind

sudo apt-get install libsmbcliet libsmbclient-dev libtevent0 libtalloc2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s all there is to it; you are now ready to configure Samba to share files.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The main Samba configuration file is located in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/samba/smb.conf&lt;/code&gt;. The default configuration file has a significant amount of comments in order to document various configuration directives.&lt;/p&gt;

&lt;p&gt;To add a new directory to share over the network.Add the following section to the configuration file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[share]
   comment = share
   browseable = yes
   path = /mnt/path
   guest ok = no
   read only = no
   writable=yes
   create mask = 0777
   valid users = klugadmin


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;browsable: enables Windows clients to browse the shared directory using Windows Explorer.&lt;/li&gt;
  &lt;li&gt;guest ok: allows clients to connect to the share without supplying a password.&lt;/li&gt;
  &lt;li&gt;create mask: determines the permissions new files will have when created.&lt;/li&gt;
  &lt;li&gt;read only: determines if the share is read only or if write privileges are granted.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Restart the samba service to enable the new configuration&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo restart smbd
sudo restart nmbd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From a Windows client you should now be able to browse to the Ubuntu file server and see the shared directory. If your client doesn’t show your share automatically, try to access your server by its IP address, e.g. \192.168.1.1, in a Windows Explorer window&lt;/p&gt;

&lt;p&gt;you can also map the network location using the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;net use e: \\10.0.0.15\share
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Samba File Server&lt;/strong&gt;
———————&lt;/p&gt;

&lt;p&gt;One of the most common ways to network Ubuntu and Windows computers is to configure Samba as a File Server. This section covers setting up a Samba server to share files with Windows clients.&lt;/p&gt;

&lt;p&gt;The server will be configured to share files with any client on the network without prompting for a password&lt;/p&gt;

&lt;h2 id=&quot;installation-1&quot;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The first step is to install the samba package. From a terminal prompt enter:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install samba

sudo apt-get install libpam-smbpass samba-dsdb-modules samba-vfs-modules python-samba samba-common-bin samba-common samba-libs nautilus-share libgnomevfs2-extra

sudo apt-get install cifs-utils samba-common system-config-samba winbind

sudo apt-get install libsmbcliet libsmbclient-dev libtevent0 libtalloc2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s all there is to it; you are now ready to configure Samba to share files.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The main Samba configuration file is located in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/samba/smb.conf&lt;/code&gt;. The default configuration file has a significant amount of comments in order to document various configuration directives.&lt;/p&gt;

&lt;p&gt;To add a new directory to share over the network.Add the following section to the configuration file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[share]
   comment = share
   browseable = yes
   path = /mnt/path
   guest ok = no
   read only = no
   writable=yes
   create mask = 0777
   valid users = klugadmin


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;browsable: enables Windows clients to browse the shared directory using Windows Explorer.&lt;/li&gt;
  &lt;li&gt;guest ok: allows clients to connect to the share without supplying a password.&lt;/li&gt;
  &lt;li&gt;create mask: determines the permissions new files will have when created.&lt;/li&gt;
  &lt;li&gt;read only: determines if the share is read only or if write privileges are granted.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Restart the samba service to enable the new configuration&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo restart smbd
sudo restart nmbd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From a Windows client you should now be able to browse to the Ubuntu file server and see the shared directory. If your client doesn’t show your share automatically, try to access your server by its IP address, e.g. \192.168.1.1, in a Windows Explorer window&lt;/p&gt;

&lt;p&gt;you can also map the network location using the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;net use e: \\10.0.0.15\share
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;https://help.ubuntu.com/12.04/serverguide/samba-fileserver.html&lt;/li&gt;
  &lt;li&gt;https://www.howtoforge.com/samba-server-ubuntu-14.04-lts&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;http://askubuntu.com/questions/557001/permission-denied-when-sharing-folders&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;https://help.ubuntu.com/12.04/serverguide/samba-fileserver.html&lt;/li&gt;
  &lt;li&gt;https://www.howtoforge.com/samba-server-ubuntu-14.04-lts&lt;/li&gt;
  &lt;li&gt;http://askubuntu.com/questions/557001/permission-denied-when-sharing-folders&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>RAM Computation for ARM Microcontroller</title>
   <link href="http://localhost:4000/software/2016/04/20/ARMRAM/"/>
   <updated>2016-04-20T00:00:00+00:00</updated>
   <id>http://localhost:4000/software/2016/04/20/ARMRAM</id>
   <content type="html">&lt;h1 id=&quot;ram-computation-for-arm-microcontroller&quot;&gt;&lt;strong&gt;RAM Computation for ARM Microcontroller&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;An ARM Cortex compiled bare metal application consists of the following sections.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;text - shows the code and read-only data in your application (in decimal)&lt;/li&gt;
  &lt;li&gt;data - shows the read-write data in your application (in decimal)&lt;/li&gt;
  &lt;li&gt;bss - show the zero initialized (‘bss’ and ‘common’) data in your application (in decimal)&lt;/li&gt;
  &lt;li&gt;dec - total of ‘text’ + ‘data’ + ‘bss’ (in decimal)&lt;/li&gt;
  &lt;li&gt;hex - hexidecimal equivalent of ‘dec’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If there are no dynamic allocations in the application these section can help us determine the RAM requirements of the microcontroller.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;klugadmin@klugserver:/mnt/hgfs/repos/arya_firmware/arya_main$ arm-none-eabi-size --format=berkeley build/src.elf
   text    data     bss     dec     hex filename
  96324     244    3460  100028   186bc build/src.elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Flash size and RAM size occupied by the compiled program can be estimated as below.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Occupied Flash size ≈ data size + bss size + text size
Occupied RAM size ≈ data size + bss size
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For the above example the&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;occupied flash size is 244+3460+96324 =100046 ~100Kb
and RAM size is 244+3460=3700Bytes~3.7Kb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;With RTOS 2 threads the RAM and flash utilizations are as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;klugadmin@klugserver:/mnt/hgfs/repos/arya_firmware/arya_main$ arm-none-eabi-size -B -t build/src.elf
   text    data     bss     dec     hex filename
  99596     244    6496  106336   19f60 build/src.elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For the above example the&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;occupied flash size is 244+6496+106376     =113116~113Kb
and RAM size is 244+6496=6740~6.7Kb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thus RTOS library increases the RAM requirements by around 3KB&lt;/p&gt;

&lt;p&gt;Each RTOS thread will allocate a default stack for each Thread which is 2048 bytes.
The stack size must be sufficient hand all the allocations happening within the thread&lt;/p&gt;

&lt;p&gt;The Regular threads constructor will use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;172 bytes&lt;/code&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;96 of these bytes&lt;/code&gt; are used for the _user_perthread_libspace.&lt;/p&gt;

&lt;p&gt;Now, according to the MBED RTOS Memory Model, the Idle Thread, Timer Thread, and OS Scheduler also consume some RAM. This infor can be found in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RTX_Conf_CM.c&lt;/code&gt; and each of them require 128Bytes&lt;/p&gt;

&lt;p&gt;Thus for each mbed RTOS thread approximately 3KB RAM will be required.&lt;/p&gt;

&lt;p&gt;Thus for an application consisting of 4 RTOS threads and user application we obtain
RAM requirement to be 18KB&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LCD (which will be used in the product) dimension: 176x220&lt;/li&gt;
  &lt;li&gt;LCD Aspect ratio: 1.25&lt;/li&gt;
  &lt;li&gt;Image size: 90x112 (Aspect ratio: 1.24)&lt;/li&gt;
  &lt;li&gt;Image data: 90&lt;em&gt;112&lt;/em&gt;2 = 20160 bytes&lt;/li&gt;
  &lt;li&gt;No. of rows to scale per row in image: 220/112 ~ 2
Hence buffer size required for scaling: 2&lt;em&gt;176&lt;/em&gt;2 (2 bytes for color) = 704 bytes ~ 0.69kB&lt;/li&gt;
  &lt;li&gt;Buffer size for LCD: 2500 (1/8th of image) ~ 2.4 kB&lt;/li&gt;
  &lt;li&gt;Buffer size for UART: 2500 (1/8th of image) ~ 2.4 kB&lt;/li&gt;
  &lt;li&gt;Total dynamic allocation: 5.5 kB&lt;/li&gt;
  &lt;li&gt;Total static allocation (computed from program size): 1.8 kB&lt;/li&gt;
  &lt;li&gt;Total RAM size required: 7.3kB&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Image size: 50x66  (Aspect ratio: 1.32)
Image data: 50&lt;em&gt;66&lt;/em&gt;2 = 6600  bytes&lt;/p&gt;

&lt;p&gt;No. of rows to scale per row in image: 220/66 ~ 3
Hence buffer size required for scaling: 3&lt;em&gt;176&lt;/em&gt;2 (2 bytes for color) = 1056 bytes ~ 1kB&lt;/p&gt;

&lt;p&gt;Buffer size for LCD: 6600 ~ 6.4 kB
Buffer size for UART: 6600 ~ 6.4 kB&lt;/p&gt;

&lt;p&gt;Total dynamic allocation: 13.8 kB
Total static allocation (computed from program size): 1.8 kB
Total RAM size required: 15.6kB&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;Image size: 30x37 (Aspect ratio: 1.23)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Image data: 30&lt;em&gt;37&lt;/em&gt;2 = 2220 bytes&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;No. of rows to scale per row in image: 220/37 ~ 6&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hence buffer size required for scaling: 6&lt;em&gt;176&lt;/em&gt;2 (2 bytes for color) = 2112 bytes ~ 2kB&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Buffer size for LCD: 2220 ~ 2.2 kB&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Buffer size for UART: 2220 ~ 2.2 kB&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Total dynamic allocation: 6.4 kB&lt;/li&gt;
  &lt;li&gt;Total static allocation (computed from program size): 1.8 kB&lt;/li&gt;
  &lt;li&gt;Total RAM size required: 8.2 kB&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Thus 30x37 is the image size we can go with current optimization with high degree optimization it may increase to 1.5x to 2x maximum size&lt;/p&gt;

&lt;p&gt;but initial design specification will be 30x37 ,we will increase it if optimization permits it over the coming month&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;https://developer.mbed.org/questions/2065/RTOS-Memory-Usage-Confusion/&lt;/li&gt;
  &lt;li&gt;https://developer.mbed.org/questions/1280/Ram-usage-of-Threads/&lt;/li&gt;
  &lt;li&gt;https://developer.mbed.org/handbook/RTOS-Memory-Model&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>ANTLR installation and usage on ubuntu 14.04</title>
   <link href="http://localhost:4000/software/2016/04/14/AntLR1/"/>
   <updated>2016-04-14T00:00:00+00:00</updated>
   <id>http://localhost:4000/software/2016/04/14/AntLR1</id>
   <content type="html">&lt;h1 id=&quot;antlr-installation-and-usage-on-ubuntu-1404&quot;&gt;&lt;strong&gt;ANTLR installation and usage on ubuntu 14.04&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;In this article we will look at installation and usage of ANTLRsoftware&lt;/p&gt;

&lt;p&gt;ANTLR (ANother Tool for Language Recognition) is a powerful parser generator for reading, processing, executing, or translating structured text or binary files. It’s widely used to build languages, tools, and frameworks.&lt;/p&gt;

&lt;h2 id=&quot;download&quot;&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Antlr repository can be downloaded by running the below commands&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/antlr/antlr4.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Pre Requisites&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Antlr requires &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Apache maven&lt;/code&gt; build manager for Java projects.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install maven maven2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;python 3.5&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Download the source files from&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://www.python.org/downloads/source/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;build and install python 3.5&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure
make
sudo make install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Compile&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Antlr can be compiled by executing the below command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mvn compile
mvn install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will deploy antlr in the default location&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/.m2/repository
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The main antlr jar files is located at&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/.m2/repository/org/antlr/antlr4/4.5/antlr4-4.5.jar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;to use this we need to set classpath as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export CLASSPATH=$CLASSPATH:~/.m2/repository/org/antlr/antlr4/4.5/antlr4-4.5.jar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Execution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Put the following grammar inside file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Hello.g4&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Define a grammar called Hello
grammar Hello;
r  : 'hello' ID ;         // match keyword hello followed by an identifier
ID : [a-z]+ ;             // match lower-case identifiers
WS : [ \t\r\n]+ -&amp;gt; skip ; // skip spaces, tabs, newlines
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then run ANTLR the tool on it:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ alias antlr4='java -Xmx500M -cp &quot;~/.m2/repository/org/antlr/antlr4/4.5/antlr4-4.5.jar:$CLASSPATH&quot; org.antlr.v4.Tool'

$ alias grun='java org.antlr.v4.runtime.misc.TestRig'

$ antlr4 Hello.g4
$ javac Hello*.java
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To test it run the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ grun Hello r -tree input.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;contents of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input.txt&lt;/code&gt; file is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hello world&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This will give a GUI with parse tree displayed in the output&lt;/p&gt;

&lt;p&gt;That pops up a dialog box showing that rule &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r&lt;/code&gt; matched keyword &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hello&lt;/code&gt; followed by identifier &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parrt&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://raw.githubusercontent.com/antlr/antlr4/master/doc/getting-started.md1&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Apache OpenNLP Installation on ubuntu 14.04</title>
   <link href="http://localhost:4000/software%20installation/2016/04/09/openNLPInstall/"/>
   <updated>2016-04-09T00:00:00+00:00</updated>
   <id>http://localhost:4000/software%20installation/2016/04/09/openNLPInstall</id>
   <content type="html">&lt;h2 id=&quot;apache-opennlp-installation-on-ubuntu-1404&quot;&gt;&lt;strong&gt;Apache OpenNLP Installation on ubuntu 14.04&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text.&lt;/p&gt;

&lt;p&gt;It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. These tasks are usually required to build more advanced text processing services. OpenNLP also includes maximum entropy and perceptron based machine learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;you can download opennlp source from&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/apache/opennlp

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Pre Requisites&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;openlp requires &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Apache maven&lt;/code&gt; build manager for Java projects.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install maven maven2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;compile and install&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;execute the following command to compile &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;opennlp&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mvn compile
mvn install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;IDE&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;opennlp&lt;/code&gt; projects can be accessed via IDE’s like netbeans or eclipse&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Accessing VMWareWorkstation machine via VNC</title>
   <link href="http://localhost:4000/software%20installation/2016/04/09/VNCVMWare/"/>
   <updated>2016-04-09T00:00:00+00:00</updated>
   <id>http://localhost:4000/software%20installation/2016/04/09/VNCVMWare</id>
   <content type="html">&lt;p&gt;You can use a VNC client to connect to a running virtual machine. Because VNC software is cross-platform, you can use virtual machines running on different types of computers.&lt;/p&gt;

&lt;p&gt;Workstation does not need to be running to use VNC to connect to a virtual machine. Only the virtual machine needs to be running, and it can be running in the background.
When you use a VNC client to connect to a virtual machine, some features do not work or are not available.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You cannot take or revert to snapshots.&lt;/li&gt;
  &lt;li&gt;You cannot power on, power off, suspend, or resume the virtual machine. You can shut down the guest operating system. Shutting down might power off the virtual machine.&lt;/li&gt;
  &lt;li&gt;You cannot copy and paste text between the host system and the guest operating system.&lt;/li&gt;
  &lt;li&gt;You cannot change virtual machine settings.&lt;/li&gt;
  &lt;li&gt;Remote display does not work well if you are also using the 3D feature.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;configuring-a-virtual-machine-as-a-vnc-server&quot;&gt;&lt;strong&gt;Configuring a Virtual Machine as a VNC Server&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;You can use Workstation to configure a virtual machine to act as a VNC server so that users on other computers can use a VNC client to connect to the virtual machine. You do not need to install specialized VNC software in a virtual machine to set it up as a VNC server.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Select the virtual machine and select VM &amp;gt; Settings.&lt;/li&gt;
  &lt;li&gt;On the Options tab, select VNC Connections and select Enable VNC.&lt;/li&gt;
  &lt;li&gt;To allow VNC clients to connect to multiple virtual machines on the same host system, specify a unique port number for each virtual machine.&lt;/li&gt;
  &lt;li&gt;Use should use a port number in the range from 5901 to 6001. Other applications use certain port numbers, and some port numbers are privileged.&lt;/li&gt;
  &lt;li&gt;(Optional) Set a password for connecting to the virtual machine from a VNC client.&lt;/li&gt;
  &lt;li&gt;(Optional) Click View VNC Connections to see a list of the VNC clients that are remotely connected to the virtual machine and find out how long they have been connected.&lt;/li&gt;
  &lt;li&gt;Click OK to save your changes.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;install-a-vnc-client-on-your-computer&quot;&gt;&lt;strong&gt;Install a VNC client on your computer.&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Download the RealVNC Viewer from following link
&lt;a href=&quot;https://www.realvnc.com/download/viewer/&quot;&gt;https://www.realvnc.com/download/viewer/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;start-the-vnc-client-on-your-computer&quot;&gt;&lt;strong&gt;Start the VNC client on your computer.&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;The IP address is of the host machine running the VMWare guest operating system&lt;/p&gt;

&lt;p&gt;In the present case it is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10.0.0.17&lt;/code&gt; and port number configured in VMWare workstation settings is 5910&lt;/p&gt;

&lt;p&gt;It might take multiple attemtps to connect if the internet speed is slow or if the connection is being done over a WifiNetwork&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://pubs.vmware.com/workstation-9/topic/com.vmware.ws.using.doc/GUID-FB23927B-98A0-45E9-BFAC-85152F14BCAC.html&lt;/li&gt;
  &lt;li&gt;https://pubs.vmware.com/workstation-9/index.jsp?topic=%2Fcom.vmware.ws.using.doc%2FGUID-FB23927B-98A0-45E9-BFAC-85152F14BCAC.html&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Install Apache Tomcat 7 on Ubuntu 14.04</title>
   <link href="http://localhost:4000/software%20installation/2016/03/31/TomcatInstall/"/>
   <updated>2016-03-31T00:00:00+00:00</updated>
   <id>http://localhost:4000/software%20installation/2016/03/31/TomcatInstall</id>
   <content type="html">&lt;p&gt;In this we will look at installing
Apache Tomcat 7 on Ubuntu 14.04 and deplyoying a web application&lt;/p&gt;

&lt;h1 id=&quot;install-tomcat-with-apt-get&quot;&gt;Install Tomcat with apt-get&lt;/h1&gt;

&lt;p&gt;This will install the tomcat software and administration tools&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install tomcat7
sudo apt-get install tomcat7-admin

sudo chgrp -R tomcat7 /etc/tomcat7
sudo chmod -R g+w /etc/tomcat7 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;#Directory Strcuture&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CATALINA_HOME&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The tomcat installation directory is referred as
CATALINA_HOME and is set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/share/tomcat7&lt;/code&gt;
by default&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CATALINA_BASE&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This tomcat runtime directory and is set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/lib/tomcat7&lt;/code&gt; by default
web applications can be found here.&lt;/p&gt;

&lt;p&gt;webapps can be found in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;webapps&lt;/code&gt; directory&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CONFIGURATION&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Configuration files can found at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/tomcat7&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;DEFAULT CONFIGURATION&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;default configuration files for tomcat can be found at
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/default/tomcat7&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;commands&quot;&gt;&lt;strong&gt;COMMANDS&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Tomcat service can be checked&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo service tomcat7 status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Commands for starting and stopping are as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo service tomcat7 start
sudo service tomcat7 stop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;configuration&quot;&gt;&lt;strong&gt;CONFIGURATION&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;PORT CONFIGURATION&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By default Tomcat runs a HTTP connector on port 8080
This ports can be configured by changing the following lines in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/tomcat7/server.xml&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;Connector port=&quot;8090&quot; protocol=&quot;HTTP/1.1&quot; 
               connectionTimeout=&quot;20000&quot; 
               redirectPort=&quot;8443&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;JVM CONFIGURATION&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By default Tomcat will run preferably with OpenJDK JVMs, then try the Sun JVMs, then try some other JVMs. You can force Tomcat to use a specific JVM by setting JAVA_HOME in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/default/tomcat7&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;JAVA_HOME=/usr/lib/jvm/java-6-sun
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Declaring users and roles&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Usernames, passwords and roles (groups) can be defined centrally in a Servlet container. This is done in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/tomcat7/tomcat-users.xml&lt;/code&gt; file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;role rolename=&quot;admin&quot;/&amp;gt;
&amp;lt;user username=&quot;tomcat&quot; password=&quot;reloded23&quot; roles=&quot;admin,manager-gui,host-gui&quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Larger uploads&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Modify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/share/tomcat7-admin/manager/WEB-INF/web.xml&lt;/code&gt; to handle larger uploads.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &amp;lt;multipart-config&amp;gt;
      &amp;lt;max-file-size&amp;gt;72428800&amp;lt;/max-file-size&amp;gt;
      &amp;lt;max-request-size&amp;gt;72428800&amp;lt;/max-request-size&amp;gt;
      &amp;lt;file-size-threshold&amp;gt;0&amp;lt;/file-size-threshold&amp;gt;
    &amp;lt;/multipart-config&amp;gt;
    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;deploying-the-web-application&quot;&gt;&lt;strong&gt;Deploying the Web Application&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Tomcat manager GUI can be accessed as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
http://10.0.0.15:8090/manager/html

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;you can manager all the webapps from this terminal&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;http://askubuntu.com/questions/135824/what-is-the-tomcat-installation-directory&lt;/li&gt;
  &lt;li&gt;https://help.ubuntu.com/lts/serverguide/tomcat.html&lt;/li&gt;
  &lt;li&gt;http://mythinkpond.wordpress.com/2011/07/01/tomcat-6-infamous-severe-error-listenerstart-message-how-to-debug-this-error/&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Apache Xerces on Ubuntu 14.04</title>
   <link href="http://localhost:4000/software/2016/01/08/Xerses1/"/>
   <updated>2016-01-08T00:00:00+00:00</updated>
   <id>http://localhost:4000/software/2016/01/08/Xerses1</id>
   <content type="html">&lt;p&gt;we will look at installation and basic usage of Xerses
XML Parser on Ubuntu 14.04 platform&lt;/p&gt;

&lt;h1 id=&quot;apache-xerces-installation-on-ubuntu-1404&quot;&gt;Apache Xerces Installation on Ubuntu 14.04&lt;/h1&gt;

&lt;p&gt;In this article we will look at the installation of Apache Xerces C++ Installation on Ubuntu 14.04&lt;/p&gt;

&lt;h2 id=&quot;apache-xerces-project&quot;&gt;Apache Xerces™ Project&lt;/h2&gt;

&lt;p&gt;The Apache Xerces™ Project is responsible for software licensed to the Apache Software Foundation intended for the creation and maintenance of XML parsers and  related software components&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://xerces.apache.org/xerces-c/&quot;&gt;Apache Xerces C++&lt;/a&gt; - A processor for parsing, validating, serializing and manipulating XML, written in C++.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Xerces-C++&lt;/code&gt; is a validating XML parser written in a portable subset of C++
providing high performance, modularity, and scalability.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Xerces-C++&lt;/code&gt; makes it easy to give your application the ability to read and write XML data. A shared library is provided for parsing, generating, manipulating, and validating XML documents. Xerces-C++ is faithful to the XML 1.0 and 1.1 recommendations and many associated standards.&lt;/p&gt;

&lt;h2 id=&quot;license&quot;&gt;License&lt;/h2&gt;

&lt;p&gt;The Xerces-C++ Version 3.1.2 is available in both source distribution and binary distribution. Xerces-C++ is made available under the Apache Software License, Version 2.0 which makes it suitable for commercial use.&lt;/p&gt;

&lt;h2 id=&quot;download-and-installation&quot;&gt;Download and Installation&lt;/h2&gt;

&lt;p&gt;The sources and binaries can be found at download link http://xerces.apache.org/xerces-c/download.cgi&lt;/p&gt;

&lt;p&gt;The Xerces-C++ source is available in the source distribution: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xerces-c-3.1.2.tar.gz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Install the Xerces-C++ source distribution xerces-c-3.1.2.tar.gz by extracting the files from the compressed archive:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gzip -d xerces-c-3.1.2.tar.gz
tar -xf xerces-c-3.1.2.tar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This creates the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xerces-c-3.1.2&lt;/code&gt; sub-directory containing the Xerces-C++ source distribution.&lt;/p&gt;

&lt;p&gt;To build the Xerces-C++ source after installation, please follow the Build Instructions.&lt;/p&gt;

&lt;p&gt;For building on UNIX and UNIX-like (GNU/Linux, Max OS X, Cygwin, MinGW-MSYS) platforms Xerces-C++ uses the GNU &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;automake-based&lt;/code&gt; build systems and requires that you have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GNU make&lt;/code&gt; installed.&lt;/p&gt;

&lt;p&gt;As with all automake-based projects the build process is divided into two parts: configuration and building. The configuration part is performed using the configure script that can be found in the xerces-c-3.1.2 directory. The build part is performed by invoking make.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;configure
make
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the configuration part is complete you can run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make&lt;/code&gt;. Running make from the xerces-c-3.1.2 directory builds Xerces-C++ library and examples. The library is placed into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src/.libs&lt;/code&gt; directory.&lt;/p&gt;

&lt;p&gt;Finally, to install the library,header files and examples you can run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make install&lt;/code&gt; (or gmake install). To change the installation directory, use the –prefix configure option.&lt;/p&gt;

&lt;p&gt;The libraries are installed in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/local/lib&lt;/code&gt; subdirectory while include files are installed in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/local/include/xercesc/&lt;/code&gt; subdirectory&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Options&lt;/strong&gt;
By default configure selects both shared and static libraries. You can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--disable-shared and --disable-static&lt;/code&gt; options to avoid building the version you don’t need.&lt;/p&gt;

&lt;p&gt;If you need to specify compiler executables that should be used to build Xerces-C++, you can set the CC and CXX variables when invoking configure. Similarly, if you need to specify additional compiler or linker options, you can set the CFLAGS, CXXFLAGS, and LDFLAGS variables. For example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure --disable-static CC=gcc-4.3 CXX=g++-4.3 CFLAGS=-O3 CXXFLAGS=-O3
make
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;programming&quot;&gt;Programming&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Initialization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Independent of the API you want to use, DOM, SAX, or SAX2, your application must initialize the Xerces system before using the API, and terminate it after you are done.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &amp;lt;xercesc/util/PlatformUtils.hpp&amp;gt;
// Other include files, declarations, and non-Xerces-C++ initializations.

using namespace xercesc;

int main(int argc, char* argv[])
{
  try {
    XMLPlatformUtils::Initialize();
  }
  catch (const XMLException&amp;amp; toCatch) {
    // Do your failure processing here
    return 1;
  }

  // Do your actual work with Xerces-C++ here.

  XMLPlatformUtils::Terminate();

  // Other terminations and cleanup.
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;XMLPlatformUtils::Initialize() and XMLPlatformUtils::Terminate must be called at least once in each process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parsing an XML File&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to use Xerces-C++ to parse XML files using DOM, you can create an instance of the XercesDOMParser class&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
       XercesDOMParser* parser = new XercesDOMParser();
        parser-&amp;gt;setValidationScheme(XercesDOMParser::Val_Always);
        parser-&amp;gt;setDoNamespaces(true);    // optional

        ErrorHandler* errHandler = (ErrorHandler*) new HandlerBase();
        parser-&amp;gt;setErrorHandler(errHandler);

        char* xmlFile = &quot;x1.xml&quot;;

        try {
            parser-&amp;gt;parse(xmlFile);
        }
        catch (const XMLException&amp;amp; toCatch) {
            char* message = XMLString::transcode(toCatch.getMessage());
            cout &amp;lt;&amp;lt; &quot;Exception message is: \n&quot;
                 &amp;lt;&amp;lt; message &amp;lt;&amp;lt; &quot;\n&quot;;
            XMLString::release(&amp;amp;message);
            return -1;
        }
        catch (const DOMException&amp;amp; toCatch) {
            char* message = XMLString::transcode(toCatch.msg);
            cout &amp;lt;&amp;lt; &quot;Exception message is: \n&quot;
                 &amp;lt;&amp;lt; message &amp;lt;&amp;lt; &quot;\n&quot;;
            XMLString::release(&amp;amp;message);
            return -1;
        }
        catch (...) {
            cout &amp;lt;&amp;lt; &quot;Unexpected Exception \n&quot; ;
            return -1;
        }

        delete parser;
        delete errHandler;
        return 0;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://xerces.apache.org/xerces-c/build-3.html&lt;/li&gt;
  &lt;li&gt;http://xerces.apache.org/xerces-c/&lt;/li&gt;
  &lt;li&gt;http://stackoverflow.com/questions/9387610/what-xml-parser-should-i-use-in-c&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>STM32 Cube Download</title>
   <link href="http://localhost:4000/software%20installation/2015/12/31/STM32Cube/"/>
   <updated>2015-12-31T00:00:00+00:00</updated>
   <id>http://localhost:4000/software%20installation/2015/12/31/STM32Cube</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;STM32 Cube Download&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;STMCube™ is an STMicroelectronics original initiative to ease developers’ life by reducing development efforts, time and cost. STM32Cube covers STM32 portfolio.&lt;/p&gt;

&lt;p&gt;STM32Cube includes the STM32CubeMX which is a graphical software configuration tool that allows generating C initialization code using graphical wizards.&lt;/p&gt;

&lt;p&gt;It also comprises the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STM32CubeF0&lt;/code&gt; platform which includes the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STM32Cube HAL&lt;/code&gt; (an STM32 abstraction layer embedded software, ensuring maximized portability across STM32 portfolio), plus a consistent set of middleware components (RTOS, USB, FatFS and STM32 touch sensing). All embedded software utilities come with a full set of examples.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STM32CubeF0 &lt;/code&gt;gathers in one single package all the generic embedded software components required to develop an application on STM32F0 microcontrollers. Following STM32Cube initiative, this set of components is highly portable, not only within STM32F0 series but also to other STM32 series.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STM32CubeF0&lt;/code&gt; is fully compatible with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STM32CubeMX&lt;/code&gt; code generator that allows generating initialization code. The package includes a low level hardware abstraction layer (HAL) that covers the microcontroller hardware, together with an extensive set of examples running on STMicroelectronics boards. The HAL is available in open-source BSD license for user convenience.&lt;/p&gt;

&lt;p&gt;The STM32Cube software can be downloaded from the following link
http://www.st.com/web/en/catalog/tools/FM146/CL2167/SC2004?icmp=sc2004_pron_pr-stm32f446_dec2014&amp;amp;sc=stm32cube-pr8&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;STM32CubeMX can be downloaded from the below link
http://www.st.com/web/en/catalog/tools/PF259242&lt;/p&gt;

&lt;p&gt;We will download both the software for STM32F0 series
&lt;a href=&quot;http://www.st.com/st-web-ui/static/active/en/st_prod_software_internet/resource/technical/software/firmware/stm32cubef0.zip&quot;&gt;STM32CubeF0&lt;/a&gt;
&lt;a href=&quot;http://www.st.com/st-web-ui/static/active/en/st_prod_software_internet/resource/technical/software/sw_development_suite/stm32cubemx.zip&quot;&gt;STM32CubeMX&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>State Machine Interface - Part 1</title>
   <link href="http://localhost:4000/software/2015/12/29/SM/"/>
   <updated>2015-12-29T00:00:00+00:00</updated>
   <id>http://localhost:4000/software/2015/12/29/SM</id>
   <content type="html">&lt;h2 id=&quot;state-machine-interface&quot;&gt;&lt;strong&gt;State Machine Interface&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;A State Machine is a design pattern in which actions are determined by events and the current context of the system. and are used most commonly to model the behavior of an object across its lifetime.&lt;/p&gt;

&lt;p&gt;Hierarchical State Machines enable to structure the application into logical states which is effective and elegant way of decomposing event driven behavior.&lt;/p&gt;

&lt;h1 id=&quot;state&quot;&gt;&lt;strong&gt;State&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;A state describes a period of time during the life cycle of object or program execution .It is a condition of an object which performs some activity or waits for an event.&lt;/p&gt;

&lt;p&gt;A state is denoted by a round-cornered rectangle with the name of the state written inside it.&lt;/p&gt;

&lt;p&gt;A state has several properties.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Name&lt;/li&gt;
  &lt;li&gt;Entry/Exit Actions&lt;/li&gt;
  &lt;li&gt;Internal Transitions&lt;/li&gt;
  &lt;li&gt;Substates&lt;/li&gt;
  &lt;li&gt;Deferred events&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will look at the properties in detail below.&lt;/p&gt;

&lt;h1 id=&quot;initial-and-final-state&quot;&gt;&lt;strong&gt;Initial and Final State&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;There are two special states that need to be defined for an objects state machine.The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initial&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;final&lt;/code&gt; state.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initial state&lt;/code&gt; is denoted by a filled black circle and indicates the default starting place for the state machine or sub-state.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;final state&lt;/code&gt; is denoted by a circle with a dot inside 
and indicates the completion of the execution of the state machine or that the enclosing state.&lt;/p&gt;

&lt;p&gt;Both the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initial&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;final&lt;/code&gt; states are pseudo states,both do not have usual properties of a normal state .&lt;/p&gt;

&lt;p&gt;The program performs a transition to the next state immediately after entering the initial state automatically&lt;/p&gt;

&lt;h1 id=&quot;simple-state&quot;&gt;&lt;strong&gt;Simple State&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;simple state&lt;/code&gt; has no sub states. It can have entry and exit behaviors and deferred events and internal transitions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/vIIrah5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above example the state transitions from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;init&lt;/code&gt; state to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Idle&lt;/code&gt; state when the state machine is initiated.&lt;/p&gt;

&lt;p&gt;The transition from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Idle&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;final&lt;/code&gt; state happens when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shutdown&lt;/code&gt; signal event is received.&lt;/p&gt;

&lt;h1 id=&quot;entry-and-exit-behavior&quot;&gt;&lt;strong&gt;Entry and Exit Behavior&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;One can provide entry and exit behaviors/actions to states which are executed when state is entered  or exited&lt;/p&gt;

&lt;p&gt;Entry and exit actions allow the same action to be dispatched every time the state is entered or left, respectively&lt;/p&gt;

&lt;p&gt;Note that the entry and exit Behavior is also executed if a self transition takes place.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/ELv3Dpa.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Upon entry into the state &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;init&lt;/code&gt; function is called and before exit the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_exit&lt;/code&gt; function is called&lt;/p&gt;

&lt;h1 id=&quot;actions&quot;&gt;&lt;strong&gt;Actions&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Once the entry behavior is complete the state machine enters the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; state for the object.Various actions may be triggered depending on event that caused the state change and/or program status.As long as the object is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; state the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; behavior is called every &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; cycle of the statemachine.&lt;/p&gt;

&lt;p&gt;in the above example the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Idle.run&lt;/code&gt; function is called every run cycle&lt;/p&gt;

&lt;p&gt;Actions are also realized in a manner similar to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exit&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;entry&lt;/code&gt; functions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/gXAEk6v.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;transition&quot;&gt;&lt;strong&gt;Transition&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;A transition is a relationship between two states indicating that an object in the first state will perform certain actions and enter a second state when a specified event occurs and specified conditions are satisfied.&lt;/p&gt;

&lt;p&gt;On such a change of state, the transition is said to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fire&lt;/code&gt;. Until the transition fires, the object is said to be in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source&lt;/code&gt; state; after it fires, it is said to be in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;target&lt;/code&gt; state.&lt;/p&gt;

&lt;p&gt;A transition has several properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;source state&lt;/li&gt;
  &lt;li&gt;target state&lt;/li&gt;
  &lt;li&gt;event trigger&lt;/li&gt;
  &lt;li&gt;guard condition&lt;/li&gt;
  &lt;li&gt;action&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Transitions from one state to the next are denoted by lines with arrowheads. A transition may have a trigger, a guard and an action&lt;/p&gt;

&lt;h1 id=&quot;event-trigger&quot;&gt;&lt;strong&gt;Event trigger&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;In the context of the state machine, an event is an occurrence of a stimulus that can trigger a state transition&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/6Ibqs9v.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the following example &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;complete&lt;/code&gt; event triggers a transition from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Idle&lt;/code&gt;
state to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;finished&lt;/code&gt; state&lt;/p&gt;

&lt;h1 id=&quot;guard-condition&quot;&gt;&lt;strong&gt;Guard condition&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;A guard condition is evaluated after the trigger event for the transition occurs.&lt;/p&gt;

&lt;p&gt;A guard condition is evaluated just once for the transition at the time the event occurs.&lt;/p&gt;

&lt;p&gt;If the guard condition evaluates to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt; then transition is eligible to fire or if condition evaluates to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt; then transition does not fire.&lt;/p&gt;

&lt;p&gt;It is possible to have multiple transitions from the same source state with the same event trigger ,as long as the guard conditions do not overlap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/9PBwPha.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above example,the transition is triggered only when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;complete&lt;/code&gt; event is triggered and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;done&lt;/code&gt; flag is true.&lt;/p&gt;

&lt;h1 id=&quot;internal-transitions&quot;&gt;&lt;strong&gt;Internal Transitions&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Internal transitions allow events to be handled within the state without leaving the state, thereby avoiding triggering entry or exit actions.&lt;/p&gt;

&lt;p&gt;Internal transitions may have events with parameters and guard conditions, and essentially represent interrupt-handlers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/9PBwPha.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above example the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reset&lt;/code&gt; is an internal transition,which sets &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;done&lt;/code&gt; flag to true upon triggering of the event.&lt;/p&gt;

&lt;h1 id=&quot;substates&quot;&gt;&lt;strong&gt;Substates&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;A simple state is one which has no substructure. A state which has substates (nested states) is called a composite state. Substates may be nested to any level. A nested state machine may have at most one initial state and one final state. Substates are used to simplify complex flat state machines by showing that some states are only possible within a particular context&lt;/p&gt;

&lt;p&gt;From a source outside an enclosing composite state, a transition may target the composite state or it may target a substate.&lt;/p&gt;

&lt;p&gt;If its target is the composite state, the nested state machine must include an initial state, to which control passes after entering the composite state and after dispatching its entry action (if any).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/0qxDXLR.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A transition leading out of a composite state may have as its source the composite state or a substate. In either case, control first leaves the nested state (and its exit action, if any, is dispatched), then it leaves the composite state (and its exit action, if any, is dispatched).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/IGSuxAd.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If its target is the nested state, control passes to the nested state after dispatching the entry action of the composite state (if any), and then the entry action of the nested state (if any).&lt;/p&gt;

&lt;h1 id=&quot;history-states&quot;&gt;&lt;strong&gt;History States&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Unless otherwise specified, when a transition enters a composite state, the action of the nested state machine starts over again at the initial state (unless the transition targets a sub-state directly). History states allow the state machine to re-enter the last sub-state that was active prior to leaving the composite state.&lt;/p&gt;

&lt;h1 id=&quot;choice&quot;&gt;&lt;strong&gt;Choice&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Choice is also a pseudo state. It can be used to model a conditional path. Choice nodes divide a transition into multiple parts.&lt;/p&gt;

&lt;p&gt;Usually the first transition points towards the choice node. One of the choice outgoing transitions can carry a condition.&lt;/p&gt;

&lt;h1 id=&quot;junction&quot;&gt;&lt;strong&gt;Junction&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;A junction is a pseudo state do combine transitions. This is very comfortable if a state machine has many similar transitions. Junctions add clear arrangement to the state machine.&lt;/p&gt;

&lt;h2 id=&quot;implementation-details&quot;&gt;&lt;strong&gt;Implementation Details&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;We will look at the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YAKINDU&lt;/code&gt; state chart tool.YAKINDU Statechart Tools provides a rich feature set to supports custom code generators out of the box.&lt;/p&gt;

&lt;p&gt;we will take this as reference for implementing state machines.It is a modular and simple structure.&lt;/p&gt;

&lt;p&gt;The interface for the state machine is as follows&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
class StatemachineInterface {
	public:
	
		virtual ~StatemachineInterface() = 0;
		
		/*
		* Initializes the statemachine. Use to init internal variables etc.
		*/
		virtual void init() = 0;
	
		/*
		* Enters the statemachine. Sets the statemachine in a defined state.
		*/
		virtual void enter() = 0;
	
		/*
		* Exits the statemachine. Leaves the statemachine with a defined state.
		*/
		virtual void exit() = 0;
	
		/*
		* Start a run-to-completion cycle.
		*/
		virtual void runCycle() = 0;
		
		/*
		* Checks if the statemachine is active. 
	 	* A statemachine is active if it was entered. It is inactive if it has not been entered at all or if it was exited.
	 	*/	
		virtual	sc_boolean isActive() = 0;
		
		/*
		* Checks if all active states are final. 
	 	* If there are no active states then the statemachine is considered as inactive and this method returns false.
	 	*/
		virtual sc_boolean isFinal() = 0;
};
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let us consider the implementation of following state machine
&lt;img src=&quot;http://imgur.com/IGSuxAd.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The state machine realizes the abstract class . The default name is name of state machine
in this case its &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HSMTest&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Each state and associated regions are maintained in a list&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
		typedef enum {
			_region1__final_,
			_region1_main,
			_region1_main_r1_Idle,
			HSMTest_last_state
		} HSMTestStates;
		
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The members of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HSMTest&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;class corresponding to Each state&lt;/li&gt;
  &lt;li&gt;interface or abstract class corresponding to functions of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;entry,exit,action&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
				class SCI_Idle_OCB {
					public:
						virtual ~SCI_Idle_OCB() = 0;
						
						virtual void init() = 0;
						
						virtual void _exit() = 0;
						
						virtual void run() = 0;
				};
	
				class SCI_Master_OCB {
					public:
						virtual ~SCI_Master_OCB() = 0;
						
						virtual void init() = 0;
						
						virtual void _exit() = 0;
				};
	

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The state machine provide setter methods to pass the instances of abstract class.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
void setSCI_Master_OCB(SCI_Master_OCB* operationCallback);
void setSCI_Idle_OCB(SCI_Idle_OCB* operationCallback);

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each state is implemented as class.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;

		class SCI_Idle {
			
			public:
				/*! Raises the in event 'complete' that is defined in the interface scope 'Idle'. */ 
				void raise_complete();
				
				/*! Raises the in event 'reset' that is defined in the interface scope 'Idle'. */ 
				void raise_reset();
				
				/*! Gets the value of the variable 'done' that is defined in the interface scope 'Idle'. */ 
				sc_boolean get_done();
				
				/*! Sets the value of the variable 'done' that is defined in the interface scope 'Idle'. */ 
				void set_done(sc_boolean value);
				
				
			private:
				friend class HSMTest;
				sc_boolean complete_raised;
				sc_boolean reset_raised;
				sc_boolean done;
		}; 
		
	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The events are boolean variable .
For example the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;complete&lt;/code&gt; event is mapped to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;complete_raised&lt;/code&gt; boolean variable
with function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;raise_complete&lt;/code&gt; to raise event.&lt;/p&gt;

&lt;p&gt;The variables are implemented as members of the class .
For example the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;done&lt;/code&gt; variable is a private member with setter and getter methods
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set_done&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_done&lt;/code&gt; respectively.&lt;/p&gt;

&lt;p&gt;The State machine returns objects corresponding to state.Which can be used to access members .&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SCI_Idle* getSCI_Idle();
SCI_Idle_OCB* ifaceIdle_OCB;
SCI_Master* getSCI_Master();
SCI_Master_OCB* ifaceMaster_OCB;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The state machine maintains a list of active states in a vector&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;HSMTestStates stateConfVector[maxOrthogonalStates];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The maxOrthogonalStates states defines the maximum number of concurrent active states .&lt;/p&gt;

&lt;p&gt;During each run cycle the the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; behavior of each is state is executed.
It also checks if flag corresponding to events have be set in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source&lt;/code&gt; state
If yes then then any actions configured to be executed at transition are executed first followed by the transition behavior.Else default run behavior is executed.
The transition behavior consists of re-configuring the set of active states to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;target&lt;/code&gt; state.&lt;/p&gt;

&lt;p&gt;Below is the application code for using the state machine where we implement interfaces,pass objects to state machine,and simulate the event to observe the state machine flow&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/*
 * HSMTest.cpp
 *
 *  Created on: 31-Dec-2015
 *      Author: Prashant
 */
#include &amp;lt;iostream&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &quot;../../src-gen/HSMTest.h&quot;
using namespace std;

//interface for Idle state
class Idle : public HSMTest::SCI_Idle_OCB
{
public:

	void init()
	{
			cerr &amp;lt;&amp;lt; &quot;idle init &quot; &amp;lt;&amp;lt; endl;
	}

	void _exit()
	{
			cerr &amp;lt;&amp;lt; &quot;idle exit &quot; &amp;lt;&amp;lt; endl;
	 }

	void run()
	{
		cerr &amp;lt;&amp;lt; &quot;idle run &quot; &amp;lt;&amp;lt; endl;
	}
};

//interface for master state
class Master : public HSMTest::SCI_Master_OCB
{

public:
		 void _exit()
		{
			cerr &amp;lt;&amp;lt; &quot;master exit &quot; &amp;lt;&amp;lt; endl;
		}

		void init()
		{
			cerr &amp;lt;&amp;lt; &quot;master init &quot; &amp;lt;&amp;lt; endl;
		}
};

//main function
int main()
{
	HSMTest s;
	Idle i;
	Master m;
	s.setSCI_Idle_OCB(&amp;amp;i);
	s.setSCI_Master_OCB(&amp;amp;m);
	s.init();
	s.runCycle();
	s.enter();
	s.runCycle();
	for (int i = 0; i &amp;lt; 2; i++) {
		sleep(1);
		s.runCycle();
	}
	s.runCycle();
			 
	s.getSCI_Idle()-&amp;gt;raise_complete();
	s.runCycle();
			
	cerr &amp;lt;&amp;lt; &quot;done&quot; &amp;lt;&amp;lt; endl;


}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;download-and-installation&quot;&gt;&lt;strong&gt;Download and Installation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;PreRequisites&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Windows - cygwin&lt;/li&gt;
  &lt;li&gt;Linux gcc&lt;/li&gt;
  &lt;li&gt;java&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt;
Download the yakindu State Chart tool from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://statecharts.org/&lt;/code&gt;.
Its eclipse based and open-source,windows and java versions are available.&lt;/p&gt;

&lt;p&gt;Just download and unizip the package to get started.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Launch&lt;/strong&gt;
Launch the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SCT&lt;/code&gt; executable to start the GUI.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;File-&amp;gt;Create-&amp;gt;Project
To launch the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;New Project&lt;/code&gt; creation wizard.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/CfJFOjU.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create Project&lt;/strong&gt;
Select java or C/C++ project depending on if you want to auto-generate C/C++ or java code&lt;/p&gt;

&lt;p&gt;Here C++ project was chosen&lt;/p&gt;

&lt;p&gt;Enter the name of the project ,select the suitable toolchain
and click on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Finish&lt;/code&gt; to create the project.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Creating state-chart&lt;/strong&gt;
Create a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state-chart&lt;/code&gt; model to get started to begin creating
state machine model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/yVrI7FU.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the GUI configure the states,interfaces and transitions&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/uSA9KFl.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configure code generator&lt;/strong&gt;
To create a generator model with the wizard,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Click &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Project &amp;gt; New &amp;gt; Code Generator Model&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Type a name and click Next&lt;/li&gt;
  &lt;li&gt;Choose the desired generator, i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YAKINDU C++ Code Generator&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Check the model(s) to generate code from and click Finish&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/MSMaepQ.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/zFKSzvA.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This will generate the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state-chart&lt;/code&gt; files in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src-gen&lt;/code&gt; directories.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/Wc83VV7.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Application code&lt;/strong&gt;
In the src directory we create the application source file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HSMTest_app.cpp&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Build&lt;/strong&gt;
upon triggering the project build it will create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Debug&lt;/code&gt; the folder and auto-generate makefiles can create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HTest.exe&lt;/code&gt; binary&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/moCvIKA.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Launch&lt;/strong&gt;
The binary can be launched in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cygwin&lt;/code&gt; shell for execution&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://imgur.com/Zh8JJ4Z.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;http://statecharts.org/documentation.html&lt;/li&gt;
  &lt;li&gt;http://statecharts.org/tutorial.html&lt;/li&gt;
  &lt;li&gt;http://sce.uhcl.edu/helm/rationalunifiedprocess/process/modguide/md_stadm.htm&lt;/li&gt;
  &lt;li&gt;http://www.boost.org/doc/libs/1_60_0/libs/msm/doc/HTML/ch02s02.html&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>STLink Installation on Ubuntu 14.04</title>
   <link href="http://localhost:4000/software%20installation/2015/12/28/STLinkInstall/"/>
   <updated>2015-12-28T00:00:00+00:00</updated>
   <id>http://localhost:4000/software%20installation/2015/12/28/STLinkInstall</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;STLink Installation on Ubuntu 14.04&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Linux a command line program can be used to program STM32 microcontrollers via the embedded ST-LINK found on evaluation boards such as the STM32F4 discovery .&lt;/p&gt;

&lt;p&gt;In this tutorial, a STM32F4 discovery board is programmed in Ubuntu Linux 14.04&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;install dependencies&lt;/strong&gt;
 First install dependencies &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libusb&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install libusb-1.0-0-dev git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Build stlink binaries&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the terminal window, change to a suitable directory to work from and enter:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;git clone https://github.com/texane/stlink stlink.git&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This will clone the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stlink&lt;/code&gt; github repository to the working directory&lt;/p&gt;

&lt;p&gt;Change to the new directory and then make the project. Enter these two lines in the terminal window:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$cd stlink.git
$ ./autogen.sh
$ ./configure
$ make

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Copy the st-flash file to the file system.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stlink.git&lt;/code&gt; directory contains the  contains the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;st-flash&lt;/code&gt; program after compilation
Copy this to systems binary path&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo cp st-flash /usr/bin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;UDEV Permissions&lt;/strong&gt;
Set up udev rules so that it is possible to run st-flash without using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo&lt;/code&gt; command. Change back to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stlink.git&lt;/code&gt; directory and then copy the rules files to the file system. In the terminal window enter:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ..
sudo cp *.rules /etc/udev/rules.d
sudo restart udev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Unity Android Application Integration</title>
   <link href="http://localhost:4000/software/2015/12/20/soft2/"/>
   <updated>2015-12-20T00:00:00+00:00</updated>
   <id>http://localhost:4000/software/2015/12/20/soft2</id>
   <content type="html">&lt;h2 id=&quot;unity-android-application-integration&quot;&gt;&lt;strong&gt;Unity Android Application Integration&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In this article we look at how to integrate an Unity and Android Project more specifically
how to add Android View and Layouts on top of exiting Unity Views.&lt;/p&gt;

&lt;h3 id=&quot;unity-application&quot;&gt;&lt;strong&gt;Unity Application&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;When a Unity app starts, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnityPlayerProxyActivity&lt;/code&gt; chooses which activity to instantiate: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnityPlayerActivity&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnityPlayerNativeActivity&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;All three classes—UnityPlayerProxyActivity, UnityPlayerActivity, UnityPlayerNativeActivity—are compiled into a file called UnityPlayer.jar.&lt;/p&gt;

&lt;p&gt;It chooses which class to instantiate based on the version of Android running on the device. Support for native activities was not added to Android until Gingerbread (API level 9) and so &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnityPlayerNativeActivity&lt;/code&gt; is only instantiated if the device is running Gingerbread or higher.Otherwise, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnityPlayerActivity&lt;/code&gt; is instantiated.&lt;/p&gt;

&lt;p&gt;Both UnityPlayerActivity and UnityPlayerNativeActivity are responsible for starting the Unity game engine
and therby instantiating the views and coroutines.&lt;/p&gt;

&lt;p&gt;Let us consider the following Unity Program ,which just starts a coroutine upon initialization.
The scene contains no views or camera objects in the Unity Application just a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GameObject&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Let us add a script to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gameobject&lt;/code&gt; called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AndroidUnityInterface.cs&lt;/code&gt;
The script is reponsible for running the routines on Unity as well as communicating with the android
Application.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/dmMxUL9.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;using UnityEngine;
using System.Collections;

public class AndroidUnityInterface : MonoBehaviour {

	// Use this for initialization
	void Start () {
	
		Debug.LogError (&quot;Starting the Android Unity Interface&quot;);
	}
	
	// Update is called once per frame
	void Update () {

		Debug.LogError (&quot;Update the Android Unity Interface&quot;);
	
	}
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This script will call the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Start&lt;/code&gt; function while initialization and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Update&lt;/code&gt; function is called at every frame.&lt;/p&gt;

&lt;h3 id=&quot;unity-project-build&quot;&gt;Unity Project Build&lt;/h3&gt;
&lt;p&gt;Build the project for the Android platform.Open the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Player settings&lt;/code&gt; and take note of the package name that you have chosen as your “Bundle Identifier as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;com.example.androidunityinterface&lt;/code&gt; and the Android API level (e.g., 4.3 “JellyBean”), as you will reuse these settings later.&lt;/p&gt;

&lt;p&gt;The same package name will be used while creating the android application in the below section.&lt;/p&gt;

&lt;p&gt;If we compile and run the application,we would observe the Default Unity View and in the android Logs we would be able to observe the Debug Log statements written in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Start&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Update&lt;/code&gt; functions.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
11-15 16:46:20.276    7766-7791/? E/Unity﹕ Starting the Android Unity Interface
    (Filename: ./artifacts/generated/common/runtime/UnityEngineDebug.gen.cpp Line: 56)
11-15 16:46:20.286    7766-7791/? E/Unity﹕ Update the Android Unity Interface
    (Filename: ./artifacts/generated/common/runtime/UnityEngineDebug.gen.cpp Line: 56)
11-15 16:46:20.321    7766-7791/? E/Unity﹕ Update the Android Unity Interface
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;adding-android-views-on-top-of-unity&quot;&gt;&lt;strong&gt;Adding Android Views on Top of Unity&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;With Unity Android it is possible to extend the standard &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnityPlayerActivity&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnityPlayerNativeActivity&lt;/code&gt;  class (the primary Java class for the Unity Player on Android) enabling an application to override any and all of the basic interaction between Android OS and Unity Android.&lt;/p&gt;

&lt;p&gt;Thus by extending the UnityPlayerNativeActivity we are manually instantiating the unity game engine.&lt;/p&gt;

&lt;h4 id=&quot;copying-unity-files&quot;&gt;Copying Unity Files&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;As mentioned in the above section the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnityPlayerNative&lt;/code&gt; class is found in the classes.jar file in Unity. This files needs to be added to android project.&lt;/p&gt;

    &lt;p&gt;The jar file can be found in 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Editor\Data\PlaybackEngines\androidplayer\release\bin&lt;/code&gt; path in Unity Installation directory.&lt;/p&gt;

    &lt;p&gt;We need to add this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;classes.jar&lt;/code&gt; file to the project.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;we also need to copy unity libraries from following path from unity installation directory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Editor\Data\PlaybackEngines\androidplayer\development\libs\armeabi-v7a&lt;/code&gt; to libs directory of the Android project.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let first look at Android Application just instantiating the UnityGame Engine for the application in the above section.&lt;/p&gt;

&lt;p&gt;We will be using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Android-Studio&lt;/code&gt; for android application development&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
public class MainActivity extends UnityPlayerNativeActivity  {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        final long delay = 1000;//ms
        Handler handler = new Handler();

        Runnable runnable = new Runnable() {
            public void run() {
	            //gets the root default unity view
                ViewGroup rootView = (ViewGroup) MainActivity.this.findViewById(android.R.id.content);
                rootView.setKeepScreenOn(true);
            }
        }

        getWindow().addFlags(WindowManager.LayoutParams.FLAG_FULLSCREEN | WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);
        
        handler.postDelayed(runnable, delay);
        
    }

}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;native-events-forwarding&quot;&gt;Native Events forwarding&lt;/h3&gt;
&lt;p&gt;we need to allow native events forwarding so that events(like touch) are forwarded from unity to android.This is done by specifying the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ForwardNativeEventsToDalvik&lt;/code&gt; setting in the manifest (must be set to true)&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;
&amp;lt;manifest xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;
    package=&quot;com.example.androidunityinterface&quot; &amp;gt;

    &amp;lt;application
        android:allowBackup=&quot;true&quot;
        android:icon=&quot;@mipmap/ic_launcher&quot;
        android:label=&quot;@string/app_name&quot;
        android:theme=&quot;@style/AppTheme&quot; &amp;gt;
        &amp;lt;activity
            android:name=&quot;.MainActivity&quot;
            android:screenOrientation=&quot;landscape&quot;
            android:label=&quot;@string/app_name&quot;
            android:configChanges=&quot;fontScale|keyboard|keyboardHidden|locale|mnc|mcc|navigation|orientation|screenLayout|uiMode|touchscreen&quot;
            &amp;gt;
            &amp;lt;intent-filter&amp;gt;
                &amp;lt;action android:name=&quot;android.intent.action.MAIN&quot; /&amp;gt;

                &amp;lt;category android:name=&quot;android.intent.category.LAUNCHER&quot; /&amp;gt;
            &amp;lt;/intent-filter&amp;gt;
            &amp;lt;meta-data
                android:name=&quot;unityplayer.ForwardNativeEventsToDalvik&quot;
                android:value=&quot;true&quot; /&amp;gt;
        &amp;lt;/activity&amp;gt;
    &amp;lt;/application&amp;gt;

&amp;lt;/manifest&amp;gt;


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;copying-unity-files-1&quot;&gt;Copying Unity Files&lt;/h3&gt;

&lt;p&gt;Use your file explorer to browse to your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Unity project directory&lt;/code&gt;, where there is a folder called “/Temp” containing a subfolder called “/StagingArea.”&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;StagingArea&lt;/code&gt; folder contains these set of files and directories (such as /res, /assets, /bin,  AndroidManifest.xml,…) similar to a typical eclipse android directory.&lt;/p&gt;

&lt;p&gt;Copy the contents of “/assets/” and “/libs” directories to corresponding directories in the AndroidProject.&lt;/p&gt;

&lt;h3 id=&quot;testing-the-application&quot;&gt;Testing The application&lt;/h3&gt;

&lt;p&gt;we can now run the android application and expect the same result as just running the unity application.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;11-15 19:17:39.881      535-578/com.example.androidunityinterface E/Unity﹕ Starting the Android Unity Interface
    UnityEngine.Debug:Internal_Log(Int32, String, Object)
    UnityEngine.Debug:LogError(Object)
    AndroidUnityInterface:Start()
    (Filename: ./artifacts/generated/common/runtime/UnityEngineDebug.gen.cpp Line: 56)
11-15 19:17:39.901      535-578/com.example.androidunityinterface E/Unity﹕ Update the Android Unity Interface
    UnityEngine.Debug:Internal_Log(Int32, String, Object)
    UnityEngine.Debug:LogError(Object)
    AndroidUnityInterface:Update()
    (Filename: ./artifacts/generated/common/runtime/UnityEngineDebug.gen.cpp Line: 56)
11-15 19:17:39.966      535-578/com.example.androidunityinterface E/Unity﹕ Update the Android Unity Interface
    UnityEngine.Debug:Internal_Log(Int32, String, Object)
    UnityEngine.Debug:LogError(Object)
    AndroidUnityInterface:Update()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;adding--android-on-top-of-unity-views&quot;&gt;Adding  Android on Top of Unity Views&lt;/h3&gt;

&lt;p&gt;We can now add android views on top of the unity view programatically.&lt;/p&gt;

&lt;p&gt;First  Scan the view hierarchy recursively, starting from the root view of your main activity, and find the leaf view in the hierarchy.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    private View getLeafView(View view) {
        if (view instanceof ViewGroup) {
            ViewGroup vg = (ViewGroup)view;
            for (int i = 0; i &amp;lt; vg.getChildCount(); ++i) {
                View chview = vg.getChildAt(i);
                View result = getLeafView(chview);
                if (result != null)
                    return result;
            }
            return null;
        }
        else {

            Log.e(&quot;ZZ&quot;,&quot;Found leaf view&quot;);
            return view;
        }
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When you find the leaf view, get the parent of that view. Call this view leafParent for ease of reference.&lt;/p&gt;

&lt;p&gt;Add your custom views as a child of the leafParent view, for example, a layout inflated from XML.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public class MainActivity extends UnityPlayerNativeActivity  {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        final long delay = 1000;//ms
        Handler handler = new Handler();

        Runnable runnable = new Runnable() {
            public void run() {
                ViewGroup rootView = (ViewGroup) MainActivity.this.findViewById(android.R.id.content);
                rootView.setKeepScreenOn(true);

                //get the topmost view
                View topMostView = getLeafView(rootView);
                // let's add a sibling to the leaf view
                ViewGroup leafParent = (ViewGroup)topMostView.getParent();

                //inflate the android view to be added
                View view = getLayoutInflater().inflate(R.layout.activity_main, null, false);
                view.setKeepScreenOn(true);

                //add the android view on top of unity view
                leafParent.addView(view, new LayoutParams(LayoutParams.FILL_PARENT,LayoutParams.FILL_PARENT));
            }
        };

        getWindow().addFlags(WindowManager.LayoutParams.FLAG_FULLSCREEN | WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);
        handler.postDelayed(runnable, delay);

    }
    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;final-applicaiton&quot;&gt;Final Applicaiton&lt;/h3&gt;

&lt;p&gt;The “hello World” text is part of the android layout&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/DSAVnfw.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The files for unity and android project can be found in the below &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bitbucket&lt;/code&gt; repository
https://bitbucket.org/pi19404/unityandroidproject/&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;http://docs.unity3d.com/Manual/PluginsForAndroid.html&lt;/li&gt;
  &lt;li&gt;https://developer.vuforia.com/library/articles/Solution/How-To-Add-Views-Over-Unity-for-Android&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>GitBucket setup on Ubuntu 14.04</title>
   <link href="http://localhost:4000/software%20installation/2015/12/18/GitBucket/"/>
   <updated>2015-12-18T00:00:00+00:00</updated>
   <id>http://localhost:4000/software%20installation/2015/12/18/GitBucket</id>
   <content type="html">&lt;h1 id=&quot;gitbucket-setup-on-ubuntu-1404&quot;&gt;GitBucket setup on Ubuntu 14.04&lt;/h1&gt;

&lt;p&gt;GitBucket is a GitHub clone powered by Scala which has easy installation and high extensibility.&lt;/p&gt;

&lt;p&gt;we will be installing GitBucket on Windows 7&lt;/p&gt;

&lt;p&gt;Download latest &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitbucket.war&lt;/code&gt; from the &lt;a href=&quot;https://github.com/gitbucket/gitbucket/releases&quot;&gt;release page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We will deploy it on Apache Tomcat 7.0
This deployment can be done from apache admistration browser by accessing the url http://localhost:8080.&lt;/p&gt;

&lt;p&gt;By selecting the option &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Select WAR file to Upload&lt;/code&gt; and then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deploy&lt;/code&gt; to deploy the application on topcat
&lt;img src=&quot;http://i.imgur.com/Ivx1VQU.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If it is deployed successfully then we can webservice is started and we can observer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitbucket&lt;/code&gt; in the list of deployed apps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/ImbMQ1g.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tomcat will always extract the contents of a war file, to a folder of the same name
Thus context path will be choosen by default as /gitbucket&lt;/p&gt;

&lt;p&gt;The gitbucket admin page can be accessed by url http://localhost:8080/gitbucket&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/6uOnJbX.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The default administrator account is root and password is root&lt;/p&gt;

&lt;p&gt;To upgrade GitBucket, only replace gitbucket.war or redeploy the war file using the TomCat Admin panel after stop GitBucket.&lt;/p&gt;

&lt;p&gt;All GitBucket data is stored in HOME/.gitbucket. In case of present installation it is C:/.gitbucket&lt;/p&gt;

&lt;p&gt;To take back up GitBucket data, copy this directory to the other disk.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://github.com/gitbucket/gitbucket&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Installing R Packages</title>
   <link href="http://localhost:4000/software/2015/09/06/soft1/"/>
   <updated>2015-09-06T00:00:00+00:00</updated>
   <id>http://localhost:4000/software/2015/09/06/soft1</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Install R Packages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let consider the installation of Knowledge Space Theory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kst&lt;/code&gt; package.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Renviron File&lt;/strong&gt;
First we create a file .Renviron in our home area&lt;/p&gt;

&lt;p&gt;Add the line &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R_LIBS=/data/Rpackages/&lt;/code&gt; to it.&lt;/p&gt;

&lt;p&gt;This means that whenever you start R, the directory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/data/Rpackages/&lt;/code&gt; is added to the list of places to look for R packages and so:&lt;/p&gt;

&lt;p&gt;**CommandLine Installation **
We just fire up an R shell and type:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;install.packages(“kst”)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;or you can explicitly pass installation path&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;install.packages(“kst”, lib=”/data/Rpackages/”)
library(kst, lib.loc=”/data/Rpackage=s/”)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The installation can also be done using GUI of RStudio by selecting the option &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tools-&amp;gt;Install Packages&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;http://www.r-bloggers.com/installing-r-packages/&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Installing R and RStudio in Ubuntu 12.04</title>
   <link href="http://localhost:4000/software/2015/09/06/soft0/"/>
   <updated>2015-09-06T00:00:00+00:00</updated>
   <id>http://localhost:4000/software/2015/09/06/soft0</id>
   <content type="html">&lt;h2 id=&quot;installing-r-and-rstudio-in-ubuntu-1204&quot;&gt;&lt;strong&gt;Installing R and RStudio in Ubuntu 12.04&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 id=&quot;universe&quot;&gt;&lt;strong&gt;Universe&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The current version of R available when a new version of Ubuntu is released is always available in the universe repository. To install R:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;sudo apt-get install r-base r-base-dev&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This will install R, the recommended packages, and files needed to install many additional packages (but not all) from within R using install.packages(“foo”). The R packages found in the universe repository are not updated when new versions of R or the recommended packages are released.&lt;/p&gt;

&lt;h2 id=&quot;cran&quot;&gt;&lt;strong&gt;CRAN&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;CRAN has current versions of R and the recommended packages available for all currently supported versions of Ubuntu. Detailed instructions for adding the CRAN repositories can be found here. The first step is to add the security key:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;sudo apt-key adv –keyserver keyserver.ubuntu.com –recv-keys E084DAB9&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Then you need to add the CRAN repository to end of /etc/apt/sources.list:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;deb http://cran.r-project.org/bin/linux/ubuntu precise/&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;R can then be installed:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;sudo apt-get update sudo apt-get install r-base r-base-dev&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The packages on CRAN are updated on a regular basis and the most recent versions will be usually be available within a couple days of their release.&lt;/p&gt;

&lt;p&gt;One advantage of using the CRAN repository is that older versions of packages are available. To install an older version of a package, it must be pinned. Directions for pinning are different for apt-get and synaptic and details can be found here.&lt;/p&gt;

&lt;h2 id=&quot;launchpad-ppa&quot;&gt;&lt;strong&gt;Launchpad PPA&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Packages for the CRAN repository are built on a Launchpad PPA called RutteR. It is possible to use the PPA itself, which includes a few more packages than the CRAN repository (JAGS and related packages, for example). Installing the PPA via apt is very simple:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;sudo add-apt-repository ppa:marutter/rrutter sudo apt-get update sudo apt-get install r-base r-base-dev&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The PPA is updated slightly quicker than the CRAN repository, but older packages are not available.&lt;/p&gt;

&lt;p&gt;If you have any questions about the CRAN or PPA repositories, please email the maintainer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Michael Rutter&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;rstudio&quot;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;RStudio is a set of integrated tools designed to help you be more productive with R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.&lt;/p&gt;

&lt;p&gt;Download the zip/tarball package for RStudio -&lt;a href=&quot;https://download1.rstudio.org/rstudio-0.99.485-amd64-debian.tar.gz&quot;&gt;RStudio 0.99.485 - Ubuntu 12.04+/Debian 8+ (64-bit)&lt;/a&gt; from the website https://www.rstudio.com/products/rstudio/download/&lt;/p&gt;

&lt;p&gt;Copy the package &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rstudio-xx.xx.xx-amd64-debian.tar.gz&lt;/code&gt; in the desired location and run the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tar&lt;/code&gt; command to decompress the package&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;tar -xvf rstudio-xx.xx.xx-amd64-debian.tar.gz&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To launch RStudio execute the binary in the package directory&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;bin/rstudio&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;references&quot;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;http://sites.psu.edu/theubunturblog/installing-r-in-ubuntu/&lt;/li&gt;
  &lt;li&gt;http://www.r-bloggers.com/download-and-install-r-in-ubuntu/&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>User Defined Library in MBED</title>
   <link href="http://localhost:4000/embedded%20firmware/2015/09/06/embed1/"/>
   <updated>2015-09-06T00:00:00+00:00</updated>
   <id>http://localhost:4000/embedded%20firmware/2015/09/06/embed1</id>
   <content type="html">&lt;p&gt;This article describes on how to add a user defined library in MBED&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Adding User Defined Library to MBED&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Following files need to be changed to add support for new libraries in mbed&lt;/p&gt;

&lt;p&gt;we will be defining a new library called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kfactory&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;build.py - main build file&lt;/li&gt;
  &lt;li&gt;libraries.py - files containing info on various libraries and build paths&lt;/li&gt;
  &lt;li&gt;path.py - files containing path definitions for various libraries&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Adding the paths for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kfactory&lt;/code&gt; source and build diretory in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;path.py&lt;/code&gt; file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; #kfactory libraries
KFACTORY=join(LIB_DIR,&quot;kfactory&quot;)
KFACTORY_LIBRARIES=join(BUILD_DIR,&quot;kfactory&quot;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This configures the build directory for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kfactory&lt;/code&gt; library&lt;/p&gt;

&lt;p&gt;Defining the source,build directories and dependent libraries in library configuration file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libraries.py&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    {
        &quot;id&quot;: &quot;kfactory&quot;,
        &quot;source_dir&quot;: KFACTORY,
        &quot;build_dir&quot;: KFACTORY_LIBRARIES,
        &quot;dependencies&quot;: [MBED_LIBRARIES,RTOS_LIBRARIES],
    },
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This adds the library dependencies for kfactory library&lt;/p&gt;

&lt;p&gt;Adding the new library in build command line parser in the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build.py&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    parser.add_option(&quot;-k&quot;, &quot;--kfactory&quot;,
                      action=&quot;store_true&quot;,
                      dest=&quot;kfactory&quot;,
                      default=False,
                      help=&quot;Compile the kfactory library&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This add a new command line argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-k&lt;/code&gt; for compilation of kfactory library&lt;/p&gt;

&lt;p&gt;Now place the library source files in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mbed/libraries/kfactory&lt;/code&gt; subdirectory&lt;/p&gt;

&lt;p&gt;To trigger the build run the command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ python build.py -t GCC_ARK -m NUCLEO_F334R8 -k
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-k&lt;/code&gt; option has been added for compiling the new library&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>MBED Setup Documentation</title>
   <link href="http://localhost:4000/embedded%20firmware/2015/09/01/embed1/"/>
   <updated>2015-09-01T00:00:00+00:00</updated>
   <id>http://localhost:4000/embedded%20firmware/2015/09/01/embed1</id>
   <content type="html">&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;MBED Setup&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;###&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mbed test suite and build scripts are Python 2.7 applications and require Python 2.7 runtime environment and &lt;a href=&quot;https://pythonhosted.org/an_example_pypi_project/setuptools.html&quot;&gt;setuptools&lt;/a&gt; to install dependencies.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install &lt;a href=&quot;https://www.python.org/download/releases/2.7&quot;&gt;Python 2.7&lt;/a&gt; programming language.&lt;/li&gt;
  &lt;li&gt;Install &lt;a href=&quot;https://pythonhosted.org/an_example_pypi_project/setuptools.html#installing-setuptools-and-easy-install&quot;&gt;setuptools&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Optionally you can install &lt;a href=&quot;https://pip.pypa.io/en/latest/installing.html&quot;&gt;pip&lt;/a&gt; which is the PyPA recommended tool for installing Python packages from command line.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Mbed SDK in its repo root directory specifies &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setup.py&lt;/code&gt; file which holds information about all packages which are dependencies for it.&lt;/p&gt;

&lt;p&gt;First, clone mbed SDK repo and go to mbed SDK repo’s directory:&lt;/p&gt;

&lt;p&gt;From a command line with Git installed, run the following command in a directory where you wish mbed to be stored:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$ git clone https://github.com/mbedmicro/mbed.git&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Or&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Download mbed sources https://github.com/mbedmicro/mbed&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Installing Tools&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Second, invoke &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setup.py&lt;/code&gt; so &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setuptools&lt;/code&gt; can install mbed SDK’s dependencies (external Python modules required by mbed SDK):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo python setup.py install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;###&lt;strong&gt;Download and Install GCC ARM ToolChain&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Download toolchain for &lt;a href=&quot;https://launchpad.net/gcc-arm-embedded/4.9/4.9-2015-q1-update/+download/gcc-arm-none-eabi-4_9-2015q1-20150306-linux.tar.bz2&quot;&gt;gcc-arm-none-eabi-4_9-2015q&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Or Follow the instructions at &lt;a href=&quot;https://launchpad.net/~terry.guo/+archive/ubuntu/gcc-arm-embedded&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, if you want to change the path to your GNU Tools for ARM Embedded Processors to a path like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c:/arm_gcc/bin&lt;/code&gt;, you simply need to have a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workspace_tools/private_settings.py&lt;/code&gt; that contains following line:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; GCC_ARM_PATH = &quot;c:/arm_gcc/bin&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;workspace-tools&quot;&gt;&lt;strong&gt;Workspace tools&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Workspace tools are set of Python scripts used off-line by Mbed SDK team to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Compile and build mbed SDK,&lt;/li&gt;
  &lt;li&gt;Compile and build libraries included in mbed SDK repo like e.g. ETH (Ethernet), USB, RTOS or CMSIS,&lt;/li&gt;
  &lt;li&gt;Compile, build and run mbed SDK tests,&lt;/li&gt;
  &lt;li&gt;Run test regression locally and in CI server,&lt;/li&gt;
  &lt;li&gt;Get library, target, test configuration (paths, parameters, names etc.).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;configure-workspace-tools-to-work-with-your-compilers&quot;&gt;&lt;strong&gt;Configure workspace tools to work with your compilers&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;we need to tell workspace tools where our compilers are.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Go &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mbed/workspace_tools/&lt;/code&gt; directory and create empty file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;private_settings.py&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ touch private_settings.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Populate this file the Python code below:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os.path&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;
 
&lt;span class=&quot;c1&quot;&gt;# ARMCC
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ARM_PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;C:/Work/toolchains/ARMCompiler_5.03_117_Windows&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ARM_BIN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ARM_PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;bin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ARM_INC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ARM_PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;include&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ARM_LIB&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ARM_PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;lib&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;n&quot;&gt;ARM_CPPLIB&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ARM_LIB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cpplib&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;MY_ARM_CLIB&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ARM_PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;lib&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;microlib&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;c1&quot;&gt;# GCC ARM
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GCC_ARM_PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;C:/Work/toolchains/gcc_arm_4_8/4_8_2013q4/bin&quot;&lt;/span&gt;
 
&lt;span class=&quot;c1&quot;&gt;# GCC CodeSourcery
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GCC_CS_PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;C:/Work/toolchains/Sourcery_CodeBench_Lite_for_ARM_EABI/bin&quot;&lt;/span&gt;
 
&lt;span class=&quot;c1&quot;&gt;# GCC CodeRed
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GCC_CR_PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;C:/Work/toolchains/LPCXpresso_6.1.4_194/lpcxpresso/tools/bin&quot;&lt;/span&gt;
 
&lt;span class=&quot;c1&quot;&gt;# IAR
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IAR_PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;C:/Work/toolchains/iar_6_5/arm&quot;&lt;/span&gt;
 
&lt;span class=&quot;n&quot;&gt;SERVER_ADDRESS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;127.0.0.1&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;LOCALHOST&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;127.0.0.1&quot;&lt;/span&gt;
 
&lt;span class=&quot;c1&quot;&gt;# This is moved to separate JSON configuration file used by singletest.py
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MUTs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Replace corresponding variable values with paths to compilers installed in your system:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARM_PATH&lt;/code&gt; for armcc compiler.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GCC_ARM_PATH&lt;/code&gt; for GCC ARM compiler.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GCC_CS_PATH&lt;/code&gt; for GCC CodeSourcery compiler.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GCC_CR_PATH&lt;/code&gt; for GCC CodeRed compiler.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IAR_PATH&lt;/code&gt; for IAR compiler.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Workspace tools will use compiler’s path variable only if you explicit ask for it from command line. You need to replace only paths for your installed compilers.&lt;/p&gt;

&lt;p&gt;Note: Settings in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;private_settings.py&lt;/code&gt; will overwrite variables with default values in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mbed/workspace_tools/settings.py&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;###&lt;strong&gt;Build System&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The mbed build system is composed of two scripts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;workspace_tools/build.py to build the libraries&lt;/li&gt;
  &lt;li&gt;workspace_tools/make.py to build and run the test projects&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both share a subset of options to specify the target microcontroller and the toolchain:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;-m MCU -t TOOLCHAIN&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If, for example, you want to build the mbed library for the LPC1768 mbed using the ARM GCC toolchain:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;python workspace_tools\build.py -m LPC1768 -t GCC_ARM&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you want to compile source files then issue the command in the root of 
project directory&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;python workspace_tools\make.py -m LPC1768 -t GCC_ARM&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;buildpy-script&quot;&gt;&lt;strong&gt;build.py script&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;It is the core script  to drive compilation, linking and building process for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mbed SDK (with libs like Ethernet, RTOS, USB, USB host).&lt;/li&gt;
  &lt;li&gt;Tests which also can be linked with libraries like RTOS or Ethernet.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Build.py&lt;/code&gt; script is a powerful tool to build mbed SDK for all available platforms using all supported by mbed cross-compilers. S&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ python build.py --help
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The command line parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-m&lt;/code&gt; specifies the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MCUs/platforms&lt;/code&gt;for which you want to build the mbed SDK. More than one MCU(s)/platform(s) may be specified with this parameter using comma as delimiter.&lt;/p&gt;

    &lt;p&gt;Example for one platform build:&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; $ python build.py -m NUCLEO_F334R8 -t GCC_ARM
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;or for many platforms:&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; $ python build.py -m NUCLEO_F303RE,NUCLEO_F334R8 -t GCC_ARM
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-t&lt;/code&gt; defined which &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toolchain&lt;/code&gt; should be used for mbed SDK build. You can build Mbed SDK for multiple toolchains using one command.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Below example will compile mbed SDK for Freescale Freedom KL25Z platform using ARM and GCC_ARM compilers:&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ python build.py -m NUCLEO_F334R8 -t ARM,GCC_ARM
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;You can combine this technique to compile multiple targets with multiple compilers.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ python build.py -m NUCLEO_F303RE,NUCLEO_F334R8 -t GCC_ARM,ARM
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Building libraries included in mbed SDK’s source code. Parameters &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-r&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-e&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-u&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-U&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-d&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-b&lt;/code&gt; will add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RTOS&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ethernet&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;USB&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;USB Host&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DSP&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;U-Blox&lt;/code&gt; libraries respectively.&lt;/p&gt;

    &lt;p&gt;Example&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ python build.py -m LPC1768 -t ARM -r -e
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can be more verbose &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-v&lt;/code&gt; especially if you want to see each compilation / linking command build.py is executing:&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  $ python build.py -t GCC_ARM -m LPC1768 -j 8 -v
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;build-mbed-sdk-library-from-sources&quot;&gt;&lt;strong&gt;Build Mbed SDK library from sources&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Now we look at how to compile the MBED SDK from sources using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build.py&lt;/code&gt;  workspace tools script.&lt;/p&gt;

&lt;p&gt;Go to the&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mbed/workspace_tools/&lt;/code&gt; directory  and type the following command to start the MBED SDK build for &lt;a href=&quot;http://developer.mbed.org/platforms/mbed-LPC1768/&quot;&gt;LPC1768&lt;/a&gt; platform using ARM compiler.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ python build.py -m LPC1768 -t GCC_ARM - j 4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For multi-threaded compilation please use option &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-j X&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt; is number of cores you want to use to compile mbed SDK.&lt;/p&gt;

&lt;p&gt;We can see for a new directory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TARGET_LPC1768&lt;/code&gt; was created in
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build/mbed&lt;/code&gt; directory which contains all the build primitives.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build/mbed&lt;/code&gt; directory contains all the generic MBED header files which are required for user defined project while  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build/mbed/TARGET_LPC1768/&lt;/code&gt; contains platform dependent header files required by user defined projects.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build/mbed/TARGET_LPC1768/TOOLCHAIN_GCC_ARM&lt;/code&gt; contains mbed SDK library &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libmbed.a&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Workspace tools track changes in source code and  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build.py&lt;/code&gt; script will recompile project with all dependencies in case of changes .&lt;/p&gt;

&lt;h2 id=&quot;makepy-script&quot;&gt;&lt;strong&gt;make.py script&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make.py&lt;/code&gt; is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mbed/workspace_tools/&lt;/code&gt; script used to build user defined projects&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make.py&lt;/code&gt; script depends on existing already built mbed SDK and library sources so you need to pre-build mbed SDK and other libraries (such as RTOS library) to link user defined projects with mbed SDK and other mbed library. To pre-build mbed SDK please use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build.py&lt;/code&gt; script.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make.py&lt;/code&gt; shares  same  subset of options as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build.py&lt;/code&gt; to specify the target microcontroller and the toolchain:&lt;/p&gt;

&lt;h3 id=&quot;directory-structure-for-user-defined-project-&quot;&gt;**Directory Structure for User Defined Project **&lt;/h3&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;—mbed&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;— workspace—src—project—Makefile&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;— workspace—src—project—src&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;— workpace—src—project—Readme.md&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;###&lt;strong&gt;Makefile&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&quot;python&quot;&gt;

DIRNAME=$(shell basename $(realpath ./))
MAKEFILE_PATH=$(realpath ../../../mbed/workspace_tools/make.py)
BUILD_DIR=$(realpath ../../build/)
SOURCE_PATH=$(realpath ./)
BUILD_PATH=$(BUILD_DIR)/$(DIRNAME)
MCU=NUCLEO_F334R8 #board or build profile name
TOOLCHAIN=GCC_ARM # tool chain
FLAGS=&quot;-D__STM__ -Wswitch&quot;
all:
        $(MAKEFILE_PATH) --source=$(SOURCE_PATH) --build=$(BUILD_PATH) -m $(MCU) -t $(TOOLCHAIN) $(FLAGS) 

clean:
        rm -r $(BUILD_PATH)

help:
        $(MAKEFILE_PATH) --help

&lt;/pre&gt;

&lt;p&gt;To get started with a  new project,simple create a new sub directory in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workspace&lt;/code&gt; and copy the Makefile  ,and execute make to compile the project&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ make 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Modify the  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FLAGS&lt;/code&gt; variable in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Makefile&lt;/code&gt; to include command line options for including MBED libraries or other compilation options&lt;/p&gt;

&lt;p&gt;###&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://developer.mbed.org/handbook/mbed-tools&lt;/li&gt;
  &lt;li&gt;https://developer.mbed.org/teams/SDK-Development/wiki/Mbed-SDK-build-script-introduction&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>OpenCV 2.4.11 Build from Sources for Linux and Android</title>
   <link href="http://localhost:4000/2015/04/18/29/"/>
   <updated>2015-04-18T00:00:00+00:00</updated>
   <id>http://localhost:4000/2015/04/18/29</id>
   <content type="html">&lt;h3 id=&quot;opencv-2411-build-from-sources-for-linux-and-android&quot;&gt;&lt;strong&gt;OpenCV 2.4.11 Build from Sources for Linux and Android&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In this article we will look at the process of compiling OpenCV 2.4.11 libraries from source for linux and Android ARM platform .&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Get the  latest snapshot from OpenCV Git repository
&lt;a href=&quot;https://github.com/Itseez/opencv&quot;&gt;https://github.com/Itseez/opencv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

cd $workspaace
git clone https://github.com/Itseez/opencv.git

&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;Install the pre-requisite packages&lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
sudo apt-get install build-essential

sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev

sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev
&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;If you are using 64-bit linux  make sure to install  ia32 shared libraries - transitional package&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cmake-gui-configure-and-build&quot;&gt;&lt;strong&gt;CMake-GUI Configure and Build&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CMake&lt;/code&gt;, the cross-platform, open-source build system consisting of a set of tools to build, test and package software. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cmake-gui&lt;/code&gt; provides graphical user interface to configure build.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a temporary directory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linux_build&lt;/code&gt;, where the Makefiles, project files as well the object files and output binaries will be generated&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Launch the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cmake-gui&lt;/code&gt; utility&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
cd linnux_build
cmake-gui

&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;Select the OpenCV source and configure build directory where the make files will be generated.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/25/a292.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Press Configure button&lt;/li&gt;
  &lt;li&gt;You may receive error configuration completed/terminated with errors .The error log can be seen in the Tab below the option panel . The errors are occuring because the default options enabled are not compatible  with software packages installed in the system&lt;/li&gt;
  &lt;li&gt;To resolve the errors deselect the packages that are not required
This will check for dependencies,download pre-requisites and generate system dependent Makefiles for compilation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;For example :&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Disabled &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WITH_IPP&lt;/code&gt; as i was facing come download issues with the package&lt;/li&gt;
  &lt;li&gt;Disable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WITH_CUDA&lt;/code&gt;  i do not have GPU  and do not  require CUDA toolkit .&lt;/li&gt;
  &lt;li&gt;Disable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WITH_GSTREAMER&lt;/code&gt; since i had not installed the GSTREAMER package&lt;/li&gt;
  &lt;li&gt;Disable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WITH_OPENEXR&lt;/code&gt; ( OPENEXR is format for HDR images )&lt;/li&gt;
  &lt;li&gt;Disable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WITH_V4l&lt;/code&gt;               ( v4l is video for linux package)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Enabling Additional options that are disabled by default&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Enable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WITH_TBB&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Enable WITH_OPENCL  (OpenCL is open source package for hetrogenous parallel programming)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Generate Build Files&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;After selecting the suitable options that i press the generate option ,and errors are resolved and make files are generated successfully.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Alternatively you can install the required packages to resolve the dependencies and then proceed to generate the make files successfully.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make and Install&lt;/strong&gt;
  Go to the build directory configured in the cmake-gui and enter&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

  make 
  make install

&lt;/pre&gt;

&lt;h3 id=&quot;android-build&quot;&gt;&lt;strong&gt;Android build&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Download Require Android Pre-Requisites like Android SDK and Android NDK&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://developer.android.com/sdk/installing.html&quot;&gt;Android SDK - Android Software development toolkit&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://developer.android.com/tools/sdk/ndk/index.html&quot;&gt;Android NDK&lt;/a&gt; - The NDK is a toolset that allows you to implement parts of your app using native-code languages such as C and C++&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Export the following &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ANDROID_SDK&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ANDROID_NDK&lt;/code&gt; shell variables indicating the path where the packages have been downloaded/installed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

export ANDROID_SDK = /opt/android-sdk-linux
export ANDROID_NDK = /opt/android-ndk-r7

&lt;/pre&gt;

&lt;h4 id=&quot;cmake-toolchain-build&quot;&gt;&lt;strong&gt;CMake Toolchain Build&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Create build directory for Android&lt;/li&gt;
  &lt;li&gt;After Launching the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cmake-gui&lt;/code&gt; click on configure .&lt;/li&gt;
  &lt;li&gt;Select the options &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Specify toolchain for cross compiling&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/25/a291.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The toolchain can be found found at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${opencv_source_dir}/platforms/android/android.toolchain.cmake&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Select the toolchain and click on finish to complete the CMake configuration process and Click on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Generate&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Remember to enable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BUILD_SHARED_LIBS&lt;/code&gt; . This options will generate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.so&lt;/code&gt; library files&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/25/a293.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To fasten the build you can disable following options&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;BUILD_EXAMPLES&lt;/li&gt;
      &lt;li&gt;BUILD_ANDROID_EXAMPLES&lt;/li&gt;
      &lt;li&gt;BUILD_OPENEXR&lt;/li&gt;
      &lt;li&gt;BUILD_PERF_TESTS&lt;/li&gt;
      &lt;li&gt;BUILD_WITH_DEBUG_INFO&lt;/li&gt;
      &lt;li&gt;BUILD_TBB&lt;/li&gt;
      &lt;li&gt;WITH_CUDA&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;android-and-arm-options&quot;&gt;&lt;strong&gt;Android and ARM options&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Select the android-sdk ,android-api level and ARM architecture type if required .Else it will be compiled with default options&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Application binary interface (ABI)&lt;/code&gt; describes the low-level interface between a computer program and the  OS or another program&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An embedded-application binary interface (EABI) specifies standard conventions for file formats, data types, register usage, stack frame organization, and function parameter passing of an embedded software program.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;armeabi ( ABI for ARM-based CPUs that support &lt;em&gt;at&lt;/em&gt; &lt;em&gt;least&lt;/em&gt; the ARMv5TE instruction set)&lt;/li&gt;
      &lt;li&gt;arm-v7a (dedicated hardware floating point registers provided by CPU)&lt;/li&gt;
      &lt;li&gt;armeabi-v7a with VFPV3 ( hardware floating point registers and computations to boost floating point performance with VFPV3 as floating-point unit )&lt;/li&gt;
      &lt;li&gt;arm-v7a-with neon (hardware floating point and vectorization and SIMD operation)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the present build i selected arm-v7a is selected as my Mobile device processor belongs to family that supports armv7 architecture and has hardware floating point support.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;This information can be observed when mobile is connected to USB and launching the DDMS utility or viewing the device details in eclipse plugin etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;cross---compiling-for-android&quot;&gt;&lt;strong&gt;Cross - Compiling for Android&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;As in the ubuntu case if errors are encountered un-check the packages not applicable or not   required   or install the required packages to resolve the error and click on generate.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Initiate the compilation by typing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make&lt;/code&gt; command in the build directory&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After completion the android compatible OpenCV libraries can be found in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${build_dir}/libs/armeabi-v7a&lt;/code&gt; directory&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;pre-compiled-libraries-downloaded&quot;&gt;&lt;strong&gt;Pre Compiled Libraries Downloaded&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;The precompiled libraries  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;armeabi-v7a&lt;/code&gt; libraries can be found at git repository &lt;a href=&quot;https://github.com/pyVision/root&quot;&gt;https://github.com/pyVision/root&lt;/a&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;adroid/OpenCV_Android_libraries/2.4.11/armeabi-v7a&lt;/code&gt;&lt;br /&gt;
subdirectory .&lt;/p&gt;

&lt;p&gt;The libraries for other &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eabi &lt;/code&gt; format will be added soon&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Android Native Programming C/C++ with OpenCV</title>
   <link href="http://localhost:4000/2015/04/17/28/"/>
   <updated>2015-04-17T00:00:00+00:00</updated>
   <id>http://localhost:4000/2015/04/17/28</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;This article describes how to setup Eclipse for using C/C++ OpenCV Programs together with Java in Android projects.&lt;/p&gt;

&lt;h3 id=&quot;installation-and-code-compilation&quot;&gt;Installation and Code Compilation&lt;/h3&gt;

&lt;p&gt;Before Proceeding make sure that you have all the below software components installed and configured in Eclipse&lt;/p&gt;

&lt;p&gt;To develop your application via Eclipse, you need to install the following softwares.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.oracle.com/technetwork/java/javase/downloads/index.html&quot;&gt;Java Development Kit 7+ (JDK)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Eclipse, the “&lt;a href=&quot;https://www.eclipse.org/downloads/packages/eclipse-ide-java-developers/lunasr2&quot;&gt;Eclipse IDE for Java Developers&lt;/a&gt;” .&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://developer.android.com/sdk/installing.html&quot;&gt;Android SDK&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Android Development Tools for Eclipse, aka ADT Plugin.
    &lt;ul&gt;
      &lt;li&gt;Update site: https://dl-ssl.google.com/android/eclipse/&lt;/li&gt;
      &lt;li&gt;Detailed instructions can be found at&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Android NDK
    &lt;ul&gt;
      &lt;li&gt;The NDK is a toolset that allows you to implement parts of your app using native-code languages such as C and C++&lt;/li&gt;
      &lt;li&gt;It can be downloaded from http://developer.android.com/tools/sdk/ndk/index.html&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Eclipse CDT for C/C++ support  - Install CDT from Eclipse update site http://download.eclipse.org/tools/cdt/releases/indigo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;OpenCV 2.4.3 precompile libraries - https://github.com/pi19404/OpenCVAndroid/blob/master/opencv_arm_libs.tar.gz&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;OpenCV sources
Get the OpenCV Sources from github repository https://github.com/Itseez/opencv.git&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Configuring Eclipse&lt;/strong&gt;
Configure eclipse to set the path for Android SDK and NDK&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Eclipse -&amp;gt; Window -&amp;gt; Preferences -&amp;gt; Android -&amp;gt; set path to SDK
&lt;img src=&quot;http://pi19404.github.io/pyVision/images/25/a251.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Eclipse -&amp;gt; Window -&amp;gt; Preferences -&amp;gt; Android -&amp;gt; NDK -&amp;gt; set path to the NDK
&lt;img src=&quot;http://pi19404.github.io/pyVision/images/25/a252.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;android-project&quot;&gt;&lt;strong&gt;Android Project&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;In Eclipse create Android project to which you want to add C/C++ code 
For this examples a project called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AndroidOpenCV_v1&lt;/code&gt; has been created&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;setting-up-for-cross-compilation&quot;&gt;&lt;strong&gt;Setting up for Cross Compilation&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;The Eclipse IDE provides features for adding native C/C++ support to an existing Android based project.&lt;/li&gt;
  &lt;li&gt;Right click on an Android project and select &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Android Tools -&amp;gt; Add native support&lt;/code&gt; or Select the option &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;File-&amp;gt;New-&amp;gt;Other&lt;/code&gt; from main menu) and select &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Convert to a C/C++ Project&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/25/a253.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Enter the desired library name : In the present article we enter the name as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OpenVision&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This will configure the Android Project for the native build.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You will be able to see additional options related to native build
    &lt;ul&gt;
      &lt;li&gt;C/C++ build&lt;/li&gt;
      &lt;li&gt;C/C++ general&lt;/li&gt;
      &lt;li&gt;CDT builder option in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Project Properties-&amp;gt;Builders&lt;/code&gt; menu&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;configure-cc-compilation-for-android-build&quot;&gt;&lt;strong&gt;configure C/C++ compilation for android build&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;In the project properties Choose &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C/C++ Build&lt;/code&gt; option&lt;/li&gt;
  &lt;li&gt;uncheck Use default build command and configure ndk-build as a build command&lt;/li&gt;
  &lt;li&gt;ndk-build script must be in your path or specify absolute path&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/25/a254.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In Behaviour setting uncheck clean (ndk-build cleans project automatically on build and removes all existing libraries from libs/armeabi folders)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/25/a255.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;configure-the-include-paths&quot;&gt;&lt;strong&gt;Configure the include paths&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Choose C/C++ General-&amp;gt;Paths and Symbols and configure include path&lt;/li&gt;
  &lt;li&gt;Add path to include directory which is located in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/opt/android-ndk-r7/platforms/android-15/arch/arm/usr/include&lt;/code&gt; subdirectory which contains Android ndk to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GNU C&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GNU C++ &lt;/code&gt; include paths.&lt;/li&gt;
  &lt;li&gt;Kindly note that Include path depends on target for which you are compiling (android-15 in my case — i.e. Android 4.0.3)&lt;/li&gt;
  &lt;li&gt;We also need to include the paths to opencv include directory&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/25/a256.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;

&lt;p&gt;We will run a OpenCV based faced detection algorithm using OpenCV on the camera frames provided by AndroidCamera&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The native code C/C++ consists of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;haarcascade_test.cpp&lt;/code&gt; file which provides a high level interface to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;haarcascade&lt;/code&gt; class which run the face detection algorithm on downsampled version input frame and draws a Circle in the ROI the face is detected.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The frames from camera are obtained using android API calls which is then passed to OpenCV for processing which is implemented in C/C++ and the output frame is passed back to android java code for rendering&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a jni subdirectory with main file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OpenVision.cpp&lt;/code&gt; file and associated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Android.mk&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Application.mk&lt;/code&gt; make file&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Android.mk&lt;/code&gt; file.This file is like a standard make file containing the include paths,source files,library dependencies etc.Few of the syntaxes are specific to android build and explanation is provided in the comments&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Application.mk&lt;/code&gt; file contains flags related to target,platform as well as additional compilation and link flags&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Create/Copy all of the following files in the jni directory
    &lt;ul&gt;
      &lt;li&gt;haarcascade_test.cpp&lt;/li&gt;
      &lt;li&gt;haarcascade_test.hpp&lt;/li&gt;
      &lt;li&gt;Android.mk&lt;/li&gt;
      &lt;li&gt;Application.mk&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;copy the precompiled OpenCV libraries required for the build to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libs/armeabi2&lt;/code&gt; directory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;We can now build the project which will trigger the native build and create a shared library &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libOpenVision_naive.so&lt;/code&gt; in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libs/armeabi&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libs/areabi-v7a&lt;/code&gt; directories and all the OpenCV 3rd party libraries are also copied from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libs/armeabi2&lt;/code&gt; directory to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libs/armeabi&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libs/armeabi-v7a&lt;/code&gt; directories.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However this cannot be directly called from java code since we have not configured JNI interface.&lt;/p&gt;

&lt;h4 id=&quot;java-jni-interface-code&quot;&gt;&lt;strong&gt;Java JNI Interface code&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Java Native Interface (JNI) enables code written in Java to access (bind to) code written in C/C++ (and vice-versa) thereby allowing developers to gain access to low-level OS APIs, reuse legacy code, and possibly even boost the execution performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The java code for JNI Interface is as below&lt;/p&gt;

&lt;pre class=&quot;brush:java&quot;&gt;

public class ProcessImage {
public static String APPLICATION_DIR=Environment.getExternalStorageDirectory().getAbsolutePath()+&quot;/Android&quot;;
        
    	static { 
    	    //loading native and dependent 3rd party libraries
    		System.loadLibrary(&quot;opencv_core&quot;);
    		System.loadLibrary(&quot;opencv_imgproc&quot;);
    		System.loadLibrary(&quot;opencv_highgui&quot;);
    		System.loadLibrary(&quot;opencv_objdetect&quot;);
    		System.loadLibrary(&quot;OpenVision_native&quot;);
    		}
   

        public ProcessImage() { 
 initDetector(APPLICATION_DIR+&quot;/haarcascade_frontalface_alt.xml&quot;);
        }

        public native int run(int width, int height, byte yuv[], int[] rgba);
        public native void initDetector(String name);
            
}
&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;The function that are required to be called by java are declared as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;native&lt;/code&gt; and their implementation is found in C/C++ files.&lt;/li&gt;
  &lt;li&gt;We also need to load the native library in this case &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libOpenVision.so&lt;/code&gt; in the java code which is performed by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;System.loadLibrary&lt;/code&gt; function call.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;generate-header-files&quot;&gt;Generate Header Files&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;We must now generate header files for JNI interface.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compile the android Project .This will generate the class file for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ProcessImage.java&lt;/code&gt; file in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bin/classes&lt;/code&gt; directory&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;javah&lt;/code&gt; is a  JDK tool that builds C-style header files from a given Java class that includes native methods&lt;/li&gt;
  &lt;li&gt;Run the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;javah&lt;/code&gt; command with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jni&lt;/code&gt; switch to generate the JNI Interface files with classpath including the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bin/classes&lt;/code&gt; directory&lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
 
cd jni
javah -jni -classpath `pwd`/../bin/classes -jni com.example.androidopencv_v1.ProcessImage

&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This will generate the header file
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;com_example_androidopencv_v1_ProcessImage.h&lt;/code&gt; 
 in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jni&lt;/code&gt; directory&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now we modify the file so that it calls the routines defined in the native calls&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The function declarations in header files are of the form&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

JNIEXPORT void JNICALL Java_com_example_androidopencv_1v1_ProcessImage_initDetector
  (JNIEnv * env, jobject obj, jstring javaString)

JNIEXPORT jint JNICALL Java_com_example_androidopencv_1v1_ProcessImage_run
  (JNIEnv *, jobject, jint, jint, jbyteArray, jintArray)

&lt;/pre&gt;

&lt;h4 id=&quot;native-method-arguments&quot;&gt;Native Method Arguments&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JNIEnv *env&lt;/code&gt;, an interface pointer (pointer to a pointer) to a function table, where each entry in the table is a pointer to a function that enables Java-to-C/C++ integration.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The second argument is typically of type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jobject obj&lt;/code&gt;, which is a reference to the object on which the method is invoked for In case of instance methods and ` jclass clazz`which is a reference to the class in which the method is defined in case of static methods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;C/C++ primitives vary in size, depending on the platform while java primitives are well defined.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To provide inter-operability with C/C++, jni.h defines the following between standard java and C/C++ primitives (taken from https://thenewcircle.com/s/post/1292/jni_reference_example)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The JNI includes a number of pre-defined reference types (passed as opaque references in C) that correspond to different kinds of Java object types:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/25/a257.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All objects passed to native code and returned from JNI functions are considered local references .Thus datatypes like arrays,strings need to be accessed via a accessor functions provided by JNI.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/25/a258.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

//access string
const char *name = (*env)-&amp;gt;GetStringUTFChars(env, javaString, 0);

//access integer array
jint *_yuv = env-&amp;gt;GetIntArrayElements(array, NULL);

//access byte array
jbyte* _bgra=env-&amp;gt;GetByteArrayElements(oarray,NULL);

&lt;/pre&gt;

&lt;p&gt;when the accessed data is no longer required we need to free the data&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

//release memory taken by UTF string locally
env-&amp;gt;ReleaseStringUTFChars(javaString, name);

//release memory take up by integer array _yuv
env-&amp;gt;ReleaseIntArrayElements(array, _yuv, 0);

//release memory take by the byte array _bgra
env-&amp;gt;ReleaseByteArrayElements(oarray, _bgra, 0);

&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;Also we will be including the header file in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OpenVision.cpp&lt;/code&gt; file.Thus &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g++&lt;/code&gt; will be used for compilation of the code . If the code is included in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c&lt;/code&gt;  file then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcc&lt;/code&gt; will be used for compilation&lt;/li&gt;
  &lt;li&gt;Kindly note that the syntax for extracting data from JNI datatypes differ are different for C/C++ functions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;building-the-project&quot;&gt;&lt;strong&gt;Building the project&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;After building the project the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libOpenVision.so &lt;/code&gt; files can be found in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libs/armeabi&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libs/armeabi-v7a&lt;/code&gt; directories.These have been cross-compiled for use on android based devices.&lt;/p&gt;

&lt;p&gt;These can now be loaded and called from java application using JNI Interface&lt;/p&gt;

&lt;pre class=&quot;brush:java&quot;&gt;

    	static { 
    	    //loading native and dependent 3rd party libraries
    		System.loadLibrary(&quot;opencv_core&quot;);
    		System.loadLibrary(&quot;opencv_imgproc&quot;);
    		System.loadLibrary(&quot;opencv_highgui&quot;);
    		System.loadLibrary(&quot;opencv_objdetect&quot;);    	    
    		System.loadLibrary(&quot;OpenVision_native&quot;);                 
    		}


&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;System.loadLibrary&lt;/code&gt; function call is used to load the native libraries.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The native functions can be called as standard java functions&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush:java&quot;&gt;

ProcessImage opencvp=new ProcessImage();

//initializes the face detector
initDetector(APPLICATION_DIR+&quot;/haarcascade_frontalface_alt.xml&quot;);

//android camera call back function,where data is returned as  byte array
public void onPreviewFrame(byte[] data, Camera camera)
{

// mDrawOnTop.mImageWidth - image width
//mDrawOnTop.mImageHeight - image height

//allocate memory for output bitmap data
 mDrawOnTop.mRGBData = new int[mDrawOnTop.mImageWidth * mDrawOnTop.mImageHeight];

//allocating and copying byte array
mDrawOnTop.mYUVData = new byte[data.length];
System.arraycopy(data, 0, mDrawOnTop.mYUVData, 0, data.length);

//calling the native function 
opencvp.run(mDrawOnTop.mImageWidth, mDrawOnTop.mImageHeight, mDrawOnTop.mYUVData, mDrawOnTop.mRGBData)

}
&lt;/pre&gt;

&lt;h3 id=&quot;code-1&quot;&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The android project can be found at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;adroid/AndroidOpenCV_v1&lt;/code&gt; subdirectory of  https://github.com/pyVision/root github repository&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Git Subtree Installation and Usage on Linux</title>
   <link href="http://localhost:4000/2015/04/17/27/"/>
   <updated>2015-04-17T00:00:00+00:00</updated>
   <id>http://localhost:4000/2015/04/17/27</id>
   <content type="html">&lt;h3 id=&quot;installing-git-subtree-from-sources&quot;&gt;Installing Git Subtree From Sources&lt;/h3&gt;

&lt;p&gt;Git subtree is a git helper to combine and manage multiple repos in one project.&lt;/p&gt;

&lt;p&gt;This article describes how to install git subtree on Ubuntu 12.04 OS.&lt;/p&gt;

&lt;p&gt;Since subtree is mostly collection of shell scripts  and was introduced in moved to contrib package as part git version &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1.7.9.11&lt;/code&gt;.The latest version of git available from standard sources on Ubuntu 12.04 is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1.7.9.5&lt;/code&gt;.Hence we will build and install the latest version of git from sources&lt;/p&gt;

&lt;h3 id=&quot;dependencies&quot;&gt;Dependencies&lt;/h3&gt;
&lt;p&gt;Prior to installing git we need to install some dependencies&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

sudo apt-get install libcurl4-gnutls-dev libexpat1-dev gettext libz-dev libssl-dev build-essential


&lt;/pre&gt;

&lt;h3 id=&quot;compile-git-modules&quot;&gt;Compile Git Modules&lt;/h3&gt;

&lt;p&gt;Get the git sources from &lt;a href=&quot;https://github.com/git/git&quot;&gt;enter link description here&lt;/a&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

make prefix=/usr/ all

&lt;/pre&gt;

&lt;h3 id=&quot;install-git&quot;&gt;Install Git&lt;/h3&gt;

&lt;p&gt;For a global install it using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/local&lt;/code&gt; prefixes&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

sudo make prefix=/usr/ install

&lt;/pre&gt;

&lt;h3 id=&quot;compile-and-install-git-contrib&quot;&gt;Compile and Install git contrib&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Go into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git/contrib/subtree&lt;/code&gt; directory&lt;/li&gt;
  &lt;li&gt;Build and install git subtree&lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

make
sudo make prefix=/usr install

&lt;/pre&gt;

&lt;h3 id=&quot;configure-a-subtree&quot;&gt;Configure a subtree&lt;/h3&gt;

&lt;p&gt;We have created a repository called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt;
we will configure the git respository  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;adroid&lt;/code&gt; as subtree in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt; repository&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

git remote add adroid git@github.com:pyVision/adroid.git
git subtree add --prefix=adroid/ adroid master

&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First the repository of sub-directory is added as remote of the present repository&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The git subtree add command tells git to add that repo’s code into a path in the parent’s project, specified by prefix in this case &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;adroid&lt;/code&gt; .The last prefix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;master&lt;/code&gt; is the branch you are pulling code from.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A complete reference for git subtree management can be found at
&lt;a href=&quot;https://medium.com/@v/git-subtrees-a-tutorial-6ff568381844&quot;&gt;enter link description here&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>DC motor Characteristic Curvers</title>
   <link href="http://localhost:4000/2015/04/11/26/"/>
   <updated>2015-04-11T00:00:00+00:00</updated>
   <id>http://localhost:4000/2015/04/11/26</id>
   <content type="html">&lt;h3 id=&quot;dc-motor-equilibrium-analysis&quot;&gt;DC Motor Equilibrium Analysis&lt;/h3&gt;

&lt;p&gt;In this article we will look  DC motor characteric curves and see how we can obtain parameters to model the DC motor from commonly provided specifications in motor Datasheet.&lt;/p&gt;

&lt;h3 id=&quot;equilibrium-analysis&quot;&gt;Equilibrium analysis&lt;/h3&gt;

&lt;p&gt;In the article “DC motor First order approximation” we saw how to characterize a DC motor using basic parameters provided in the motor Datasheet and predict the speed and responsiveness of motor for any given input voltage.&lt;/p&gt;

&lt;p&gt;Let us look at some of parameters in mode detail under steady state conditions with a load.We apply a voltage and wait  till the motor achieves constant velocity .&lt;/p&gt;

&lt;p&gt;If we apply a voltage source to the motor terminal and mechanical load torque $\tau$ on the rotor&lt;/p&gt;

&lt;p&gt;$ b \dot{\theta} = K_{t} i -\tau $&lt;/p&gt;

&lt;p&gt;$  Ri = V - K_{b}\dot{\theta}$&lt;/p&gt;

&lt;p&gt;Solving we get&lt;/p&gt;

&lt;p&gt;$ i = \frac{b \dot{\theta} + \tau}{K_t}$&lt;/p&gt;

&lt;p&gt;$ V= R \frac{b \dot{\theta} + \tau}{K_t} +K_{b}\dot{\theta}$&lt;/p&gt;

&lt;p&gt;$ \dot{\theta} \big(\frac{Rb}{K_{t}}+K_{b}\big) = V - \frac{R*\tau}{K_{t}} $&lt;/p&gt;

&lt;p&gt;$ \dot{\theta} \big(b+\frac{K_{b}K_{t}}{R}\big) = \frac{VK_{t}}{R} - \tau $&lt;/p&gt;

\[\tau =\frac{VK_{t}}{R} -\dot{\theta} \big(b+\frac{K_{b}K_{t}}{R}\big)\]

\[\dot{\theta}  = (\frac{VK_{t}}{R} - \tau ){\big(b+\frac{K_{b}K_{t}}{R}\big)}^{-1}\]

&lt;ul&gt;
  &lt;li&gt;Slope of torque speed graph indicates inverse proportionality due to negative sign.&lt;/li&gt;
  &lt;li&gt;The linear relationship between the torque provided by a motor
and the speed at which it operates is specific to the input voltage at the motor’s leads&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus we can plot the load v/s speed graph at specified voltage and power specifications.&lt;/p&gt;

&lt;p&gt;Let us plot the torque v/s speed graph for nominal voltage of 12V
and let load torque vary from 0 to 50 mNm and observe the RPM and power requirement of the motor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/24/242.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;At zero torque, the speed is highest. This speed is called no-load speed $\dot{\theta}_{n}$. The no-load speed can easily be calculated from the applied voltage and the speed constant of the motor.&lt;/li&gt;
  &lt;li&gt;Enhancing the load torque leads to a linear reduction of the speed.&lt;/li&gt;
  &lt;li&gt;Increasing the torque further reduces speed up to the point where the motor stops. The corresponding torque is called stall torque $\tau_{s}$&lt;/li&gt;
  &lt;li&gt;The current corresponding to the stall torque is named starting current
or no load current $i_{o}$&lt;/li&gt;
  &lt;li&gt;A higher voltage results in a higher no-load speed&lt;/li&gt;
  &lt;li&gt;The speed-torque gradient is unaffected with increase in voltage.  Changing the applied voltage results in a parallel shift of the speed-torque line&lt;/li&gt;
  &lt;li&gt;A strong motor is characterized by a flat speed-torque line. The speed drops only slightly when the load torque is enhanced. The value of the gradient is small.&lt;/li&gt;
  &lt;li&gt;On a weaker motor the speed drop is larger. The speed-torque line is steeper and the value of its gradient is high.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stall-torque&quot;&gt;Stall Torque&lt;/h3&gt;
&lt;p&gt;The torque provided by the motor when the rotor no longer turns is known as the stall torque.The torque of motor at zero  speed.&lt;/p&gt;

&lt;p&gt;It can be calculated as&lt;/p&gt;

&lt;p&gt;$\tau_{s} =\frac{VK_{t}}{R} $&lt;/p&gt;

&lt;h3 id=&quot;no-load-speed&quot;&gt;No Load Speed&lt;/h3&gt;

&lt;p&gt;The speed at which the rotor turns when no torque is provided is known as the no-load speed. The no-load speed is called as such since it is the speed at which the rotor will turn when there is no load connected.&lt;/p&gt;

\[\dot{\theta} = (\frac{V*K_{t}}{R}){\big(b+\frac{K_{b}K_{t}}{R}\big)}^{-1}\]

&lt;p&gt;No load speed is maximum speed that can be attained by the motor at the specified voltage .&lt;/p&gt;

&lt;h3 id=&quot;stall-current&quot;&gt;Stall Current&lt;/h3&gt;
&lt;p&gt;The stall current is the current drawn by the motor when rotor no longer turns. The current draw is proportional to the torque.The stall current is the maximum current drawn by the motor under equilibrium condition.&lt;/p&gt;

&lt;p&gt;$ i = \frac{\tau}{K_t}$
$ i = \frac{V}{R}$&lt;/p&gt;

&lt;h3 id=&quot;no-load-current&quot;&gt;No load Current&lt;/h3&gt;
&lt;p&gt;$ i = \frac{b\dot{\theta}}{K_{t}}$&lt;/p&gt;

&lt;h3 id=&quot;equilibrium-torquespeed--current&quot;&gt;Equilibrium Torque,Speed &amp;amp; Current&lt;/h3&gt;
&lt;p&gt;For a fixed voltage the equilibrium torque,speed and current can be expressed as&lt;/p&gt;

&lt;p&gt;\(\tau =\tau_{s} -\dot{\theta} \big(b+\frac{K_{b}K_{t}}{R}\big)\)
\(\dot{\theta}  = \theta_{n} - \tau {\big(b+\frac{K_{b}K_{t}}{R}\big)}^{-1}\)
\(I =  I_{S} - \dot{\theta} \big(\frac{K_{b}}{R}\big)\)&lt;/p&gt;

&lt;h3 id=&quot;motor-datasheet&quot;&gt;Motor Datasheet&lt;/h3&gt;
&lt;p&gt;Not all motor manufactures provide parameters that can be used directly to model the motor.&lt;/p&gt;

&lt;p&gt;The parameter like L,R,torque constant,backemf constant etc are called as Intrinsic Parameters of the motor.&lt;/p&gt;

&lt;p&gt;The values like stall torque,stall current,no load speed and current are the values measured values and most often found in datasheets.&lt;/p&gt;

&lt;p&gt;For example the important specifications of motor are often given in the form&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Nominal voltage 4.5V&lt;/li&gt;
  &lt;li&gt;No load speed 23000rpm&lt;/li&gt;
  &lt;li&gt;no load current 70mA&lt;/li&gt;
  &lt;li&gt;Stall torque of 0.34mNm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let us assume that we have also some intrinsic parameters like R&lt;/p&gt;

&lt;p&gt;We know that the stall torque
$\tau_{s} =\frac{VK_{t}}{R} $&lt;/p&gt;

&lt;p&gt;Thus we can compute $K_{t}$ from the equation
The no load speed is given by&lt;/p&gt;

\[\dot{\theta_{n}}  = (\frac{VK_{t}}{R} ){\big(b+\frac{K_{b}K_{t}}{R}\big)}^{-1}\]

\[\frac{\tau_{s}}{\dot{\theta_{n}}} =\big(b+\frac{K_{b}K_{t}}{R}\big)\]

&lt;p&gt;viscous friction coefficient $b$ can be computed from this equation.&lt;/p&gt;

&lt;p&gt;Just by observing the graph we can determine the slope and the intercept ie  $\big(b+\frac{K_{b}K_{t}}{R}\big)$ and $\frac{VK_{t}}{R}$.&lt;/p&gt;

&lt;p&gt;Thus knowing the nominal voltage and estimate of R we can determine $K_{t}$ and $b$ from the graph itself.&lt;/p&gt;

&lt;p&gt;Thus we have all the information we need for the first order model of a DC Motor.&lt;/p&gt;

&lt;p&gt;For example for the portscape 216E motor http://www.portescap.com/sites/default/files/26n58_specifications.pdf we found that simple armature resistance yeilded a value of 15.2 
and based on that given specifications ans above formulat the intrinsic model parameters computed are&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
Kt =0.0236
b =9.9544e-07
&lt;/pre&gt;

&lt;p&gt;These are approximately equal to the values in the datasheet.&lt;/p&gt;

&lt;h3 id=&quot;mechanical-power&quot;&gt;Mechanical Power&lt;/h3&gt;

&lt;p&gt;The mechanical power $P$ delivered by the motor is&lt;/p&gt;

&lt;p&gt;$P = \tau \dot{\theta}$
$P =\tau_{s}\dot{\theta} -\dot{\theta}^2 \big(b+\frac{K_{b}K_{t}}{R}\big) $&lt;/p&gt;

&lt;p&gt;We can find the velocity that provides the maximum power by computing the gradient wrt $\dot{\theta}$&lt;/p&gt;

&lt;p&gt;$\dot{\theta_{max}} = 0.5\tau_{s}\big(b+\frac{K_{b}K_{t}}{R}\big)^{-1}$&lt;/p&gt;

&lt;p&gt;$\dot{\theta_{max}} = 0.5\dot{\theta_{n}}$&lt;/p&gt;

&lt;p&gt;Thus maximum mechanical power is achieved when motor is running at half the no load velocity.&lt;/p&gt;

&lt;p&gt;At this point the torque is 
$\tau_{max} = 0.5\tau_{s}$&lt;/p&gt;

&lt;p&gt;Thus maximum power is achieved when the torque is half of stall torque.&lt;/p&gt;

&lt;p&gt;Since the current draw is proportional to the torque, then the current at the point of maximum power transfer shall also be one half of the stall current.&lt;/p&gt;

&lt;p&gt;$I_{max} = 0.5I_{s}$&lt;/p&gt;

&lt;p&gt;The maximum mechanical power produced by the motor is as follows
$P_{max}=\frac{1}{4} \tau_{s}\dot{\theta}_{n}$&lt;/p&gt;

&lt;p&gt;Thus maximum efficiency is obtained when speed is half  no load speed
and torque is half stall torque.&lt;/p&gt;

&lt;p&gt;Power delivered by the motor is area of a rectangle under the torque/speed curve with one corner at the origin and another corner at a point on the curve&lt;/p&gt;

&lt;p&gt;We can see the maximum area is obtained when&lt;/p&gt;

&lt;p&gt;$\dot{\theta_{max}} = 0.5\dot{\theta_{n}}$&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;$\tau_{max} = 0.5\tau_{s}$&lt;/p&gt;

&lt;p&gt;corresponding to the peak in the power v/s speed curve&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/24/251.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;motor-efficiency&quot;&gt;Motor Efficiency&lt;/h3&gt;

&lt;p&gt;The efficiency $\eta$ of a motor is defined as the ratio between the input electrical power and output mechanical power&lt;/p&gt;

&lt;p&gt;$\eta = \frac{P}{VI}$&lt;/p&gt;

&lt;p&gt;$I =  \frac{V_{S}}{R} - \dot{\theta} \big(\frac{K_{b}}{R}\big)$
$P_{i} = VI = V\frac{V_{S}}{R} - V\dot{\theta} \big(\frac{K_{b}}{R}\big)$
$ \tau =\tau_{s} -\dot{\theta} \big(b+\frac{K_{b}K_{t}}{R}\big)$
$P_{o} = \frac{K_{t}V_{s}}{R}\dot{\theta} -\dot{\theta}^2 \big(b+\frac{K_{b}K_{t}}{R}\big)$&lt;/p&gt;

&lt;p&gt;$\eta = \frac{\frac{K_{t}V_{s}}{R}\dot{\theta} -\dot{\theta}^2 \big(b+\frac{K_{b}K_{t}}{R}\big)}{V\frac{V_{S}}{R} - V\dot{\theta} \big(\frac{K_{b}}{R}\big)}$&lt;/p&gt;

&lt;p&gt;$\dot{\theta}_{me} = \frac{bc - \sqrt{b^2c^2-abcd}}{bd}$&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;$a = \tau_{s}-Kt*i_{o},$&lt;/p&gt;

&lt;p&gt;$b = b + \frac{K_{b}K_{t}}{R},$&lt;/p&gt;

&lt;p&gt;$c= \frac{V*Vs}{R},$&lt;/p&gt;

&lt;p&gt;$d = V\big(\frac{K_{b}}{R}\big),$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/24/252.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see from the plot that the  velocity at max efficiency is 4237 rpm
And operating of max power is different from that of max efficiency.&lt;/p&gt;

&lt;p&gt;The first order model we enables us to plot the motor characteristics plot and observe the behavious of motor to a good degree of accuracy.&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;
&lt;p&gt;The code to compute and plot the various DC motor curves can be found in file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dcmotor2&lt;/code&gt; at bitbucket repository &lt;a href=&quot;https://bitbucket.org/snippets/pi19404/55ep&quot;&gt;https://bitbucket.org/snippets/pi19404/55ep&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;http://www.portescap.com/sites/default/files/wp_drivers_for_ironless_dc_motors.pdf&lt;/li&gt;
  &lt;li&gt;http://dsc.ijs.si/Damir.Vrancic/Files/dp7091.pdf
-http://www.precisionmicrodrives.com/application-notes-technical-guides/application-bulletins/ab-025-using-spice-to-model-dc-motors&lt;/li&gt;
  &lt;li&gt;http://www.ti.com/lit/an/slua076/slua076.pdf&lt;/li&gt;
  &lt;li&gt;http://mechanical.poly.edu/faculty/slee/ME3411/Exp3.pdf&lt;/li&gt;
  &lt;li&gt;http://mplab.ucsd.edu/tutorials/dc.pdf
-http://ece.ut.ac.ir/Classpages/S89/ECE425/HW5SOL.pdf
-http://www.site.uottawa.ca/~rhabash/ELG3311A4Ch9.pdf&lt;/li&gt;
  &lt;li&gt;http://mplab.ucsd.edu/tutorials/&lt;/li&gt;
  &lt;li&gt;http://www.cecs.uci.edu/publication/d-d-gajski-s-abdi-a-gerstlauer-and-g-schirner-embedded-system-design-modeling-synthesis-verification/&lt;/li&gt;
  &lt;li&gt;http://www.ni.com/white-paper/4074/en/&lt;/li&gt;
  &lt;li&gt;http://www.maxonmotor.com/medias/sys_master/8798985748510.pdf&lt;/li&gt;
  &lt;li&gt;http://www.me.umn.edu/courses/me2011/arduino/technotes/dcmotors/&lt;/li&gt;
  &lt;li&gt;http://lancet.mit.edu/motors/motors3.html&lt;/li&gt;
  &lt;li&gt;http://www.delta-line.com/data/download/Brush_DC_Motor_Basics.pdf&lt;/li&gt;
  &lt;li&gt;http://www.gearseds.com/files/Lesson3_Mathematical%20Models%20of%20Motors.pdf&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>First Order Approximation of DC Motor</title>
   <link href="http://localhost:4000/2015/04/10/25/"/>
   <updated>2015-04-10T00:00:00+00:00</updated>
   <id>http://localhost:4000/2015/04/10/25</id>
   <content type="html">&lt;h3 id=&quot;-dc-motor-model&quot;&gt;&lt;i class=&quot;icon-pencil&quot;&gt;&lt;/i&gt; DC Motor Model&lt;/h3&gt;

&lt;p&gt;In this article we will look at how to model DC motor and first order model that enables us to quickly predict response of DC motor.&lt;/p&gt;

&lt;h3 id=&quot;-dc-motor&quot;&gt;&lt;i class=&quot;icon-pencil&quot;&gt;&lt;/i&gt; DC Motor&lt;/h3&gt;

&lt;p&gt;A DC motor is a energy conversion device that converts electric energy to mechanical energy and is a common actuator used in many robotic systems.&lt;/p&gt;

&lt;p&gt;A DC motor driven by a voltage and rotates at constant speed that is
proportional to the applied voltage and provides rotational torque which when coupled with wheels provides translational motion of the vehicle.&lt;/p&gt;

&lt;p&gt;Usually the motor armature has some resistance that limits its ability to accelerate, so the motor will have some delay between the change in input voltage and the resulting change in speed.&lt;/p&gt;

&lt;h3 id=&quot;dc-motor-model&quot;&gt;DC motor Model&lt;/h3&gt;

&lt;p&gt;The input of the system is the voltage source $v$ applied to the motor’s armature, while the output is the rotational speed of the shaft $\dot{\theta}$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/24/243.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;R and L are the armature resistance and inductance.$e$ is called as backemf and is voltage induced in the rotor which results in torque $t$ being produced.&lt;/p&gt;

&lt;p&gt;A viscous friction model is generally assumed for DC motor .Where $b\dot{\theta}$ is term which opposed or damps the torque and $b$ is called as damping constant.&lt;/p&gt;

&lt;h4 id=&quot;electric-equivalent-circut-of-dc-motor&quot;&gt;Electric Equivalent circut of DC motor&lt;/h4&gt;

&lt;p&gt;The power supply voltage $v$  is equal to the sum of the voltage drop produced by the current I in the ohmic resistance R of the rotor winding, and the voltage $e$ induced in the rotor&lt;/p&gt;

&lt;p&gt;Thus
$v = I * R+ e$&lt;/p&gt;

&lt;p&gt;The voltage $e$ induced in the rotor is proportional to the angular velocity ω of the rotor&lt;/p&gt;

&lt;p&gt;For an armature controlled DC-motor the torque produced by the motor  is proportional to only the armature current $i$ by a constant factor $K_{t}$ called  motor torque constant.&lt;/p&gt;

\[T = K_{t} (i-i_{o})\]

&lt;p&gt;$i_{o}$ is the no-load current consumed by the motor to generate torque required to overcome the internal frictions of motor.&lt;/p&gt;

&lt;p&gt;The back emf  e is voltage induced in the rotor and is proportional to the angular velocity of the shaft by a constant factor $K_{t}$ called the motor/torque constant.&lt;/p&gt;

\[e = K_{t} \dot{\theta}\]

&lt;h4 id=&quot;ideal-motor&quot;&gt;Ideal Motor&lt;/h4&gt;
&lt;p&gt;For ideal motor with no losses mechanical power is equal to the electrical power dissipated by the back emf in the armature&lt;/p&gt;

&lt;p&gt;$T\omega = e i$
$K_{t} i \omega = K_{e}\omega i$&lt;/p&gt;

&lt;p&gt;The motor torque and back emf constants are equal, that is, $K_{t} = K_{e}$&lt;/p&gt;

&lt;h4 id=&quot;real-motor&quot;&gt;Real Motor&lt;/h4&gt;

&lt;p&gt;The mechanical power developed by the motor is equal to the sum electrical power given to the motor and the power dissipated&lt;/p&gt;

&lt;p&gt;$P_{e} = P{m} + P_{l}$&lt;/p&gt;

&lt;p&gt;$P_{e} = v * I$
$P_{m} = T * ω$
$P_{l} =R * I^2 + K_{t}* I_{O} * ω$&lt;/p&gt;

&lt;p&gt;The efficiency of the motor is defined as
$\eta = \frac{P_{m}}{P_{e}}$&lt;/p&gt;

&lt;h4 id=&quot;motor-model&quot;&gt;Motor Model&lt;/h4&gt;

&lt;p&gt;We assume a loss less model in obtaining the first order approximation
of the motor model&lt;/p&gt;

&lt;p&gt;From Newton’s 2nd law it can be derieved&lt;/p&gt;

\[J\ddot{\theta} + b \dot{\theta} = K_{t} i\]

&lt;p&gt;where 
$ J $is the  moment of inertia of the rotor 
$ b $ is the motor viscous friction constant 
$ K $ electromotive force constant
$ i $ is the armature current&lt;/p&gt;

&lt;p&gt;Based on Kirchhoff’s voltage law we obtain the equation&lt;/p&gt;

\[L \frac{di}{dt} + Ri = V - K_{b}\dot{\theta}\]

&lt;p&gt;$R $ electric resistance&lt;br /&gt;
$ L $ is electric inductance
$K $ is motor torque constant &lt;br /&gt;
$ V $ is input voltage to the motor
$i $ is the input armature current&lt;/p&gt;

&lt;p&gt;Applying the Laplace transform, the above modeling equations can be expressed in terms of the Laplace variable s.&lt;/p&gt;

\[s(Js + b)\Theta(s) = K_{t}I(s)\]

\[(Ls + R)I(s) = V(s) - K_{b}s\Theta(s)\]

&lt;p&gt;We arrive at the following open-loop transfer function by eliminating $I(s)$ between the two above equations, where the rotational speed is considered the output and the armature voltage is considered the input.&lt;/p&gt;

\[P(s) = \frac {\dot{\Theta}(s)}{V(s)} = \frac{K_{t}}{(Js + b)(Ls + R) + K_{b}K_{t}} \qquad [ \frac{rad/sec}{V}]\]

&lt;p&gt;The Block diagram can be expressed as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/24/244.png&quot; alt=&quot;enter image description here&quot; /&gt;
( In the above diagram Kf=b ,Km=Kb=Kt)&lt;/p&gt;

&lt;p&gt;Similarity the transfer function of torque v/s angular velocity can be expressed as&lt;/p&gt;

\[\frac {\dot{\Theta}(s)}{\tau(s)} = \frac{-(Ls+R)}{(Js + b)(Ls + R) + K_{b}K_{t}} \qquad\]

&lt;p&gt;We assume that ration $\frac{L}{R} \le \frac{J_{m}}{b}$ and we obtain the first order approximation of DC motor as&lt;/p&gt;

\[P(s) = \frac {\dot{\Theta}(s)}{V(s)} = \frac{\frac{K_{t}}{R}}{(Js + b)+ \frac{K_{b}K_{t}}{R}} \qquad\]

\[P(s) = \frac {\dot{\Theta}(s)}{V(s)} = \frac{\frac{K_{t}}{R}}{Js + (b+ \frac{K_{b}K_{t}}{R})} \qquad\]

&lt;p&gt;$\dot{\theta(t)} = v(t) \frac{K_{t}}{R&lt;em&gt;J}&lt;/em&gt;e^{-\frac{(b+ \frac{K_{b}K_{t}}{R})}{J}}$&lt;/p&gt;

&lt;p&gt;Thus given an input voltage we can compute the instantaneous angular velocity of the motor .&lt;/p&gt;

&lt;h3 id=&quot;example&quot;&gt;Example&lt;/h3&gt;

&lt;p&gt;Let us look at  a example of coreless dc motors by portscape&lt;/p&gt;

&lt;p&gt;For the present example we use the motor constants from following datasheet&lt;/p&gt;

&lt;p&gt;http://www.portescap.com/sites/default/files/26n58_specifications.pdf&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

J = 6*10-7 kg.cm2
b = 0
Kt = 23.90 mNm/Am
Ke = 2.50 mNm/Am
R = 10 ohms
L = 0.8e-3 h

&lt;/pre&gt;

&lt;p&gt;For ironless motors viscous friction/damping constant $b=0$&lt;/p&gt;

&lt;h4 id=&quot;open-loop-response&quot;&gt;Open Loop Response&lt;/h4&gt;

&lt;p&gt;First we will look at the open loop response of the DC motor . Given input voltage what is the angular velocity of the motor.&lt;/p&gt;

&lt;p&gt;For this we look at the step response of the motor&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/24/245.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the plot we see that when 1 Volt is applied to the system the motor can only achieve a maximum speed of 41.8 rad/sec or 399 rpm .Also it takes around 98 msec to reach the desired speed.&lt;/p&gt;

&lt;h4 id=&quot;-pole-zero-plot&quot;&gt;&lt;i class=&quot;icon-pencil&quot;&gt;&lt;/i&gt; Pole Zero Plot&lt;/h4&gt;

&lt;p&gt;We can see that the motor model is a second order LTI system,which enables us to predict the characteristics using pole zeros of the transfer function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/24/247.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the the pole zero plot we can see that the open-loop transfer function has two real poles, one at s = -1.24e4 and one at s =-95.9&lt;/p&gt;

&lt;p&gt;Since both poles are real, there is no oscillation in the step response (or overshoot) as we have already seen.&lt;/p&gt;

&lt;h4 id=&quot;-first-order-model&quot;&gt;&lt;i class=&quot;icon-pencil&quot;&gt;&lt;/i&gt; First Order Model&lt;/h4&gt;

&lt;p&gt;We expect the slower of the two poles will dominate the dynamics. That is, the pole at s = -95.9 primarily determines the speed of response of the system and the system behaves similarly to a first-order system.&lt;/p&gt;

\[P(s) = \frac {\dot{\Theta}(s)}{V(s)} = \frac{\frac{K_{t}}{R}}{Js + (b+ \frac{K_{b}K_{t}}{R})} \qquad\]

&lt;p&gt;Thus the location of the dominant pole is at&lt;/p&gt;

&lt;p&gt;$s= \frac{(b+ \frac{K_{b}K_{t}}{R})}{J}$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/24/246.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the plot we see that when 1 Volt is applied to the system the motor can only achieve a maximum speed of 41.8 rad/sec or 399 rpm .Also it takes around 98 msec to reach the desired speed which is same as the original second order model&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/24/248.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can also see that pole is located at s=-95.2 which agrees with dominant pole of the second order pole which was located at s=-95.9&lt;/p&gt;

&lt;p&gt;Now we approximate the motor using first order model and compare the step responses of the model.&lt;/p&gt;

&lt;p&gt;With a first-order system, the settling time is equal to&lt;/p&gt;

\[T_s = 4 \tau\]

&lt;p&gt;where $\tau$ is the time constant which in this case is 0.0105. Therefore, our first-order model has a settling time of 42 ms which is off by order of some ms since the pole is far away from origin.&lt;/p&gt;

&lt;p&gt;we can see that a first-order approximation of our motor system is relatively accurate&lt;/p&gt;

&lt;h4 id=&quot;information-from-step-response&quot;&gt;Information from Step Response&lt;/h4&gt;

&lt;p&gt;Thus we can predict the motor RPM fairly accurately using the first order system to determine what input voltage to be set to obtain the desired RPM under no load conditions.&lt;/p&gt;

&lt;p&gt;The step response gives us idea of voltage vs RPM graph and also the idea about responsiveness of the motor.&lt;/p&gt;

\[\dot{\theta(t)} = v(t) \frac{K_{t}}{R*J}*e^{-\frac{(b+ \frac{K_{b}K_{t}}{R})}{J}t}\]

\[P(s) = \frac {\dot{\Theta}(s)}{V(s)} = \frac{\frac{K_{t}}{R}}{Js + (b+ \frac{K_{b}K_{t}}{R})} \qquad\]

&lt;p&gt;using final value theorem we get&lt;/p&gt;

&lt;p&gt;$\dot{\Theta} =  V\frac{\frac{K_{t}}{R}}{(b+ \frac{K_{b}K_{t}}{R})}$&lt;/p&gt;

&lt;p&gt;This equations helps us compute the angular velocity of motor for any given input voltage.&lt;/p&gt;

&lt;p&gt;Let us look at the speed v/s voltage graph&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/24/249.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As per the datasheet the rpm at nomial voltage of 12V is 4735rpm
which agrees approximately with our estimated of 4975 rpm.We have overestimated the RPM since we have not taken into account the losses in the motor.&lt;/p&gt;

&lt;h5 id=&quot;losses-in-motor&quot;&gt;Losses in Motor&lt;/h5&gt;
&lt;p&gt;$v = I * R+ e$
The voltage induced in the motor is not $V$ but $V-RI$&lt;/p&gt;

&lt;p&gt;In general It is assumed that at no-load no torque is provided, some torque $\tau_{f}$is required to keep the inertia of the rotor turning and to overcome frictional torques and current draw under is $i_{o}$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/24/250.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thus if we take this into account we get estimate of speed at 12V as 4731 rpm which is approximately equal to value of 4735 rpm specified in the datasheet.&lt;/p&gt;

&lt;p&gt;This can be verified for different motor models specified in the datasheet&lt;/p&gt;

&lt;h4 id=&quot;code&quot;&gt;Code&lt;/h4&gt;
&lt;p&gt;The matlab code can be found at bitbucket repository 
https://bitbucket.org/snippets/pi19404/55ep&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Libgdx Setup and 3D animation Test Utility On Linux Platform</title>
   <link href="http://localhost:4000/2015/04/09/24/"/>
   <updated>2015-04-09T00:00:00+00:00</updated>
   <id>http://localhost:4000/2015/04/09/24</id>
   <content type="html">&lt;h3 id=&quot;libgdx-setup-on-linux-platform&quot;&gt;Libgdx Setup On Linux Platform&lt;/h3&gt;

&lt;p&gt;In this article we will look at setting up Libgdx on Ubuntu 12.04 from sources and running a sample test utility for 3D models.&lt;/p&gt;

&lt;p&gt;libGDX is a cross-platform Java game development framework based on OpenGL (ES) that works on Windows, Linux, Mac OS X, Android, your WebGL enabled browser and iOS.&lt;/p&gt;

&lt;h3 id=&quot;setting-up-eclipse&quot;&gt;Setting up Eclipse&lt;/h3&gt;

&lt;p&gt;To develop your application via Eclipse, you need to install the following softwares.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.oracle.com/technetwork/java/javase/downloads/index.html&quot;&gt;Java Development Kit 7+ (JDK)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Eclipse, the “&lt;a href=&quot;https://www.eclipse.org/downloads/packages/eclipse-ide-java-developers/lunasr2&quot;&gt;Eclipse IDE for Java Developers&lt;/a&gt;” .&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://developer.android.com/sdk/installing.html&quot;&gt;Android SDK,&lt;/a&gt; you only need the SDK, Install the latest stable Android platforms via the SDK Manager in eclipse.&lt;/li&gt;
  &lt;li&gt;Android Development Tools for Eclipse, aka ADT Plugin. Update site: &lt;a href=&quot;https://dl-ssl.google.com/android/eclipse/&quot;&gt;https://dl-ssl.google.com/android/eclipse/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://git-scm.com/&quot;&gt;Git&lt;/a&gt;  distributed version control system&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;libgdx-download&quot;&gt;Libgdx download&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Get the libgdx sources by cloning the &lt;a href=&quot;https://github.com/libgdx/libgdx&quot;&gt;libgdx github&lt;/a&gt; repository or &lt;a href=&quot;https://github.com/libgdx/libgdx/archive/master.zip&quot;&gt;downloading&lt;/a&gt; the repository&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let the source be placed at the path &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$WORKSPACE/libgdx&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;libgdx-eclipse-setup&quot;&gt;Libgdx Eclipse Setup&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Select the option File-&amp;gt;Import&lt;/li&gt;
  &lt;li&gt;Choose General-&amp;gt;Existing Projects into Workspace&lt;/li&gt;
  &lt;li&gt;Navigate to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$WORKSPACE/libgdx&lt;/code&gt; directory and complete the import process.&lt;/li&gt;
  &lt;li&gt;All the libgdx projects would have been imported into eclipse&lt;/li&gt;
  &lt;li&gt;Build all the sources successfully to make sure there are no dependency issues&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;testing-libgdx&quot;&gt;Testing libgdx&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Run the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gdx-tests-lwjgl&lt;/code&gt; project .Which is default testing utility shipped with the sources.&lt;/li&gt;
  &lt;li&gt;Select the various options to test and checkout the features of libgdx&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;testing-utility-for-linux&quot;&gt;Testing utility for Linux&lt;/h3&gt;
&lt;p&gt;We will look at a small test program that can be used to test 3D static and animated models on linux&lt;/p&gt;

&lt;p&gt;libgdx allows static models files in obj,g3dj,gd3b format and animated models in g3dj and g3db format.&lt;/p&gt;

&lt;p&gt;Presently the test utility has the following basic features&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Display the models at their default location&lt;/li&gt;
  &lt;li&gt;Sequentiall play animations&lt;/li&gt;
  &lt;li&gt;Rotate the view about the origin&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The test utility has following main classes&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BaseCharacter&lt;/code&gt; - used for static models&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AnimatedBaseCharacter&lt;/code&gt; - used for dynamic models&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GameWorld&lt;/code&gt; - configures the world,camera and initiates the rendering process&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To change the scale or position of the object you can modify the following lines in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initializeActors&lt;/code&gt; function in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GameWorld.java&lt;/code&gt;  file&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
		Pose p=new Pose();
		p.mPosition=new Vector3(0,0,0f);
		p.mOrientation.setFromAxis(new Vector3(1,0,0),90);		
		p.mScale=new Vector3(1f,1f,1f);
&lt;/pre&gt;

&lt;p&gt;The models files are placed in same directory as the project in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assets&lt;/code&gt; sub-directory.&lt;/p&gt;

&lt;p&gt;Upon running the project the GUI will display the list of models in the folder.Upon selection of the model a new window will open which will display the 3D view of the model.&lt;/p&gt;

&lt;p&gt;If the model is a obj file directly a static model is loaded ,&lt;/p&gt;

&lt;p&gt;The source for the testing utility can be downloaded from the bitbucket repository &lt;a href=&quot;https://pi19404@bitbucket.org/pi19404/libgdx-apps.git&quot;&gt;https://pi19404@bitbucket.org/pi19404/libgdx-apps.git&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Import the project into eclipse to and select the suitable model file to run&lt;/p&gt;

&lt;h3 id=&quot;converting-the-models&quot;&gt;Converting the models&lt;/h3&gt;
&lt;p&gt;Models can be converted from fbx to g3dj or g3db format using 
The source for the conversion utility can be found at &lt;a href=&quot;https://github.com/libgdx/fbx-conv&quot;&gt;link&lt;/a&gt; or can be downloaded from build server &lt;a href=&quot;http://libgdx.badlogicgames.com/fbx-conv/&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Currently libgdx only supports pose transformations and material transformations are not supported.&lt;/p&gt;

&lt;p&gt;Use the command
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fbx-conv -f -o G3DJ A.fbx &lt;/code&gt;  or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fbx-conv -f -o G3DB A.fbx &lt;/code&gt; 
to convert the fbx model files to libgdx compatible files.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>De-Compiling Android Application</title>
   <link href="http://localhost:4000/software/2015/04/03/23/"/>
   <updated>2015-04-03T00:00:00+00:00</updated>
   <id>http://localhost:4000/software/2015/04/03/23</id>
   <content type="html">&lt;h4 id=&quot;de-compiling-android-application&quot;&gt;De-compiling Android Application&lt;/h4&gt;

&lt;p&gt;All applications for Android phones are distributed as APK Files. These files contain all the source,UI , images and other media necessary to run the application on your phone.&lt;/p&gt;

&lt;p&gt;The decompilation process isn’t perfect and the code you get won’t reflect the original code 100%. Things like variable names, loop structures and anonymous inner classes might be interpreted differently especially if the application will obfuscate your code.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Getting the APK File&lt;/li&gt;
  &lt;li&gt;Download the apk file from net of fetch it from the android phone&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This command gives the list of installed apk files with associated path&lt;/p&gt;

    &lt;pre class=&quot;brush:python&quot;&gt; 

 adb shell pm list packages -f -3 

 &lt;/pre&gt;

    &lt;ul&gt;
      &lt;li&gt;This command will get the apk file on the PC&lt;/li&gt;
    &lt;/ul&gt;

    &lt;pre class=&quot;brush:python&quot;&gt;

 adb pull /data/app/programming.apk 

 &lt;/pre&gt;

    &lt;ul&gt;
      &lt;li&gt;Change file extension to .zip&lt;/li&gt;
    &lt;/ul&gt;

    &lt;pre class=&quot;brush:python&quot;&gt;

 cp programming.apk programming.zip

 &lt;/pre&gt;

    &lt;ul&gt;
      &lt;li&gt;Extract the contents of the file&lt;/li&gt;
    &lt;/ul&gt;

    &lt;pre class=&quot;brush:python&quot;&gt;

  unzip programming.zip 

  &lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;Decompiling the code
    &lt;ul&gt;
      &lt;li&gt;Download &lt;a href=&quot;http://code.google.com/p/dex2jar/&quot;&gt;dex2jar&lt;/a&gt;  utility&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Get the classes.dex file unzipped contents&lt;/p&gt;

    &lt;pre class=&quot;brush:python&quot;&gt;

 ${PATH}/dex2jar/d2j-dex2jar.sh classes.dex 

 &lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This will convert the dex to jar file and output will result in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;classes_dex2jar.jar&lt;/code&gt; file.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Download java decompilers like &lt;a href=&quot;http://jd.benow.ca/&quot;&gt;jd-gui &lt;/a&gt; or &lt;a href=&quot;https://github.com/deathmarine/Luyten&quot;&gt;Luyten&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Open decompiler gui and open the jar file 		&lt;br /&gt;
 For Lyuten type the command&lt;/p&gt;

    &lt;pre class=&quot;brush:python&quot;&gt;

 java -jar luyten.jar 

 &lt;/pre&gt;

    &lt;p&gt;you will be able to view all the sources of project,external jar files etc.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Save the source file&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Select option File -&amp;gt; Save All Sources 
         This will save all the files on the file system.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At this point the sources have been decompiled but not the resources etc&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Decompiling Resources
    &lt;ul&gt;
      &lt;li&gt;Download the latest version of &lt;a href=&quot;http://ibotpeaches.github.io/Apktool/&quot;&gt;apktool&lt;/a&gt; which is a tool for reverse engineering APK files. This tool dissassembles the code in Smali  format along with resources . We will user only resources obtained from this utility as java file had been obtained from earlier approach.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;pre class=&quot;brush:python&quot;&gt;
	
  java -jar apktool_2.0.0rc4.jar d programming.apk
	
  &lt;/pre&gt;

    &lt;p&gt;This will create programming folder containing source and resource files&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At this point we have recovered both the source and resource files.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Serial Bluetooth Terminal on Linux</title>
   <link href="http://localhost:4000/linux/2015/04/03/22/"/>
   <updated>2015-04-03T00:00:00+00:00</updated>
   <id>http://localhost:4000/linux/2015/04/03/22</id>
   <content type="html">&lt;p&gt;In this article we will look at how to establish a serial connection with devices via bluetooth .&lt;/p&gt;

&lt;h3 id=&quot;device-pairing&quot;&gt;&lt;i class=&quot;icon-pencil&quot;&gt;&lt;/i&gt;Device Pairing&lt;/h3&gt;

&lt;p&gt;We need to pair the device before starting the communication&lt;/p&gt;

&lt;p&gt;You can pair the device using standard bluetooth GUI utilities or command line utilties.In this article we will look at command-line utilities to do so.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[BlueZ](https://apps.ubuntu.com/cat/applications/bluez/)&lt;/code&gt; is the official Linux Bluetooth protocol stack
This package contains tools and system daemons for using Bluetooth devices.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install this package on your system.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install bluez bluez-tools

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Get the Bluetooth adapter details on linux&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The hciconfig command gives the Bluetooth adapter details.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
#hciconfig
hci0:	Type: BR/EDR  Bus: USB
	BD Address: 18:F4:6A:DE:EB:9D  ACL MTU: 1021:8  SCO MTU: 64:1
	UP RUNNING PSCAN ISCAN 
	RX bytes:2492 acl:3 sco:0 events:99 errors:0
	TX bytes:1421 acl:2 sco:0 commands:82 errors:0
&lt;/pre&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hci0&lt;/code&gt; is out bluetooth adapter name .&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Get the Bluetooth device Mac Address&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hcitool scan&lt;/code&gt; command gives us the bluetooth device mac address and name&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

hcitool scan
Scanning ...
	98:D3:31:30:1A:BA	HC-05

&lt;/pre&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;98:D3:31:30:1A:BA&lt;/code&gt; is the mac address of HC-05 bluetooth module&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pairing the devices&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bluez-simple-agent&lt;/code&gt; utility can be used to pair to the device with mac address &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;98:D3:31:30:1A:BA&lt;/code&gt; using the adapter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hci0&lt;/code&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

bluez-simple-agent hci0 98:D3:31:30:1A:BA
RequestPinCode (/org/bluez/3772/hci0/dev_98_D3_31_30_1A_BA)
Enter PIN Code: 1234
Release
New device (/org/bluez/3772/hci0/dev_98_D3_31_30_1A_BA)

&lt;/pre&gt;

&lt;p&gt;To remove a paired device enter the command&lt;/p&gt;
&lt;pre class=&quot;brush:python&quot;&gt;
bluez-simple-agent hci0 98:D3:31:30:1A:BA remove
&lt;/pre&gt;

&lt;h3 id=&quot;-rfcomm-communication-protocol&quot;&gt;&lt;i class=&quot;icon-pencil&quot;&gt;&lt;/i&gt; RFComm communication protocol&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;RF Comm&lt;/strong&gt; protocol handles the transmission of information and control commands from one communication device to another communication device (such as from a computer to an accessory device) that is performed serially (one bit at a time).&lt;/p&gt;

&lt;p&gt;The Bluetooth system uses radio frequency communication (RFComm) protocol to setup and coordinate the transfer of serial data.&lt;/p&gt;

&lt;p&gt;RFCOMM is intended to cover applications that make use of the serial ports of the devices in which they reside.In the present application the Bluetooth Device used is HC-05 which communicates using UART protocol with devices.&lt;/p&gt;

&lt;p&gt;RFCOMM only allows one connected client per channel and supports
up to 60 simultaneous connections between two BT device&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Configure Rfcomm
First Step is to configure the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rfcomm&lt;/code&gt; module on linux to communicate with our Bluetooth module&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/bluetooth/rfcomm.conf&lt;/code&gt; file make the following entry&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
rfcomm0 {
        bind no;
        device 98:D3:31:30:1A:BA;
        channel 1;
        comment &quot;Serial Port&quot;;
        }
        
&lt;/pre&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;98:D3:31:30:1A:BA&lt;/code&gt; is the mac address of our device.&lt;/p&gt;

&lt;p&gt;The “bind no” is important, otherwise it will try to automatically bind to device and not when the user wants.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start the connection&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Type the command  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo rfcomm connect 0&lt;/code&gt; to start the communication&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

#sudo rfcomm connect 0
Connected /dev/rfcomm0 to 98:D3:31:30:1A:BA on channel 1
Press CTRL-C for hangup

&lt;/pre&gt;

&lt;p&gt;This will also create a serial device file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/dev/rfcomm0&lt;/code&gt;  ,though which we can communicate with the bluetooth device.&lt;/p&gt;

&lt;p&gt;If you have errors, restart the bluetooth service and enter the above command again&lt;/p&gt;

&lt;h3 id=&quot;-communication-with-bluetooth-device&quot;&gt;&lt;i class=&quot;icon-pencil&quot;&gt;&lt;/i&gt; Communication with Bluetooth Device&lt;/h3&gt;

&lt;p&gt;User graphical serial terminal like &lt;a href=&quot;http://cutecom.sourceforge.net/&quot;&gt;CuteCom&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/dev/rfcomm0&lt;/code&gt; device to communicate with the bluetooth device.&lt;/p&gt;

&lt;p&gt;You can send and receive commands in ASCII or HEX format via serial terminal interface.&lt;/p&gt;

&lt;p&gt;Various bluetooth devices will correspond to different serial device files though which we can communicate with them.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Conjugate Quadrature Filter Bank - Deriving Daubechies Filter Coefficients</title>
   <link href="http://localhost:4000/signal%20processing/2014/12/21/21/"/>
   <updated>2014-12-21T00:00:00+00:00</updated>
   <id>http://localhost:4000/signal%20processing/2014/12/21/21</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In this article we will look at the concept of conjugate quadrature filter bank and process computationally derive the Daubechies wavelet filter coefficient
for any filter length&lt;/p&gt;

&lt;h3 id=&quot;conjugate-quadrature-filters-cqfs&quot;&gt;Conjugate Quadrature Filters (CQFs)&lt;/h3&gt;

&lt;p&gt;Following up on the Perfect Reconstruction Filter Banks.In this article we will look at look at realization of perfect reconstruction filter bank using Conjugate Quadrature Filters.&lt;/p&gt;

&lt;p&gt;Let $x[n]$ represent  a real sequence and $X(z)$ represent the Z transform.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conjugate  Filter&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$X(z) = \sum x[n] z^{-k}$
$Y(z) = X^{*}(Z)=\sum x[n] z^{k} = X(z^{-1})$
$y[n] = x[N-n]$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Quadrature Filters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If $Y(z) = H_{0}(-z)$
$y[n] = \sum_{k} (-1)^{k} h_{o}[k] = (-1)^{n}h_{o}[n]$&lt;/p&gt;

&lt;p&gt;The filter responses are symmetric about $\Omega = \pi / 2$&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$&lt;/td&gt;
      &lt;td&gt;Y(e^{j\Omega})&lt;/td&gt;
      &lt;td&gt;=&lt;/td&gt;
      &lt;td&gt;Y(e^{j(\pi - \Omega)})&lt;/td&gt;
      &lt;td&gt;$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Conjugate  Quadrature Filters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The relation between the analysis HPF (high pass filter) and analysis LPF (low pass filter) of the filter
bank forms conjugate quadrature relationship&lt;/p&gt;

&lt;p&gt;$H_{1}(z) = z^{-d}H_{0}(-z^{-1})$&lt;/p&gt;

&lt;p&gt;$G_{0}(z)  = H_{1}(-z)$&lt;/p&gt;

&lt;p&gt;$G_{1}(z)  = -H_{0}(-z)$&lt;/p&gt;

&lt;p&gt;$h_{1}(n) = \sum_{k=0}^{N} h_{0}(N-k-d)(-1)^{k} $&lt;/p&gt;

&lt;p&gt;$ z^{-d}$ is used to introduce causality&lt;/p&gt;

&lt;p&gt;$H_{1}(e^{j\omega}) = e^{-j\omega d}H_{0}(-e^{-j\omega}) $&lt;/p&gt;

&lt;p&gt;Magnitude response is given by
$|H_{1}(e^{j\omega})| = |H_{0}(-e^{-j\omega})| $&lt;/p&gt;

&lt;p&gt;if $H_{0}(z)$ is a low pass filter with real impulse response then
$H_{0}(e^{-j\omega}) =H_{0}^{*}(e^{-j\omega})$&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$&lt;/td&gt;
      &lt;td&gt;H_{1}(e^{j\omega})&lt;/td&gt;
      &lt;td&gt;=&lt;/td&gt;
      &lt;td&gt;H_{0}&lt;/td&gt;
      &lt;td&gt;(e^{-j(\omega+\pi)})&lt;/td&gt;
      &lt;td&gt;$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Condition For Perfect Reconstruction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$T_{1}(z) =[H_{0}(z) H_{1}(-z) - H_{1}(z) H_{0}(-z)] =C_{0} z^{-D} $&lt;/p&gt;

&lt;p&gt;$T_{1}(z) = H_{0}(z) {-z}^{-d}H_{0}(z^{-1}) - z^{-d}H_{0}(-z^{-1})H_{0}(-z)=c_{0}z^{-d}$&lt;/p&gt;

&lt;p&gt;${-1}^{d} H_{0}(z) H_{0}(z^{-1}) - H_{0}(-z^{-1})H_{0}(-z) = C_{0}$&lt;/p&gt;

&lt;p&gt;if D is odd&lt;/p&gt;

&lt;p&gt;$ H_{0}(z) H_{0}(z^{-1}) + H_{0}(-z^{-1})H_{0}(-z) = C_{0}=2$&lt;/p&gt;

&lt;p&gt;$ H_{0}(e^{j\omega}) H_{0}(e^{-j\omega}) + H_{0}(-e^{-j\omega})H_{0}(-j\omega) = C_{0}=2$&lt;/p&gt;

&lt;p&gt;For real impulse response&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$&lt;/td&gt;
      &lt;td&gt;H_{0}(e^{j\omega})&lt;/td&gt;
      &lt;td&gt;^2 +&lt;/td&gt;
      &lt;td&gt;H_{0}(e^{-j(\omega+\pi)})&lt;/td&gt;
      &lt;td&gt;^2 = C_{0}=2$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;$K_{0}(z) = H_{0}(z) H_{0}(z^{-1}) $&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$K_{0}(e^{j\omega}) = H_{0}(e^{j\omega}) H_{0}(e^{-j\omega}) =&lt;/td&gt;
      &lt;td&gt;H_{0}(z)&lt;/td&gt;
      &lt;td&gt;^2$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;$K_{0}(-z) = H_{0}(-z) H_{0}(-z^{-1}) $&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$K_{0}(-e^{j\omega}) = H_{0}(-e^{j\omega}) H_{0}(-e^{-j\omega}) =&lt;/td&gt;
      &lt;td&gt;H_{0}(-z)&lt;/td&gt;
      &lt;td&gt;^2$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With odd D&lt;/p&gt;

&lt;p&gt;$K_{0}(z)+ K_{0}(-z)= C_{0}=2$&lt;/p&gt;

&lt;p&gt;with even D&lt;/p&gt;

&lt;p&gt;$K_{0}(z)- K_{0}(-z) = C_{0}=2$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Normalization Constraint&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$ x[n] \Rightarrow X(z) $&lt;/p&gt;

&lt;p&gt;$ x[-n] \Rightarrow X(z^{-1}) $&lt;/p&gt;

&lt;p&gt;$K_{0}(z) = H_{0}(z) H_{0}(z^{-1}) $ are convolution of signal $h_{0}[n]$ and $h_{0}[-n]$ which is correlation function $R_{k}[n]$ which is symmetrical&lt;/p&gt;

&lt;p&gt;$K_{0}(-z) = H_{0}(-z) H_{0}(-z^{-1}) $ are convolution of signal $h_{0}[n]e^{j\pi n}$ and $h_{0}[-n]e^{-j\pi n}$ which is correlation function $R_{k}[n]e^{j\pi n}$ which is symmetrical&lt;/p&gt;

&lt;p&gt;These give rise to&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;Square Normalization Constraint &lt;br /&gt;
$\sum_{n} h_{0}^2[n] =1$&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Scaling Function Constraint&lt;/strong&gt;
We impose a constraint that scaling function has unit area&lt;/p&gt;

&lt;p&gt;$\phi(t) = \sum_{k} h_{0}[k] \phi[2t-k]$
$\int \phi(t) dt = \int  \sum_{k} h_{0}[k] \phi[2t-k]$ dt $&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;Normalization Constraint  &lt;br /&gt;
$\sum_{n} h_{0}[n] = \sqrt{2}$&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Conditions for Perfect Reconstruction&lt;/strong&gt;
For perfect reconstruction filter banks we need to select $H_{0}(z)$ which will satisfy the condition&lt;/p&gt;

&lt;p&gt;With odd D &lt;br /&gt;
$K_{0}(z)+ K_{0}(-z)= C_{0}$&lt;/p&gt;

&lt;p&gt;with even D	 
$K_{0}(z)- K_{0}(-z) = C_{0}$&lt;/p&gt;

&lt;p&gt;From the above equation the summation  $K_{0}(z)+ K_{0}(-z)$ represents the nonzero sample value at even location and zero sample value at the odd location.&lt;/p&gt;

&lt;p&gt;Let $k_{0}[n]$ correspond to the sequence $K_{0}(z)$.The requirement is that we only want non zero value at zero location and zero value at all other locations&lt;/p&gt;

&lt;p&gt;This gives rise to&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;Even shift orthogonality &lt;br /&gt;
$\sum_{k=0}^{N-1} h_{0}[k]h_{0}[k-2l]  = 0 ,\forall l \ne 0 $&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let $k_{0}[n]$ correspond to the sequence $K_{0}(z)$.The requirement is that we only want non zero value at zero location and zero value at all other locations&lt;/p&gt;

&lt;p&gt;This gives rise to&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;Even shift orthogonality &lt;br /&gt;
$\sum_{k=0}^{N-1} h_{0}[k]h_{0}[k-2l]  = 0 ,\forall l \ne 0 $&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;The two additional constrains on the filter moments are due to vanishing moment constrain seen in the article &lt;a href=&quot;http://pi19404.github.io/pyVision/2014/11/29/14/&quot;&gt;Approximation of Piecewise Polynomial Using Wavelets&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;Zero order vanishing moment constraint &lt;br /&gt;
$\sum\_{k} (-1)^{k} h(k) =0$&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;pth order vanishing moment constraint &lt;br /&gt;
$\sum\_{k} (-1)^{k} k^{p}h(k) =0$&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thus the constraints imposed on filter coefficients like square normalization,normalization,vanishing moments and even shift orthogonality are consequence or requirement of perfect reconstruction filter bank&lt;/p&gt;

&lt;h3 id=&quot;daubechies-family-of--wavelets&quot;&gt;Daubechies Family of  Wavelets&lt;/h3&gt;

&lt;p&gt;Daubechies is a class of wavelets belonging to class of Conjugate Quadrature Filters .&lt;/p&gt;

&lt;p&gt;In Daubechies family of wavelets  the idea is to have more $(1 − z^{−1}) $ terms on the high pass branch.
Daubechies wavelet filters bank are realization of type of filter bank called Conjugate Quadrature Filter bank.&lt;/p&gt;

&lt;p&gt;As a example let us consider a filter of length 4 (Daubechies 2 Wavelets)
As mentioned we are seeking a filter that has 2 $(1-z^{-1})$ terms on it high pass branch&lt;/p&gt;

&lt;p&gt;The filter is of the form&lt;/p&gt;

&lt;p&gt;$H_{1}(z) = z^{-d}H_{0}(-z^{-1})$&lt;/p&gt;

&lt;p&gt;That means the low pass filter will have 2 $(1+z^{-1})$ terms on it high pass branch&lt;/p&gt;

&lt;p&gt;$H_{0}(z)=h_{0}+ h_{1}z^{-1} + h_{2}z^{-2}+h_{3}z^{-3}$&lt;/p&gt;

&lt;p&gt;$H_{0}(z)=(1+z^{-1})^2(1+B_{0}z^{-1})$&lt;/p&gt;

&lt;p&gt;$H_{0}(\omega) =( {1+e^{-j\omega}})^2 L(\omega)$&lt;/p&gt;

&lt;p&gt;$H_{0}(\pi) =0 $&lt;/p&gt;

&lt;p&gt;This constraint comes from above condition&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;Vanishing moment Constraint &lt;br /&gt;
$H_{0}(\pi) =0 $
$h_{0} - h_{1} + h_{2} - h_{3} =0$&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;$H_{0}(z)=(1+2z^{-1}+z^{-2})(1+B_{0}z^{-1})$&lt;/p&gt;

&lt;p&gt;$H_{0}(z)=1+(2+B_{0})z^{-1}+(1+2B_{0})z^{-2}+B_{0}z^{-3}$&lt;/p&gt;

&lt;p&gt;$h_{0}[n]=C_{0}[ 1 , (2+B_{0}) ,(1+2B_{0}) , B_{0} ]$&lt;/p&gt;

&lt;p&gt;The even values of autocorrelation function should be 0&lt;/p&gt;

&lt;p&gt;$R(2) = (1+2B_{0}) +(2+B_{0})B_{0}=0$&lt;/p&gt;

&lt;p&gt;$R(2) =  $B_{0}^{2}+ 4 B_{0} + 1=0$&lt;/p&gt;

&lt;p&gt;solving we get&lt;/p&gt;

&lt;p&gt;$B_{0} =  -2 + \sqrt{3} =-0.268$&lt;/p&gt;

&lt;p&gt;we also get&lt;/p&gt;

&lt;p&gt;$h_{0}[n] = C_{n}[1,1.732,0.464,-0.268]$&lt;/p&gt;

&lt;p&gt;$R(0) = 0.4829$&lt;/p&gt;

&lt;p&gt;Thus we have $C_{0}=\frac{1}{0.4829}$&lt;/p&gt;

&lt;p&gt;$h_{0}[n] = [0.4829,0.8364,0.2241,-0.129]$&lt;/p&gt;

&lt;p&gt;The filter coefficient is said to belong to family of Daubechies wavelts .And since the high has filter
has 2 zeros it is called as daub-2 wavelet.&lt;/p&gt;

&lt;p&gt;For example in the above case we have&lt;/p&gt;

&lt;p&gt;$h_{0}^{2}+ h_{1}^{2} + h_{2}^{2}+h_{3}^{2}=1$&lt;/p&gt;

&lt;p&gt;$h_{0}+ h_{1} + h_{2}+h_{3}=\sqrt{2}$&lt;/p&gt;

&lt;p&gt;$h_{0}h_{2}+ h_{1}h_{3}=0$&lt;/p&gt;

&lt;p&gt;$h_{0} - h_{1} + h_{2} - h_{3} =0$&lt;/p&gt;

&lt;p&gt;$0*h_{0} - h_{1} + 2 h_{2} - 3 h_{3} =0$&lt;/p&gt;

&lt;p&gt;The next member of Daubechies family is a length 6 filter of degree 5.And $H_{0}(z)$ can be written as&lt;/p&gt;

&lt;p&gt;$H_{0}(z)=(1+z^{-1})^3(1+B_{0}z^{-1})(1+B_{1}z^{-1})$&lt;/p&gt;

&lt;p&gt;we can solve for the coefficient using the same method as above&lt;/p&gt;

&lt;p&gt;This type of filter banks are called Conjugate Quadrature filter bank. The reason for this
nomenclature is that the low pass and the high pass filter frequency responses are $\pi$ apart from
each other and are governed by the constraint&lt;/p&gt;

&lt;p&gt;$K_{0}(z)+ K_{0}(-z)= C_{0}$&lt;/p&gt;

&lt;p&gt;Typically the Daubechies can be defined for any filter size.&lt;/p&gt;

&lt;p&gt;However we need a computationally way to derive these equations.The equations have a  nonlinear
constraint due to double shift orthogonality .The approach described above is not a scalable approach for higher filter sizes.&lt;/p&gt;

&lt;h3 id=&quot;computing-the-wavelet-coefficients&quot;&gt;Computing the Wavelet Coefficients&lt;/h3&gt;

&lt;p&gt;We know that if a filter has $p$ zeros at $\pi$ it has 2 vanishing moments and total filter length if $2p$ and transfer function will have $2p-1$ zeros&lt;/p&gt;

&lt;p&gt;The product filter $K_{0}(z)$ will have impulse response $4p-1$ coefficients and transfer function will have $4p-2$ zeros&lt;/p&gt;

&lt;p&gt;for example&lt;/p&gt;

&lt;p&gt;$h_{0}[n]=[-0.1294, 0.2241, 0.8365, 0.48296]$&lt;/p&gt;

&lt;p&gt;$H_{0}(z)=-0.1294,+0.2241z^{-1}+0.8365z^{-2}0.48296z^{-3}$&lt;/p&gt;

&lt;p&gt;$k_{0}[n]=[ 0.01674682, -0.0580127 , -0.16626588 , 0.25     ,   0.91626588 , 0.8080127,0.23325318]$&lt;/p&gt;

&lt;p&gt;The pole zero plot of $h_{0}[n],k_{0}[n]$ is shown below
we can see that $h_{0}[n]$ has 3 zeros and $k_{0}[n]$ as 6 zeros&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/15/w151.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/15/w152.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;since $h(n)$ is real ,$h_{0}[Z]$ is a polynomial with real coefficients&lt;/p&gt;

&lt;p&gt;Furthermore from complex conjugate theorem if  a polynomial&lt;/p&gt;

&lt;p&gt;$H(z) = a_{0} + a_{1}z + a_{2}z^2 + \cdots + a_{n} z^n$&lt;/p&gt;

&lt;p&gt;has real coefficients, then any complex zeros occur in conjugate pairs. That is, if $a + bi$ is a zero then so is $a – bi$ and vice-versa.This is a direct implication that magnitude spectrum of polynomial with real coefficients is even symmetric.&lt;/p&gt;

&lt;p&gt;This can be also seen in the below plot of arbitrary polynomial&lt;/p&gt;

&lt;p&gt;$h[n]=[1,2,2,-1,5,4]$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/15/w153.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If $H(z)$ has p zeros then $K(z)=H(z)&lt;em&gt;H^{&lt;/em&gt;}(z)$ will have 2p zeros&lt;/p&gt;

&lt;p&gt;if $K(z)$ is real and even symmetric $K(z)=K^{&lt;em&gt;}(z)=K(z^{&lt;/em&gt;})=K(z^{-1})$&lt;/p&gt;

&lt;p&gt;if $z$ is a zero then $z^*=z^{-1}$ is also a zero&lt;/p&gt;

&lt;p&gt;Thus $K_{0}(z)=H(z)*H^{* }(z)$ has zeros that come in quadrapulets of $z_{0},z^{*}_{0},\frac{1}{z_{0}},\frac{1}{z_{0}^{ * }}$ for complex zeros and duplets for real zero $z_{0},\frac{1}{z_{0}}$&lt;/p&gt;

&lt;p&gt;Again we take a arbitraty polynomial&lt;/p&gt;

&lt;p&gt;$h[n]=[1,1,2,2]$&lt;/p&gt;

&lt;p&gt;$k[n]=[ 2 , 4  ,7 ,10,  7  ,4  ,2]$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision//images/15/w154.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/15/w155.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let us assume that the frequency response  of filter will take the form&lt;/p&gt;

&lt;p&gt;$H_{0}(\omega) =( \frac{1+e^{-j\omega}}{2})^p L(\omega)$&lt;/p&gt;

&lt;p&gt;The filter will have $p$ zeros at $\pi$  and let us assume that $L(\omega)$ is a  polynomial which has a degree $p-1$ .&lt;/p&gt;

&lt;p&gt;$L(\omega)$ is not unique . For any quadrapulet of complex zeros in $K(z)$.One can choose a pair of zeros to retain for construction of zeros in $L(z)$.Similarly for every duplet of real zeros there are 2 ways of choosing the $L(z)$&lt;/p&gt;

&lt;p&gt;Thus there are total of $2^p$ different ways of choosing $L$.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;In the above example $&lt;/td&gt;
      &lt;td&gt;L(w)&lt;/td&gt;
      &lt;td&gt;^2$ contains zeros at $[j\sqrt{2},-j\sqrt{2},j\frac{1}{\sqrt2},-j\frac{1}{\sqrt2}]=[z1,z2,z3,z4]$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$&lt;/td&gt;
      &lt;td&gt;H_{0}(\omega)&lt;/td&gt;
      &lt;td&gt;^2 =cos^2(\frac{\omega}{2})^p&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;^2$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In case of the above example the zeros are located at&lt;/p&gt;

&lt;p&gt;$[j\sqrt{2},-j\sqrt{2},-1,-1,j\frac{1}{\sqrt2},-j\frac{1}{\sqrt2}]$&lt;/p&gt;

&lt;p&gt;The zeros at $(-1)$ are not part of $L(\omega)$&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$&lt;/td&gt;
      &lt;td&gt;L(w)&lt;/td&gt;
      &lt;td&gt;^2$ contains zeros at $[j\sqrt{2},-j\sqrt{2},j\frac{1}{\sqrt2},-j\frac{1}{\sqrt2}]$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This can be written as&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$&lt;/td&gt;
      &lt;td&gt;L(z)&lt;/td&gt;
      &lt;td&gt;^2 = (z-j\sqrt{2})(z+j\sqrt{2})(z-j\frac{1}{\sqrt2})(z+j\frac{1}{\sqrt2})$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$&lt;/td&gt;
      &lt;td&gt;L(z)&lt;/td&gt;
      &lt;td&gt;^2 = z^{4} + 2.5z^{2} +1$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It can also be seen that fourier transform of $K[z]$ can be written as polynomial of $cos(\omega)$&lt;/p&gt;

&lt;p&gt;$K(\omega) = 2 e^{-j3\omega}[e^{-j3\omega}+4e^{-j2\omega} + 7 e^{-j\omega} + 10 + 2 e^{j3\omega}+4e^{j2\omega}+ 7 e^{j\omega} ]$&lt;/p&gt;

&lt;p&gt;$K(\omega)= 2e^{-j3\omega}[4cos(2\omega ) + 7 cos(\omega ) + 10 + 2 cos(3\omega )]$&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;^2 = e^{j4\omega} + 2.5e^{j2\omega} + 1=e^{j2\omega}[2.5+2cos(2\omega)$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;^2 =e^{j2\omega}[0.5  + 4 cos^2(\omega)]=e^{j2\omega}[16cos^2(\omega/2)-16cos(\omega/2)-3.5]$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This can be always expressed in the form of $sin^2(\omega/2)$&lt;/p&gt;

&lt;p&gt;Again&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$&lt;/td&gt;
      &lt;td&gt;H(e^{j\omega})&lt;/td&gt;
      &lt;td&gt;^2 +&lt;/td&gt;
      &lt;td&gt;H(e^{-j(\omega+\pi)})&lt;/td&gt;
      &lt;td&gt;^2=1$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;if $y=cos^2(\omega/2)$ where $y \in [-1,1] $&lt;/p&gt;

&lt;p&gt;Let us assume that $L(\omega)$ can always expressed as trigonometric polynomial interms of 1-y and $L(\omega+\pi)$ be expressed as polynomial of y.&lt;/p&gt;

&lt;p&gt;The equation can be written as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;$y^{p}F(1-y) + (1-y)^{p}F(y) = 1 ,\forall  p $&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;This implies that any $H_{0}(\omega)$ satisfying the condition for perfect reconstruction corresponds to a polynomial $F(1-y)$ solving the above equation&lt;/p&gt;

&lt;p&gt;Conversly every polynomial which satisfies the above equation will satisfy perfect reconstruction criteria and define the analysis filter of perfect reconstruction filter bank&lt;/p&gt;

&lt;h3 id=&quot;a-possible-solution&quot;&gt;A possible solution&lt;/h3&gt;

&lt;p&gt;An example of such a polynomial is&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;$F(y) = \frac{1}{2(1-y)^p} $&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;From binomial theorem we have&lt;/p&gt;

&lt;p&gt;$\frac{1}{(1-x)^s} = \sum_{k=0}^\infty {s+k-1 \choose k} x^k \equiv \sum_{k=0}^\infty {s+k-1 \choose k} x^k = \sum_{k=0}^{p-1} {p+k-1 \choose k}  x^{k} + O(x^p)$&lt;/p&gt;

&lt;p&gt;This translates to&lt;/p&gt;

&lt;p&gt;$1 + y^{k}O((1-y)^k) + (1-y)^{k} O(y^k) = 1$&lt;/p&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;p&gt;$y^{k}O((1-y)^k) + (1-y)^{k} O(y^k)=0$&lt;/p&gt;

&lt;p&gt;Let us take a finite number of terms of the series&lt;/p&gt;

&lt;p&gt;$F_{p}(y) =0.5 \sum_{k=0}^p [{p+k \choose k} y^k]$&lt;/p&gt;

&lt;p&gt;$F_{p}(1-y) = 0.5 \sum_{k=0}^p [{p+k \choose k} (1-y)^k]$&lt;/p&gt;

&lt;p&gt;$S_{p} = 0.5 \sum_{k=0}^p {p+k \choose k}  [{p+k \choose k}  y^{p}(1-y)^k + y^{k}(1-y)^p ]	$&lt;/p&gt;

&lt;p&gt;Let $A_{n,j}$ represent the coefficient $jth$ term of the series&lt;/p&gt;

&lt;p&gt;$A_{n,j} = 0.5{n+j\choose j}  $&lt;/p&gt;

&lt;p&gt;$A_{n,0} = 1  $&lt;/p&gt;

&lt;p&gt;$(1-y)^{n+1}  = (1-y)^n - y(1-y)^n $
 $(y)^{n+1}  = (y)^n - (1-y)(y)^n $&lt;/p&gt;

&lt;p&gt;Now $S_{0} = 1$&lt;/p&gt;

&lt;p&gt;$S_{p-1} = \sum_{k=0}^{p-1} A_{p-1,k}A_{p-1,j}  [y^{p-1}(1-y)^k + y^{k}(1-y)^{p-1} 	]$&lt;/p&gt;

&lt;p&gt;$S_{p-1} = A_{p-1,0}[(1-y)^{p-1}  +y^{p-1}] + A_{p-1,1}[y(1-y)^{p-1}  +y^{p-1}(1-y)] + \ldots$&lt;/p&gt;

&lt;p&gt;$S_{p-1} = A_{p-1,0}[(1-y)^{p-1}  -y(1-y)^{p-1}+y^{p-1} -y^{p-1}(1-y)] + (A_{p-1,1}+A_{p-1,0})[y(1-y)^{p-1} (A_{p-1,1}+A_{p-1,0}][y(1-y)^{p-1}  +y^{p-1}(1-y)] + \ldots$&lt;/p&gt;

&lt;p&gt;$S_{p-1} =  A_{p-1,0}[(1-y)^{p}+(y)^{p}] + (\sum_{k=0}^{1}A_{p-1,k})[y(1-y)^{p-1}  +y^{p-1}(1-y)] + \ldots$&lt;/p&gt;

&lt;p&gt;$S_{p-1} =  \sum_{j=0}^{p-1} (\sum_{k=0}^{j+1}A_{p-1,k})[y^{j}(1-y)^{p-1}  +y^{p-1}(1-y)^{j}] +2\sum_{k=0}^{p}A_{p-1,k} (1-y)^{p} y^{p} $&lt;/p&gt;

&lt;p&gt;Again by properties of binomial coefficients&lt;/p&gt;

&lt;p&gt;$\sum_{k=0}^{j+1}A_{p-1,k} = A_{p,j}$&lt;/p&gt;

&lt;p&gt;$2 A_{p,p-1} = A_{p,p}$&lt;/p&gt;

&lt;p&gt;$S_{p-1} =  \sum_{j=0}^{p-1} A_{p,j}[y^{j}(1-y)^{p-1}  +y^{p-1}(1-y)^{j}] +2A_{p,p-1}(1-y)^{p-1} y^{p-1} $&lt;/p&gt;

&lt;p&gt;$S_{p-1} =  \sum_{j=0}^{p-1} A_{p,j}[y^{j}(1-y)^{p-1}  +y^{p-1}(1-y)^{j}] +2 A_{p,p-1} (1-y)^{p-1} [y^{p} + (1-y)y^{p-1}]$&lt;/p&gt;

&lt;p&gt;$S_{p-1} =  \sum_{j=0}^{p-1} A_{p,j}[y^{j}(1-y)^{p-1}  +y^{p-1}(1-y)^{j}] +2 A_{p,p-1} (1-y)^{p-1} y^{p} + (1-y)^{p}y^{p-1}]$&lt;/p&gt;

&lt;p&gt;$S_{p-1}=S_{p}$&lt;/p&gt;

&lt;p&gt;By induction if $S_{0}=1$ then $S_{p}=1 ,\forall p$&lt;/p&gt;

&lt;p&gt;Thus the polynomial&lt;/p&gt;
&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;$F_{p}(y) =0.5 \sum_{k=0}^p {p+k \choose k} y^k$  solves the equation&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;$y^{p}F(1-y) + (1-y)^{p}F(y) = 1 ,\forall  p $ and $F(y) &amp;gt; 0$&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;and $F_{p}(y) =0.5 \sum_{k=0}^p {p+k \choose k} y^k$  solves the equation&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;$y^{p}F(1-y) + (1-y)^{p}F(y) = 1 ,\forall  p $ and $F(y) &amp;gt; 0$&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;and thus satisfies the conditions for perfect reconstruction filter bank&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;^2=F(1-y)=F(\omega)$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;$F(\omega) =0.5 \sum_{k=0}^p c_{k} (sin(\omega/2) )^{2k} $&lt;/p&gt;

&lt;p&gt;$F(\omega) =0.5 \sum_{k=0}^p c_{k} sin^{2k}(\frac{\omega}{2}) $&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;By the properties of  trigonometric polynomials we have&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$ cos^{2n} (x) = (1/2)^{2n}  \sum_{k=0}^{2n} {2n \choose k} exp( i(2n-2j)x )$&lt;/p&gt;

&lt;p&gt;$ sin^{2n} (x) = (1/2)^{2n}  \sum_{k=0}^{2n} {2n \choose k} exp( i(2n-2j)(x - \pi/2) )$&lt;/p&gt;

&lt;p&gt;$sin^{2n}(\omega/2) = \frac{1}{2^{2n}}{2n \choose  n}-\frac{1}{2^{2n}} \sum_{k=0}^{n-1} (-1)^{n-k}{2n \choose k}e^{(n-k)j\omega}+e^{-(n-k)j\omega}$&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;

&lt;p&gt;Now after lengthy mathematical derivation ,we look at implementing this so that we can determine the filter coefficient for Daubechies file for any length.&lt;/p&gt;

&lt;p&gt;The first selection parameter in the design of Daubechies wavelets number of zeros at $\pi$&lt;/p&gt;

&lt;p&gt;If the filter has $p$ zeros at $\pi$ then total filter length is 2p and the Z transform of the filter has $2p-1$ zeros&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Zeros at $\pi$&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
zeros=2

 #populating the array containing zeros of transfer function
z=[]
for i in range(zeros):
    z.append(-1)

&lt;/pre&gt;

&lt;p&gt;We have to find the locations of $p-1$ zeros to complete determine the transfer function of the filter
These zeros will belong to polynomial defined by $L(\omega)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Coefficients of Trigonometric polynomial&lt;/strong&gt;
We have seen that $|L(\omega)|^2$ is expressed as trigonometric polynomial $F(1-y)$&lt;/p&gt;

&lt;p&gt;$F_{p}(1-y) = 0.5 \sum_{k=0}^{p-1} [{p+k \choose k} (1-y)^k]$&lt;/p&gt;

&lt;p&gt;The function to compute the coefficients of function is given below&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

def calculate_combinations(p, k):
    &quot;&quot;&quot; calculates the combination p+kCk &quot;&quot;&quot;
    r=0.5*combination(p+k,k);
    return r

def combination(n,r):
    &quot;&quot;&quot; calculates the combination nCr &quot;&quot;&quot;
    r=factorial(n) /( factorial(n-r) * factorial(r))
    return r

trigo_coeffs=np.vectorize(calculate_combinations)

 #coefficient of trigonometric polynomial
v=[]

for i in range(zeros):
    v.append(calculate_combinations(zeros-1,i))
    
&lt;/pre&gt;

&lt;p&gt;Thus the trigonometric polynomial&lt;/p&gt;

&lt;p&gt;$F(\omega) =0.5 \sum_{k=0}^p c_{k} sin^{2k}(\frac{\omega}{2}) $&lt;/p&gt;

&lt;p&gt;where $c_{k}={p+k \choose k}$
 and&lt;/p&gt;

&lt;p&gt;$sin^{2n}(\omega/2) = \frac{1}{2^{2n}}{2n \choose  n}-\frac{1}{2^{2n}} \sum_{k=0}^{n-1}(-1)^{n-k} {2n \choose k}e^{(n-k)j\omega}+e^{-(n-k)j\omega}$&lt;/p&gt;

&lt;p&gt;The function to compute the coefficients of $sin^{2n}(\omega/2)$ is as below&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

def sinp(p,pad=0):
    &quot;&quot;&quot; calculates the coefficients trigonometric polynomial sin^2p(x) &quot;&quot;&quot;
    
    result=np.zeros(2*p+1)
    g1=1.0/(2**(2*p))
    t1=combination(2*p,p)
    result[p]=g1*t1
    

    for k in range(p):
        v=g1*combination(2*p,k)
        if (p-k)%2!=0:
            v=-v       
        result[k]=v
        result[2*p-k]=v
    
    l=2*pad-2*p
    result=np.append(np.zeros(l/2),result)
    result=np.append(result,np.zeros(l/2))
    
    return result



&lt;/pre&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;**Compute the transfer function of $&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;^2$**&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The number of terms in the polynomial $F(1-y)$ in terms of y will be $p-1$  and in terms of $&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;^2$ will be  $2p-1$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&amp;lt;/pre class=”brush:python”&amp;gt;&lt;/p&gt;

&lt;p&gt;result=np.zeros(2&lt;em&gt;zeros-1)
for k in range(N):
    result=result+v[k]&lt;/em&gt;sinp(k,zeros-1)&lt;/p&gt;

&lt;p&gt;&amp;lt;/pre&amp;gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Thus we have now discrete time representation  of $&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;^2$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;**Obtain poles and zeros of $&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;^2$**&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;We can obtain the poles and zeros of $&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;^2$ from the discrete time representation&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

z1, p, k = tf2zpk(v,[1])

z2=[]
for i in z1:
    z2.append(i)

&lt;/pre&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;**Zeros of $&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;$**&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;we know that the filter has $p$ zeros at $\pi$ and $2p-1$ zeros belong to $&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;^2$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;out of the $2p-1$ zeros we need to select $p-1$ zeros of $&lt;/td&gt;
      &lt;td&gt;L(\omega)&lt;/td&gt;
      &lt;td&gt;$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This selection is not unique ,we can select any pair of $p-1$ zeros&lt;/p&gt;

&lt;p&gt;However for the filter to the stable we require that polse and zeros lie inside the unit circle of Z transform of the function.&lt;/p&gt;

&lt;p&gt;Since the filter is real we know that if $z_{o} $ is a real zero then so is $\frac{1}{z_{o}}$.Thus we need to only select one zero from such a pair.&lt;/p&gt;

&lt;p&gt;Since the filter is real we know that if $z_{o}$ is a complex zero then so are $z_{o}^{ * },\frac{1}{z_{o}},\frac{1}{z_{o}^{ * }}$.From the 4 zeros we need to select only pair $z_{o},z_{o}^{ * }$ or  $\frac{1}{z_{o}},\frac{1}{z_{o}^{ * }}$ correponding to zero that lie within the unit circle.&lt;/p&gt;

&lt;p&gt;Thus we select a subset of only those zeros that lie within the unit circle&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

def stable_zeros(z):
    &quot;&quot;&quot; get the stable complex zeros from magnitude squared transfer function &quot;&quot;&quot;
   
    fz=[]
    for l in z:
        if np.imag(l)!=0:
            l1=l*np.conj(l)
            l1=np.abs(l)            
            #compare magnitude of imageinary component with 1
            if l1&amp;amp;lt1 or abs(l1-1)&amp;amp;lt1e-3:
                fz.append(l)
      
      
    
    fz=np.array(fz)
    return fz

def unique_real_zeros(z):
    &quot;&quot;&quot; calculates the stable unique zeros from magnitude squared transfer function &quot;&quot;&quot;
    unique = scipy.unique(z)
    
    counts=np.zeros(len(unique))

    for i in range(len(unique)):
        for k in range(len(z)):
            if unique[i]==z[k]:
                counts[i]=counts[i]+1

    zeros=[]
    
    for e in range(len(unique)):
	    #compare the magnitude of real component with 1
        if (np.imag(unique[e])==0 or abs(np.imag(unique[e]))&amp;amp;lt1e-3) and (abs(np.real(unique[e])-1)&amp;amp;lt1e-3 or abs(np.real(unique[e]))&amp;amp;lt1):                    
            if counts[e]&amp;gt;1:
                multi=counts[e]/2
                print multi
                zeros.append(np.repeat(np.real(unique[e]),multi))
            if counts[e]==1 and (abs(np.real(unique[e])-1)&amp;amp;lt1e-3 or abs(np.real(unique[e]))&amp;amp;lt1):
                zeros.append(np.real(unique[e]))


    zeros=np.array(zeros,float)
    return zeros

&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Filter Transfer Function&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now that we have zeros at $\pi$ and zeros corresponding to $L(\omega)$,we can compute
the zeros of complete filter and then compute the discrete time representation of the filter coefficient&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

def trigonometric_filter_zeros(zeros):
    &quot;&quot;&quot; function return the zeros of trigonometric polynomial L(w)&quot;&quot;&quot;
    v=[]
    z=[]
    for i in range(zeros):
        v.append(calculate_combinations(zeros-1,i))
        
    v=np.array(v,float)
    
    result=np.zeros(2*zeros-1)
    for k in range(zeros):
        result=result+v[k]*sinp(k,zeros-1)
        
      
    den=np.zeros(len(result))
    den[len(den)-1]=1
   
    #convertize the transfer function to pole zero representation
    z1, p, k = tf2zpk(result,den)

    z=np.append(z,unique_real_zeros(z1))
    z=np.append(z,stable_zeros(z1))

        
    return z


def compute_coeffs(z,z1):
    &quot;&quot;&quot; function computes the filter coefficients of decomposition low pass filter
        given zeros at pi and zeros of trigonmonetric polynomial 
        for daubach filter &quot;&quot;&quot;
        
    #adding poles for causal system
    zeros=len(z)
    p=np.zeros(zeros-1)
    p=np.array(p)
    z=np.append(z,z1)
    
    h = zpk2tf(z,p,[1])
    h=np.array(h[0])

    #normalizing the filter coefficients
    h=h/math.sqrt(np.sum(h**2))
    return h,z,p


   ...........
   #populating zeros at pi
    z=[]
    for i in range(zeros):
        z.append(-1)


   #compute the zeros corresponding to trigonometric polynomial
    tz=trigonometric_filter_zeros(zeros)

   #compute the low pass filter decomposition filter
    h,z,p=compute_coeffs(z,tz)
        
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;CQF Filters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once we have computed the low pass filter coefficients we can compute the filter coefficients corresponding to all other filters of the CQF filter bank&lt;/p&gt;

&lt;p&gt;$h_{1}(n) = \sum_{k=0}^{N} h_{0}(N-k-d)(-1)^{k} $&lt;/p&gt;

&lt;p&gt;$g_{o}(n) = \sum_{k=0}^{N} h_{1}(k)(-1)^{k}$&lt;/p&gt;

&lt;p&gt;$g_{1}(n) = \sum_{k=0}^{n} h_{o}(n)(-1)^{k}&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

def cqf_filters(dec_lo):
    &quot;&quot;&quot; computs the decomposition and analysis low and high pass filters given decomposition low pass filter &quot;&quot;&quot;
    index=np.array(range(len(dec_lo)))
    dec_hi=np.exp(-1j*math.pi*index)*np.flipud(dec_lo)
    dec_hi=dec_hi.real
    
    
    recon_lo=np.exp(-1j*math.pi*index)*dec_hi.real
    recon_lo=recon_lo.real
    
    recon_hi=-np.exp(-1j*math.pi*index)*dec_lo.real
    recon_hi=recon_hi.real
    

    return dec_lo,dec_hi,recon_lo,recon_hi

&lt;/pre&gt;

&lt;h3 id=&quot;pole-zero-plots&quot;&gt;Pole Zero plots&lt;/h3&gt;

&lt;p&gt;Below are the pole zero plots for filter sizes of length 4&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/15/w156.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;###Code&lt;/p&gt;

&lt;p&gt;The code can be found at &lt;a href=&quot;https://github.com/pi19404/pyVision&quot;&gt;pyVision&lt;/a&gt; github repository&lt;/p&gt;

&lt;p&gt;Files&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;pyVision/pySignalProc/tutorial/wavelet5.py&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;https://gist.github.com/endolith/4625838&lt;/li&gt;
  &lt;li&gt;http://enc.tfode.com/Binomial-QMF&lt;/li&gt;
  &lt;li&gt;http://web.njit.edu/~akansu/s1.htm&lt;/li&gt;
  &lt;li&gt;http://www.bearcave.com/misl/misl_tech/wavelets/lifting/predict.html&lt;/li&gt;
  &lt;li&gt;https://blancosilva.wordpress.com/teaching/mathematical-imaging/wavelets-in-sage/&lt;/li&gt;
  &lt;li&gt;http://web.mit.edu/1.130/WebDocs/1.130/Software/Examples/example6.m&lt;/li&gt;
  &lt;li&gt;http://fourier.eng.hmc.edu/e161/lectures/filterbank/node3.html&lt;/li&gt;
  &lt;li&gt;http://enc.tfode.com/Binomial-QMF&lt;/li&gt;
  &lt;li&gt;http://web.njit.edu/~akansu/s1.htm&lt;/li&gt;
  &lt;li&gt;https://www.safaribooksonline.com/library/view/audio-signal-processing/9780471791478/ch006-sec004.html&lt;/li&gt;
  &lt;li&gt;http://www.globalspec.com/reference/9046/348308/chapter-9-2-quadrature-mirror-filters-and-conjugate-quadrature-filters&lt;/li&gt;
  &lt;li&gt;http://www.dsprelated.com/dspbooks/sasp/Conjugate_Quadrature_Filters_CQF.html&lt;/li&gt;
  &lt;li&gt;http://www.raywenderlich.com/12065/how-to-create-a-simple-android-game&lt;/li&gt;
  &lt;li&gt;https://gist.github.com/endolith/4625838&lt;/li&gt;
  &lt;li&gt;http://enc.tfode.com/Binomial-QMF&lt;/li&gt;
  &lt;li&gt;http://web.njit.edu/~akansu/s1.htm&lt;/li&gt;
  &lt;li&gt;http://www.bearcave.com/misl/misl_tech/wavelets/lifting/predict.html&lt;/li&gt;
  &lt;li&gt;https://blancosilva.wordpress.com/teaching/mathematical-imaging/wavelets-in-sage/&lt;/li&gt;
  &lt;li&gt;http://web.mit.edu/1.130/WebDocs/1.130/Software/Examples/example6.m&lt;/li&gt;
  &lt;li&gt;http://fourier.eng.hmc.edu/e161/lectures/filterbank/node3.html&lt;/li&gt;
  &lt;li&gt;http://enc.tfode.com/Binomial-QMF&lt;/li&gt;
  &lt;li&gt;http://web.njit.edu/~akansu/s1.htm&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>GStreamer Typefinding and Dynamic Pipelines - Part 2</title>
   <link href="http://localhost:4000/software/2014/12/17/21/"/>
   <updated>2014-12-17T00:00:00+00:00</updated>
   <id>http://localhost:4000/software/2014/12/17/21</id>
   <content type="html">&lt;p&gt;###Introduction&lt;/p&gt;

&lt;p&gt;In this tutorial we see how to build dynamic piplines and its application towards playing a video file.&lt;/p&gt;

&lt;p&gt;We will see how to identify the type of source stream being proceed and dynamically build a pipeline to handle the stream.&lt;/p&gt;

&lt;h3 id=&quot;reading-the-source-stream&quot;&gt;Reading the Source Stream&lt;/h3&gt;
&lt;p&gt;Let us consider a example that we want to play a WebM video file&lt;/p&gt;

&lt;p&gt;Typically for steps in displaying an video file would be&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reading the Source&lt;/li&gt;
  &lt;li&gt;DeMultiplex audio and video channels&lt;/li&gt;
  &lt;li&gt;Decode the video and audio&lt;/li&gt;
  &lt;li&gt;Convert video into a format that can be displayed&lt;/li&gt;
  &lt;li&gt;Display - Render the video frames on screen&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Typicall a Video stream will contains individual video,audio and sub-title streams. The job of demultipler is to provide access to these individual streams so that they can be processed accordingly.&lt;/p&gt;

&lt;p&gt;In the decoding step encoded  video and audio streams are passed to video and audio decoder.&lt;/p&gt;

&lt;p&gt;Next the decoded raw video streams are converted to a format suitable for display and finally video and audio streams are passed to respective video and audio sinks .&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http:/pi19404.github.io/pyVision/images/others/gst2.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this article we will only look at flow of video streams.&lt;/p&gt;

&lt;p&gt;We define a structure to hold all the information&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

/* Structure to contain all our information, so we can pass it to callbacks */
typedef struct _CustomData {
  GstElement *pipeline;
  GstElement *source;
  GstElement *typefinder;
  GstElement *demux;
  GstElement *decoder;
  GstElement *convert;
  GstElement *sink;
} CustomData;


&lt;/pre&gt;

&lt;h4 id=&quot;source-type&quot;&gt;&lt;strong&gt;Source Type&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;In the present application we are going to create GStreamer source element from URI passed by the user&lt;/p&gt;

&lt;pre class=&quot;brush:cpp &quot;&gt;

data.source = gst_element_make_from_uri (GST_URI_SRC,&quot;file:/home/pi19404/Downloads/sample.webm&quot; ,NULL);

g_print(&quot;GStreamer Source Type %s \n&quot;,GST_ELEMENT_NAME (data.source));
&lt;/pre&gt;

&lt;p&gt;Since the passed URI is a file location data.source will of GStreamer Element of type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;filesrc&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&quot;gstreamer-pads&quot;&gt;&lt;strong&gt;Gstreamer Pads&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The primary complexity in gstreamer applications is that different types of input streams need to be handled differently.&lt;/p&gt;

&lt;p&gt;Even demultiplexing techniques of streams need to be handled differently.To this we need a mechanism to analyse the type of source dynamically.&lt;/p&gt;

&lt;p&gt;Gstreams provides mechanism called as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pads&lt;/code&gt; which are the element’s interface to the outside world.
Data streams from one element’s source pad to another element’s sink pad.&lt;/p&gt;

&lt;p&gt;A pad type is defined by two properties: its direction and its availability.GStreamer defines two pad directions: source pads and sink pads.The elements receive data on their sink pads and generate data on their source pads.&lt;/p&gt;

&lt;p&gt;A pad can have any of three availabilities: always, sometimes and on request.In case of some Gstreamer Elements the pads are are created during element creation/initialization and are always available.&lt;/p&gt;

&lt;p&gt;Pads are created from Pad Templates specified for Gstreamer Elements.&lt;/p&gt;

&lt;p&gt;we can see the pads associated with element using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gst-inspect&lt;/code&gt; tool.
Let us look at the pads available in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uridecodebin&lt;/code&gt; element&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

Pad Templates:
  SRC template: 'src%d'
    Availability: Sometimes
    Capabilities:
      ANY

&lt;/pre&gt;

&lt;p&gt;We can see that element has a source pad and is only available &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sometimes&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Typically such pads are created during runtime.The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uridecodebin&lt;/code&gt; element will analyze the input stream before creating the source pads that can link to other elements.&lt;/p&gt;

&lt;p&gt;In the present application the source is off type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;filesrc&lt;/code&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

Pad Templates:
  SRC template: 'src'
    Availability: Always
    Capabilities:
      ANY

&lt;/pre&gt;

&lt;p&gt;We can see that the pad is always present can be immediately form a like to next element of the pipeline&lt;/p&gt;

&lt;h4 id=&quot;gstreamer-typefind-element&quot;&gt;&lt;strong&gt;Gstreamer TypeFind Element&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The filesrc element, for example, does not know what type of file it is reading. Before you can attach an element to the pad of the filesrc, you need to determine the media type in order to be able to choose a compatible element.&lt;/p&gt;

&lt;p&gt;To solve this problem, a plugin can provide the GStreamer core library with a typedefinition library with a typedefinition.&lt;/p&gt;

&lt;p&gt;GStreamer uses the typefinding library to determine the video stream type.GStreamer  will read data for as long as the type of a stream is unknown.During this period, it will provide data to all typefind functions for all plugins .&lt;/p&gt;

&lt;p&gt;This typefind function will inspect a GstBuffer with data and will output a GstCaps structure describing the type. If the typefind function does not understand the buffer contents, it will return NULL.&lt;/p&gt;

&lt;p&gt;GStreamer has a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;typefind element&lt;/code&gt; in its core elements that can be used to determine the type of a given pad.&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

  /* create the typefind element */
  data.typefind = gst_element_factory_make (&quot;typefind&quot;, &quot;typefind&quot;);
  g_assert (data.typefind != NULL);

/* Adding Elements to List */
gst_bin_add_many (GST_BIN (data.pipeline),data.source,data.typefinder,NULL);

/*link source and typefind elements */
if (!gst_element_link (data.source, data.typefinder)) {
    g_printerr (&quot;Elements could not be linked.\n&quot;);
    gst_object_unref (data.pipeline);
    return -1;
  }

&lt;/pre&gt;

&lt;h4 id=&quot;gstreamer-signals&quot;&gt;&lt;strong&gt;GStreamer Signals&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Gstreamer provides signalling which is a general purpose notification mechanism  that can be used as tool for customization object behavior.&lt;/p&gt;

&lt;p&gt;Signals allow the application to notified by means of a callback when something interesting has happened. Signals are identified by a name, and each GObject has its own signals.&lt;/p&gt;

&lt;p&gt;For example when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;typefind function&lt;/code&gt; associated with a plugin recognizes a video stream it uses signaling mechanism to notify the application . We can capture the event and run used defined code upon notification of occurrence of the event.&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

/* Connect to the typefind signal */
g_signal_connect (data.typefinder, &quot;have-type&quot;, G_CALLBACK (cb_typefound), &amp;amp;data);


static void
cb_typefound (GstElement *typefind,
	      guint       probability,
	      GstCaps    *caps,
	      CustomData    data)
{
  gchar *type;

  type = gst_caps_to_string (caps);
  g_print (&quot;Media type %s found, probability %d%%\n&quot;, type, probability);
  g_free (type);


}


&lt;/pre&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g_signal_connect&lt;/code&gt; function attaches a signal handler&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(  g_signal_connect  )&lt;/code&gt; to an GStreamer element &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(data.typefinder = typefind )&lt;/code&gt;  to inform the application via callback function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(cb_typefound)&lt;/code&gt; when the type of input stream been determined .&lt;/p&gt;

&lt;p&gt;Once we determine the media type we know the elements that are required to be used in the pipeline
In the example we are going to use Web Stream&lt;/p&gt;

&lt;p&gt;The Demuxer element type is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matroskademux&lt;/code&gt; which  Demuxes Matroska/WebM streams into video/audio/subtitles.&lt;/p&gt;

&lt;p&gt;Decoder element is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vp8dec&lt;/code&gt; that is capable for decoding the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;webm&lt;/code&gt; format video into raw image format&lt;/p&gt;

&lt;p&gt;we can also establish the link of typefinder with demuxer element and the decoder with converter and sink.&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;


  if (strcmp(type,&quot;video/webm&quot;)==0)
  {
	  data-&amp;gt;demux=gst_element_factory_make (&quot;matroskademux&quot;, &quot;demux&quot;);
	  data-&amp;gt;decoder=gst_element_factory_make (&quot;vp8dec&quot;, &quot;decoder&quot;);

	  if (gst_element_link (data-&amp;gt;typefinder, data-&amp;gt;demux) !=TRUE || gst_element_link_many (data-&amp;gt;decoder, data-&amp;gt;filter,sink,NULL) !=TRUE) {
	  g_printerr (&quot;Elements could not be linked.\n&quot;);
	  gst_object_unref (data-&amp;gt;pipeline);
	  return ;
	  }
   
  }
  
&lt;/pre&gt;

&lt;h4 id=&quot;gstreamer-demultiplexer&quot;&gt;&lt;strong&gt;Gstreamer Demultiplexer&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Demuxers do not have any pads till they receive the buffers to parse. As data is available to parse, pads are dynamically added based on the streams available.&lt;/p&gt;

&lt;p&gt;A demuxer contains one sink pad, through which the muxed data arrives, and multiple source pads, one for each stream found in the container:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http:/pi19404.github.io/pyVision/images/others/gst3.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The primary complexity when dealing with demuxers is that they cannot produce any information until they have received some data and have analyzed the type of stream.&lt;/p&gt;

&lt;p&gt;Initially when the pipeline is set to run the demuxer does not have any source pads to which other units can link hence the pipeline terminates at the demuxer,&lt;/p&gt;

&lt;p&gt;However when the demuxer has received sufficient data to know the type of streams it will create the source pads.This is a time when pipeline can be completed.&lt;/p&gt;

&lt;p&gt;The gstreamer provides us a facility to set up a  callback function at this point.
The pipeline from the demux to decoder can be configured and complete pipeline can be established.&lt;/p&gt;

&lt;p&gt;Again gstreamer provides us a singalling mechanism to reveive notification when demuxer has parsed the data and source pads are created.The signal type to be captured is “pad-added”&lt;/p&gt;

&lt;p&gt;Thus the idea is to build the pipeline from the source down to the demuxer, and set it to run .
When the demuxer has received enough information to know about the number and kind of streams in the container, it will start creating source pads.This is the right time for us to finish building the pipeline and attach it to the newly added demuxer pads.&lt;/p&gt;

&lt;p&gt;Since only link till the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;typefinder&lt;/code&gt; as created earlier and no element is connected to the source pads of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;typefind&lt;/code&gt; elements the GStreamer pipeline sees that pipeline has terminated and enters the  PAUSED state.&lt;/p&gt;

&lt;p&gt;Thus after linking the GStreamer Demux elements the pipeline proceedes futher by changing its state from PAUSED to PLAYING&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

  /* Connect to the pad-added signal */
  g_signal_connect (data.demux, &quot;pad-added&quot;, G_CALLBACK (pad_added_handler), data);

static void
pad_added_handler (GstElement *element,
              GstPad     *pad,
              CustomData    *data)
{
  GstPad *sinkpad;

  /* We can now link this pad with the vorbis-decoder sink pad */
  g_print (&quot;Dynamic pad created, linking demuxer/decoder\n&quot;);

 /*Get the sink pad of the decoder element */
  sinkpad = gst_element_get_static_pad (data-&amp;gt;decoder, &quot;sink&quot;);

/*link the newly created source pads with decoder sink pads */
  gst_pad_link (pad,sinkpad);

  gst_object_unref (sinkpad);
}

&lt;/pre&gt;

&lt;h3 id=&quot;debug-output&quot;&gt;Debug Output&lt;/h3&gt;

&lt;p&gt;The Gstreamer provides mechanism to observe debug output messages.
The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GST_DEBUG&lt;/code&gt; environment variable sets the logging level with 0 being lowest to 5 being the highest.
We can export the environment variable to get the detailed output&lt;/p&gt;
&lt;pre class=&quot;bruch:python&quot;&gt;
 
 # export GST_DEBUG=3
 # ./example3

0:00:00.000276094  3334      0x108a600 INFO                GST_INIT gst.c:613:init_pre: Initializing GStreamer Core Library version 0.10.36
0:00:00.000474378  3334      0x108a600 INFO                GST_INIT gst.c:614:init_pre: Using library installed in /usr/lib/x86_64-linux-gnu
0:00:00.000548656  3334      0x108a600 INFO                GST_INIT gst.c:624:init_pre: Linux prasad-TA790GX-A3 3.11.0-15-generic #25~precise1-Ubuntu SMP Thu Jan 30 17:39:31 UTC 2014 x86_64
0:00:00.000835787  3334      0x108a600 INFO                GST_INIT gstquery.c:107:_gst_query_initialize: init queries
0:00:00.002662900  3334      0x108a600 INFO                GST_INIT gstmessage.c:73:_gst_message_initialize: init messages
0:00:00.003684321  3334      0x108a600 INFO      GST_PLUGIN_LOADING gstplugin.c:350:_gst_plugin_initialize: registering 0 static plugins
0:00:00.004054157  3334      0x108a600 INFO      GST_PLUGIN_LOADING gstplugin.c:255:gst_plugin_register_static: registered static plugin &quot;staticelements&quot;
0:00:00.004125636  3334      0x108a600 INFO      GST_PLUGIN_LOADING gstplugin.c:257:gst_plugin_register_static: added static plugin &quot;staticelements&quot;, result: 1
0:00:00.005255415  3334      0x108a600 INFO            GST_REGISTRY gstregistry.c:1672:ensure_current_registry: reading registry cache: /home/pi19404/.gstreamer-0.10/registry.x86_64.bin
0:00:00.020256731  3334      0x108a600 INFO            GST_REGISTRY gstregistrybinary.c:614:gst_registry_binary_read_cache: loaded /home/pi19404/.gstreamer-0.10/registry.x86_64.bin in 0.014912 seconds
0:00:00.020330088  3334      0x108a600 INFO            GST_REGISTRY gstregistry.c:1522:scan_and_update_registry: Validating plugins from registry cache: /home/pi19404/.gstreamer-0.10/registry.x86_64.bin
0:00:00.021159862  3334      0x108a600 INFO            GST_REGISTRY gstregistry.c:1634:scan_and_update_registry: Registry cache has not changed
0:00:00.021181003  3334      0x108a600 INFO            GST_REGISTRY gstregistry.c:1707:ensure_current_registry: registry reading and updating done, result = 1
0:00:00.021192969  3334      0x108a600 INFO                GST_INIT gst.c:805:init_post: GLib runtime version: 2.32.4
0:00:00.021205269  3334      0x108a600 INFO                GST_INIT gst.c:807:init_post: GLib headers version: 2.32.0
0:00:00.021221264  3334      0x108a600 INFO                GST_INIT gst.c:456:gst_init_check: initialized GStreamer successfully
0:00:00.021790040  3334      0x108a600 INFO      GST_PLUGIN_LOADING gstplugin.c:859:gst_plugin_load_file: plugin &quot;/usr/lib/x86_64-linux-gnu/gstreamer-0.10/libgstcoreelements.so&quot; loaded
0:00:00.021820224  3334      0x108a600 INFO     GST_ELEMENT_FACTORY gstelementfactory.c:376:gst_element_factory_create: creating element &quot;filesrc&quot;
0:00:00.021957404  3334      0x108a600 INFO        GST_ELEMENT_PADS gstelement.c:728:gst_element_add_pad:&amp;lt;GstBaseSrc@0x1254040&amp;gt; adding pad 'src'
0:00:00.021992128  3334      0x108a600 INFO                 filesrc gstfilesrc.c:374:gst_file_src_set_location: filename : /home/pi19404/Downloads/sample.webm
0:00:00.022004967  3334      0x108a600 INFO                 filesrc gstfilesrc.c:375:gst_file_src_set_location: uri      : file:///home/pi19404/Downloads/sample.webm
0:00:00.022058217  3334      0x108a600 INFO     GST_ELEMENT_FACTORY gstelementfactory.c:374:gst_element_factory_create: creating element &quot;typefind&quot; named &quot;typefind&quot;
0:00:00.022136507  3334      0x108a600 INFO        GST_ELEMENT_PADS gstelement.c:728:gst_element_add_pad:&amp;lt;GstTypeFindElement@0x11c72b0&amp;gt; adding pad 'sink'
0:00:00.022174432  3334      0x108a600 INFO        GST_ELEMENT_PADS gstelement.c:728:gst_element_add_pad:&amp;lt;GstTypeFindElement@0x11c72b0&amp;gt; adding pad 'src'
GStreamer Source Type filesrc0 
0:00:00.022326110  3334      0x108a600 INFO      GST_PLUGIN_LOADING gstplugin.c:859:gst_plugin_load_file: plugin &quot;/usr/lib/x86_64-linux-gnu/gstreamer-0.10/libgstautodetect.so&quot; loaded
0:00:00.022343305  3334      0x108a600 INFO     GST_ELEMENT_FACTORY gstelementfactory.c:374:gst_element_factory_create: creating element &quot;autovideosink&quot; named &quot;sink&quot;
0:00:00.022450911  3334      0x108a600 INFO        GST_ELEMENT_PADS gstelement.c:728:gst_element_add_pad:&amp;lt;GstAutoVideoSink@0x1257010&amp;gt; adding pad 'sink'
0:00:00.022466793  3334      0x108a600 INFO     GST_ELEMENT_FACTORY gstelementfactory.c:374:gst_element_factory_create: creating element &quot;fakesink&quot; named &quot;tempsink&quot;

&lt;/pre&gt;

&lt;p&gt;###Code&lt;/p&gt;

&lt;p&gt;The file containing the code is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;example3.c&lt;/code&gt;.The compilation command for the same is as below&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

gcc example3.c -o example3 `pkg-config --cflags --libs gstreamer-0.10` -g

./example3
&lt;/pre&gt;

&lt;p&gt;The code mentioned in the article can be found in pi19404 &lt;a href=&quot;https://github.com/pi19404/tutorials&quot;&gt;tutorials&lt;/a&gt; github repository at path&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;gst/example3.c&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The webM video files can be found at&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;http://docs.gstreamer.com/media/sintel_trailer-480p.webm&lt;/li&gt;
  &lt;li&gt;http://www.http://pi19404.github.io/pyVision/media/others/sample.webm&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Basics of GStreamer - Part 1</title>
   <link href="http://localhost:4000/software/2014/12/14/20/"/>
   <updated>2014-12-14T00:00:00+00:00</updated>
   <id>http://localhost:4000/software/2014/12/14/20</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;GStreamer is a framework designed to handle streaming multimedia flows. Media travels from the “source” elements (the producers), down to the “sink” elements (the consumers), passing through a series of intermediate elements performing all kinds of tasks. The set of all the interconnected elements is called a “pipeline”.&lt;/p&gt;

&lt;p&gt;The basic construction block of GStreamer are the elements, which process the data as it flows downstream from the source elements (the producers of data) to the sink elements (the consumers of data), passing through filter elements.&lt;/p&gt;

&lt;p&gt;In this article we will look at two methods to implement the GStreamer Pipeline.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;gst_parse_launch&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This function takes a textual representation of a pipeline and turns it into an actual pipeline.This method can be used in case if you are satisfied using basic Gstreamer features .&lt;/p&gt;

&lt;p&gt;The ‘gst-launch’ GStreamer utility uses this method to execute GStreamer pipeline.The textual representation of pipelines are passed through command like arguments to the utility.&lt;/p&gt;

&lt;p&gt;Lets look at example to play a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://docs.gstreamer.com/media/sintel_trailer-480p.webm&lt;/code&gt; video file using GStreamer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;playbin2 Gstreamer Element&lt;/strong&gt;
We will be using Gstreamer element &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;playbin2&lt;/code&gt; in the present example.&lt;/p&gt;

&lt;p&gt;playbin2 is a special element which acts as a source and as a sink, and is capable of implementing a whole pipeline. Internally, it creates and connects all the necessary elements to play your media, so you do not have to worry about it.&lt;/p&gt;

&lt;p&gt;If figures out how to play the video for you and renders the video . It is one of easiest ways to implement media flow since there is not input from user at all apart from the video file name.However It does not allow the control granularity that a manual pipeline does, but, still, it permits enough customization to suffice for a wide range of applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Initialization&lt;/strong&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

// Initialize GStreamer 
gst_init (&amp;amp;argc, &amp;amp;argv);

&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;This is the first command in any GStreamer application.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The function initializes the GStreamer library  by initializing all internal structures , setting up internal path lists, registering built-in elements, and loading standard plugins.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The user can pass inputs to GStreamer initialization function via command line options (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;argv&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;argc&lt;/code&gt;)which can be processed by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gst-init&lt;/code&gt; function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Building Pipeline&lt;/strong&gt;
In this example, we are only passing one parameter to playbin2, which is the URI of the media we want to play&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

/* Build the pipeline */
  pipeline = gst_parse_launch (&quot;playbin2 uri=http://docs.gstreamer.com/media/sintel_trailer-480p.webm&quot;, NULL);

or

  pipeline = gst_parse_launch (&quot;playbin2 uri=file:/home/pi19404/Downloads/sample.webm&quot;, NULL);

&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Start Playing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every GStreamer element has an associated state which indicates the mode in which GStreamer element is in. One such state is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GST_STATE_PLAYING&lt;/code&gt;.Video playback will only start if the state of the pipeline is set to the PLAYING state.&lt;/p&gt;
&lt;pre class=&quot;brush:cpp&quot;&gt;

/* Start playing */
gst_element_set_state (pipeline, GST_STATE_PLAYING);

&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Feedback&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you mistype the URI, or the file does not exist, or you are missing a plug-in, GStreamer provides several notification mechanisms.In this example we will be exiting on error.&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

/* Wait until error or EOS */
bus = gst_element_get_bus (pipeline);
gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE, GST_MESSAGE_ERROR | GST_MESSAGE_EOS);

&lt;/pre&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gst_element_get_bus()&lt;/code&gt; retrieves the pipeline’s bus, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gst_bus_timed_pop_filtered()&lt;/code&gt; is a blocking call  till either ERROR or an EOS (End-Of-Stream) occurs on the  bus&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gst_bus_pop_filtered&lt;/code&gt; returns a element of type GSTMessage.The GSTMessage can be checked to see if the pipeline has exited because of an error or EOS.&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

/* Parse message */
if (msg != NULL) {
  GError *err;
  gchar *debug_info;
   
  switch (GST_MESSAGE_TYPE (msg)) {
    case GST_MESSAGE_ERROR:
      gst_message_parse_error (msg, &amp;amp;err, &amp;amp;debug_info);
      g_printerr (&quot;Error received from element %s: %s\n&quot;, GST_OBJECT_NAME (msg-&amp;gt;src), err-&amp;gt;message);
      g_printerr (&quot;Debugging information: %s\n&quot;, debug_info ? debug_info : &quot;none&quot;);
      g_clear_error (&amp;amp;err);
      g_free (debug_info);
      break;
    case GST_MESSAGE_EOS:
      g_print (&quot;End-Of-Stream reached.\n&quot;);
      break;
    default:
      /* We should not reach here because we only asked for ERRORs and EOS */
      g_printerr (&quot;Unexpected message received.\n&quot;);
      break;
  }
  gst_message_unref (msg);
}

&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Cleanup&lt;/strong&gt;
We free GStreamer resources gracefully bu setting the pipeline to NULL State before releasing GStreamer objects/references.&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

/* Free resources */
if (msg != NULL)
  gst_message_unref (msg);
gst_object_unref (bus);
gst_element_set_state (pipeline, GST_STATE_NULL);
gst_object_unref (pipeline);


&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Compilation and Execution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The file containing the code is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;example1.c&lt;/code&gt;.We will use gstreamer-0.10&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
 #compile the commands
gcc example1.c -o example1 `pkg-config --cflags --libs gstreamer-0.10

 #run the program
 ./example1
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Verifying the pipeline&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The gstreamer also comes with a utility called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gst-launch&lt;/code&gt;.This utility can be used to verify the textual representation of pipeline by passing it as command line parameter to the utility.This will execute the same function as the above program&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

gst-launch-0.10 playbin2 uri=file:/home/pi19404/Downloads/sample.webm

&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Manual Method&lt;/strong&gt;
In GStreamer you usually build the pipeline by manually assembling the individual elements.This is a more complicated method but provides developer with tools for more advanced features and customizations.&lt;/p&gt;

&lt;p&gt;The basic construction block of GStreamer are the elements, which process the data as it flows downstream from the source elements (the producers of data) to the sink elements (the consumers of data), passing through filter elements.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/others/gst1.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this method instead of specifying a textual representation we create pipeline using GStreamer API function calls.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Element creation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The new GStreamer elements can be created with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gst_element_factory_make()&lt;/code&gt;
The first parameter is the type of element to create  and the second parameter is the name we want to give to this particular instance .&lt;/p&gt;

&lt;p&gt;The various GStreamer element type can be found in the below link 
http://docs.gstreamer.com/display/GstSDK/Basic+tutorial+14%3A+Handy+elements&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

/* Create the elements */

playbin = gst_element_factory_make(&quot;playbin2&quot;,&quot;playbin&quot;);


&lt;/pre&gt;

&lt;p&gt;We need to pass paramters to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;playbin2&lt;/code&gt; gstreamer element&lt;/p&gt;

&lt;p&gt;This is done using the function call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g_object_set&lt;/code&gt; which can be set to set arguments for gstreamer elements&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

//Create Elemet
playbin   = gst_element_factory_make (&quot;playbin2&quot; ,&quot;playbin&quot;);

//Set Parameters
g_object_set (playbin, &quot;uri&quot;, &quot;file:/home/pi19404/Downloads/sample.webm&quot;, NULL);

&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Start Playing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As mentioned in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;playbin2&lt;/code&gt; element is of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GstBin&lt;/code&gt; type .This element contains a set of gstreamer elements which realizes a pipeline.Thus to start playing the video we set the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;playbin2&lt;/code&gt; element to playing state.&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

/* Start playing */
  ret = gst_element_set_state (playbin, GST_STATE_PLAYING);
  if (ret == GST_STATE_CHANGE_FAILURE) {
    g_printerr (&quot;Unable to set the pipeline to the playing state.\n&quot;);
    gst_object_unref (playbin);
    return -1;
  }
   
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Feedback&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We wait till end of stream or error.We parse the message to check for errors&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

/* Wait until EOS of error */
  bus = gst_element_get_bus (playbin);
  msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE, GST_MESSAGE_ERROR | GST_MESSAGE_EOS);

/* Parse message */
if (msg != NULL) {
  GError *err;
  gchar *debug_info;
   
  switch (GST_MESSAGE_TYPE (msg)) {
    case GST_MESSAGE_ERROR:
      gst_message_parse_error (msg, &amp;amp;err, &amp;amp;debug_info);
      g_printerr (&quot;Error received from element %s: %s\n&quot;, GST_OBJECT_NAME (msg-&amp;gt;src), err-&amp;gt;message);
      g_printerr (&quot;Debugging information: %s\n&quot;, debug_info ? debug_info : &quot;none&quot;);
      g_clear_error (&amp;amp;err);
      g_free (debug_info);
      break;
    case GST_MESSAGE_EOS:
      g_print (&quot;End-Of-Stream reached.\n&quot;);
      break;
    default:
      /* We should not reach here because we only asked for ERRORs and EOS */
      g_printerr (&quot;Unexpected message received.\n&quot;);
      break;
  }
  gst_message_unref (msg);
}

&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Cleanup&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;And finally the cleanup code to free the resources&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

 /* Free resources */
  if (msg != NULL)
    gst_message_unref (msg);
  gst_object_unref (bus);
  gst_element_set_state (playbin, GST_STATE_NULL);
  gst_object_unref (playbin);

&lt;/pre&gt;

&lt;p&gt;The file containing the code us &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;example2.c&lt;/code&gt;.The compilation command for the same is as below&lt;/p&gt;

&lt;pre class=&quot;brush:cpp&quot;&gt;

gcc example2.c -o example2 `pkg-config --cflags --libs gstreamer-0.10` -g

./example2
&lt;/pre&gt;

&lt;p&gt;#Code&lt;/p&gt;

&lt;p&gt;The code mentioned in the article can be found in my &lt;a href=&quot;https://github.com/pi19404/tutorials&quot;&gt;tutorials&lt;/a&gt; github repository at path&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;gst/example1.c&lt;/li&gt;
  &lt;li&gt;gst/example2.c&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The webM video files can be found at&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;http://docs.gstreamer.com/media/sintel_trailer-480p.webm&lt;/li&gt;
  &lt;li&gt;http://www.http://pi19404.github.io/pyVision/media/others/sample.webm&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Wavelet Two Channel FilterBank</title>
   <link href="http://localhost:4000/signal%20processing/2014/12/08/19/"/>
   <updated>2014-12-08T00:00:00+00:00</updated>
   <id>http://localhost:4000/signal%20processing/2014/12/08/19</id>
   <content type="html">&lt;h3 id=&quot;wavelet-two-channel-filterbank&quot;&gt;Wavelet Two Channel Filterbank&lt;/h3&gt;

&lt;p&gt;We had seen the concept of Haar two channel filter bank in the article &lt;a href=&quot;http://pi19404.github.io/pyVision/2014/11/26/13/&quot;&gt;“Haar Wavelet Filter Bank”&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We saw that the analysis filter bank are characterized by decomposition filter while synthesis filter banks are characterized by reconstruction filters.&lt;/p&gt;

&lt;p&gt;Instead of coefficients defined by haar wavelet and scaling function.Let us assume that the decomposition filters are $h_{o}[n],h_{1}[n]$ and reconstuction filters are $g_{o}[n],g_{1}[n]$.&lt;/p&gt;

&lt;p&gt;Let us look at following generic two channel filter bank&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w7.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A Perfect Reconstruction filter bank is any filter bank whose reconstruction is the original signal, possibly delayed and scaled version.&lt;/p&gt;

&lt;p&gt;Crticially sampled filter banks are ones in which downsampling factor is the same as the number of filter channels.We have filter bank with 2 channels (high pass and low pass decomposition and synthesis) and downsampling factor of 2&lt;/p&gt;

&lt;p&gt;We will compute the output at the output of each stage&lt;/p&gt;

&lt;h3 id=&quot;analysis-filters&quot;&gt;Analysis Filters&lt;/h3&gt;
&lt;p&gt;$Y_{1}[z] = H_{o}[z]X[z]$&lt;/p&gt;

&lt;p&gt;$Y_{2}[z] = H_{1}[z]X[z]$&lt;/p&gt;

&lt;h3 id=&quot;downsampling-block&quot;&gt;Downsampling Block&lt;/h3&gt;

&lt;p&gt;$X_{down}(z)=\sum_{n=-\infty}^\infty x[Mn] z^{-n} $&lt;/p&gt;

&lt;p&gt;Let $m=Mn$, i.e., $n=M/2$, we get 
$\displaystyle X_{down}(z)$	$\textstyle =$	$\displaystyle \sum_{m=\cdots,-2,0,,2,\cdots} x[m] (z^{1/2})^{m}=\sum_{n} x_{M}[n] z^{-n/M}$&lt;/p&gt;

&lt;p&gt;Let $\psi_{M}[n] $ represent a modulation function&lt;/p&gt;

&lt;p&gt;$\psi_{M}[n] = \begin{cases}
      1, &amp;amp; \text{if}\ mod(n,M) =1 \&lt;br /&gt;
      0, &amp;amp; \text{otherwise}
    \end{cases} $&lt;/p&gt;

&lt;p&gt;We can see that $\psi_{M}[n]$ is a periodic impulse function with period of $M$&lt;/p&gt;

&lt;p&gt;Thus $\psi_{M}[n] = \frac{1}{M} \sum_{k=0}^{M-1} e^{-j k \omega_{o} n}$&lt;/p&gt;

&lt;p&gt;$x_{M}[n]=  \frac{1}{M} \sum_{k=0}^{M-1} e^{-j k \omega_{o} n}*x[n]$&lt;/p&gt;

&lt;p&gt;$X_{M}(z)=  \frac{1}{M} \sum_{n} \sum_{k=0}^{M-1} e^{-j k \omega_{o} n}*x[n] z^{-n}$&lt;/p&gt;

&lt;p&gt;$X_{M}(z)=  \frac{1}{M}  \sum_{k=0}^{M-1} \sum_{n} {e^{-j k \omega_{o} n}*z^{-n} }x[n] $&lt;/p&gt;

&lt;p&gt;$X_{M}(z)=  \frac{1}{M}  \sum_{k=0}^{M-1} \sum_{n} {e^{-j k \frac{2&lt;em&gt;\pi}{M} n}&lt;/em&gt;z^{-n} }x[n] $&lt;/p&gt;

&lt;p&gt;$X_{M}(z)=  \frac{1}{M}  \sum_{k=0}^{M-1} X(z * e^{j k \frac{2*\pi}{M}}) $&lt;/p&gt;

&lt;p&gt;$X_{down}(z) =  \frac{1}{M}  \sum_{k=0}^{M-1} X(z^{\frac{1}{M}} * e^{j k \frac{2*\pi}{M}}) $&lt;/p&gt;

&lt;p&gt;for M=2 this becomes&lt;/p&gt;

&lt;p&gt;$X_{2}(z)= \frac{1}{2} [  X(z^{\frac{1}{2}} ) + X(-z^{\frac{1}{2}} ) ] $&lt;/p&gt;

&lt;p&gt;The $X(-z^{\frac{1}{2}} ) $ represents aliasing compnent in downsampling&lt;/p&gt;

&lt;p&gt;The output of downsampling block is&lt;/p&gt;

&lt;p&gt;$Y_{3}[z]= \frac{1}{2} [  Y_{1}(z^{\frac{1}{2}} ) + Y_{1}(-z^{\frac{1}{2}} ) ] $&lt;/p&gt;

&lt;p&gt;$Y_{4}[z]= \frac{1}{2} [  Y_{2}(z^{\frac{1}{2}} ) + Y_{2}(-z^{\frac{1}{2}} ) ] $&lt;/p&gt;

&lt;h3 id=&quot;upsampling-block&quot;&gt;Upsampling Block&lt;/h3&gt;

&lt;p&gt;$X_{o}[z] = X_{in}(z^2)$&lt;/p&gt;

&lt;p&gt;$Y_{5}[z]= \frac{1}{2} [  Y_{1}(z ) + Y_{1}(-z)] $&lt;/p&gt;

&lt;p&gt;$Y_{6}[z]= \frac{1}{2} [  Y_{2}(z) + Y_{2}(-z) ] $&lt;/p&gt;

&lt;h3 id=&quot;synthesis-filtering&quot;&gt;Synthesis Filtering&lt;/h3&gt;

&lt;p&gt;$Y_{7}[z]= \frac{1}{2} [  Y_{1}(z ) + Y_{1}(-z)] G_{0}[z]$&lt;/p&gt;

&lt;p&gt;$Y_{8}[z]= \frac{1}{2} [  Y_{2}(z) + Y_{2}(-z) ] G_{1}[z]$&lt;/p&gt;

&lt;h3 id=&quot;output-of-filterbank&quot;&gt;Output of FilterBank&lt;/h3&gt;
&lt;p&gt;$Y(z) = Y_{7}[z]+Y_{8}[z]$&lt;/p&gt;

&lt;p&gt;$Y(z) = \frac{1}{2}  [H_{0}(z) G_{0}(z) + H_{1}(z) G_{1}(z)] X(z) + [H_{0}(-z) G_{0}(z) + H_{1}(-z) G_{1}(z)] X(-z)$&lt;/p&gt;

&lt;p&gt;$Y(z) = \frac{1}{2} [ G_{0}(z) , G_{1}(z) ] \left(\begin{array}{ccc} H_{0}(z) &amp;amp; H_{0}(-z) \ H_{1}(z) &amp;amp; H_{1}(-z)\end{array} \right) \left(\begin{array}{ccc} X(z) \ X(-z) \end{array} \right)$&lt;/p&gt;

&lt;p&gt;Thus the output of the filterbank is a linear combination of $X(z) $ and $X(-z)$&lt;/p&gt;

&lt;p&gt;The term $X(-z)$ is a consequence of aliasing&lt;/p&gt;

&lt;p&gt;$T_{0}(z)$ is component associated with aliasing term.For reconstruction to be possible this should be 0.&lt;/p&gt;

&lt;p&gt;In order that output of filter bank recovers $x[z]$ perfectly we require that&lt;/p&gt;

&lt;p&gt;$T_{0}(z) = H_{0}(-z) G_{0}(z) + H_{1}(-z) G_{1}(z) = 0$&lt;/p&gt;

&lt;p&gt;and $T_{1}(z) = R(z)[H_{0}(z) H_{1}(-z) - H_{1}(z) H_{0}(-z)] =2 $&lt;/p&gt;

&lt;p&gt;This gives us a condition that
$H_{0}(-z) G_{0}(z) =- H_{1}(-z) G_{1}(z) $&lt;/p&gt;

&lt;p&gt;$\frac{G_{0}(z)}{G_{1}(z)} =- \frac{H_{1}(-z)}{H_{0}(-z) }  $&lt;/p&gt;

&lt;p&gt;A very simple choice is ‘
$G_{0}(z)  = R(z) H_{1}(-z)$
$G_{1}(z)  = -R(z) H_{0}(-z)$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Two basic constraint for perfect reconstruction filter bank are 
$T_{1}(z) = R(z)[H_{0}(z) H_{1}(-z) - H_{1}(z) H_{0}(-z)] =2 $
$T_{0}(z) = H_{0}(-z) G_{0}(z) + H_{1}(-z) G_{1}(z) = 0$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As there are four function variables $H_0$, $H_1$, $G_0$ and $G_1$ in the two equations, there exist many possible designs for the filter banks.Some of which are amplitude complimentary filters,quadrature mirror filters,conjugate quadrature mirror filters,orthogonal filters,bi-orthogonal filters etc&lt;/p&gt;

&lt;h3 id=&quot;haar-filter-bank&quot;&gt;Haar filter Bank&lt;/h3&gt;

&lt;p&gt;$H_{0}[z] = \frac{1}{\sqrt2} [ 1 + z^{-1}]$
$H_{1}[z] = \frac{1}{\sqrt2} [ 1 - z^{-1}]$&lt;/p&gt;

&lt;p&gt;$G_{0}[z] = \frac{1}{\sqrt2} [ 1 + z^{-1}]$
$G_{1}[z] = -\frac{1}{\sqrt2} [ 1 - z^{-1}]$&lt;/p&gt;

&lt;p&gt;we can see that
$T_{0}(z) = H_{0}(-z) G_{0}(z) + H_{1}(-z) G_{1}(z) = 0$&lt;/p&gt;

&lt;p&gt;$T_{0}(z) = \frac{1}{2} [ 1 - z^{-1}][ 1 + z^{-1}] - [ 1 + z^{-1}] [ 1 - z^{-1}] =0$&lt;/p&gt;

&lt;p&gt;$T_{1}(z) = R(z)[H_{0}(z) H_{1}(-z) - H_{1}(z) H_{0}(-z)] $&lt;/p&gt;

&lt;p&gt;$T_{1}(z) =  \frac{1}{2} [ 1 + z^{-1}][ 1 + z^{-1}] - [ 1 - z^{-1}][ 1 - z^{-1}]] $&lt;/p&gt;

&lt;p&gt;$T_{1}(z) = z^{-1} $&lt;/p&gt;

&lt;p&gt;A delay of one sample and $R(z)=2$ for haar wavelets&lt;/p&gt;

&lt;p&gt;we can see that Haar wavelets satisfy the criteria for perfect reconstruction&lt;/p&gt;

&lt;p&gt;we can also see that if we replace the analysis filters with synthesis filter,the perfect reconstruction criteria is still satisfied&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Program Arduino Sketch Files From Commandline</title>
   <link href="http://localhost:4000/2014/12/06/18/"/>
   <updated>2014-12-06T00:00:00+00:00</updated>
   <id>http://localhost:4000/2014/12/06/18</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In this article we will look at how to program Arduino Sketch files from commandline&lt;/p&gt;

&lt;h3 id=&quot;arduino&quot;&gt;Arduino&lt;/h3&gt;

&lt;p&gt;Arduino  is an open-source physical computing platform based on a simple i/o board and a development environment that implements the Processing/Wiring language.&lt;/p&gt;

&lt;p&gt;Once Arduino board is connected to the USB port of Raspberry PI ,It will power on through the port.
We can program and communicate with the Arduino throught the USB Port using Serial Communication Protocol.&lt;/p&gt;

&lt;p&gt;If the Arduino is detected properly by Raspberry PI a file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/dev/ttyUSBX&lt;/code&gt; or /dev/ttyACMX` will be created.&lt;/p&gt;

&lt;p&gt;Following lines may be observed in the output of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dmeg |tail &lt;/code&gt; command&lt;/p&gt;
&lt;pre class=&quot;brush:python&quot;&gt;

 #dmeg |tail 
[21748.171290] usb 1-1.2: new full-speed USB device number 5 using dwc_otg
[21748.285641] usb 1-1.2: New USB device found, idVendor=2341, idProduct=0042
[21748.285679] usb 1-1.2: New USB device strings: Mfr=1, Product=2, SerialNumber=220
[21748.285695] usb 1-1.2: Manufacturer: Arduino (www.arduino.cc)
[21748.285708] usb 1-1.2: SerialNumber: 85334333931351907011
[21748.349390] cdc_acm 1-1.2:1.0: ttyACM0: USB ACM device
[21748.354136] usbcore: registered new interface driver cdc_acm
[21748.354174] cdc_acm: USB Abstract Control Model driver for USB modems and ISDN adapters

&lt;/pre&gt;

&lt;p&gt;The Arduino detected can be also verified form ` lsusb` command output&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

pi@raspberrypi ~ $ lsusb
Bus 001 Device 002: ID 0424:9514 Standard Microsystems Corp. 
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. 
Bus 001 Device 004: ID 148f:5370 Ralink Technology, Corp. RT5370 Wireless Adapter
**Bus 001 Device 005: ID 2341:0042 Arduino SA Mega 2560 R3 (CDC ACM)**


&lt;/pre&gt;

&lt;h3 id=&quot;udev-rulse&quot;&gt;UDEV Rulse&lt;/h3&gt;

&lt;p&gt;if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/dev/ttyUSBX&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/dev/ttyACMX&lt;/code&gt; files are not found.It means that the Raspberry PI has not identified that Arduino is connected to it&lt;/p&gt;

&lt;p&gt;Let us again refer the output of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lsusb&lt;/code&gt; command&lt;/p&gt;

&lt;p&gt;2341a is the vendor ID
0042 is the product ID&lt;/p&gt;

&lt;p&gt;Go To the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/udev/rules.d/&lt;/code&gt; directory&lt;/p&gt;

&lt;p&gt;Create a file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;99-arduino.rules&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;pre class=’brush:python”&amp;gt;&lt;/p&gt;

&lt;p&gt;SUBSYSTEMS==”usb”, ATTRS{idProduct}==”2341a”, ATTRS{0042}==”YYYY”, SYMLINK+=”ttyACM%n”&lt;/p&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;p&gt;SUBSYSTEMS==”usb”, ATTRS{2341a}==”XXXX”, ATTRS{0042}==”YYYY”, SYMLINK+=”ttyUSB%n”
&amp;lt;/pre&amp;gt;&lt;/p&gt;

&lt;p&gt;Finally reload the udev rules&lt;/p&gt;

&lt;p&gt;` udevadm control –reload_rules`&lt;/p&gt;

&lt;h3 id=&quot;setting-the-permissions&quot;&gt;Setting the Permissions&lt;/h3&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

pi@raspberrypi ~ $ ls -l /dev/ttyACM0 
crw-rw---T 1 root dialout 166, 0 Jan  1  1970 /dev/ttyACM0
pi@raspberrypi ~ $ 


&lt;/pre&gt;

&lt;p&gt;We can see that only root users and users belonging to group &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dialout&lt;/code&gt; have permission to access the device files and hence communicate over the USB-Serial Interface.&lt;/p&gt;

&lt;p&gt;To enable other users to communicate over the interface we need to add the user to the dialout gropup&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;usermod -a -G group-name username&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Also check the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/lock&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/run/lock&lt;/code&gt; directory for permissions which are created at boot time&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

pi@raspberrypi ~ $ ls -ld /run/lock
drwxrwxrwt 2 root root 40 Dec  6 15:17 /run/lock

&lt;/pre&gt;

&lt;p&gt;This 777 permission hence no issues&lt;/p&gt;

&lt;p&gt;If users other that root do not have permission to access the directory then we need to change the configuration&lt;/p&gt;

&lt;p&gt;You can include the commands to run on boot&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo chmod o+rwx /run/lock&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo chmod o+rwx /var/lock&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;arduino-ide&quot;&gt;ARDUINO IDE&lt;/h3&gt;

&lt;p&gt;The arduino integrated development environment allows editing, compiling and uploading sketches (programs) for Arduino (and compatible) microcontroller boards.&lt;/p&gt;

&lt;p&gt;Normally, running the arduino command starts the IDE, optionally loading any .ino files specified on the commandline.&lt;/p&gt;

&lt;p&gt;Running the compilation and build from commandline is only available for Arduino version 1.5.x or higher.This version is not available in standard repositories.The command line options can be found at link &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://github.com/arduino/Arduino/blob/ide-1.5.x/build/shared/manpage.adoc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Another alrenative is to install anduino-mk utility
` sudo apt-get install arduino-mk`&lt;/p&gt;

&lt;p&gt;The arduino-mk will install general purpose Arduino make file at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/share/arduino/Arduino.mk&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In this article we will look at this approach&lt;/p&gt;

&lt;h4 id=&quot;creating-makefile&quot;&gt;Creating Makefile&lt;/h4&gt;
&lt;p&gt;First step is to create a makefile in the same directory as the sketch file&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

BOARD_TAG    = mega2560         # Here goes your board type, e.g. : uno
ARDUINO_PORT = /dev/ttyACM0 # Change to your own tty interface
AVR_TOOLS_DIR=/usr/share/arduino/tools

ARDUINO_LIBS = # The libs needed by your sketchbook, examples are : Wire Wire/utility Ethernet...

ISP_PROG	   = stk500v2
ISP_PORT = /dev/ttyACM0
AVRDUDE_ISP_BAUDRATE=115200


include /usr/share/arduino/Arduino.mk # This is where arduino-mk installed

&lt;/pre&gt;

&lt;p&gt;If you are not sure of board name to be used leave it blank initially and type the below command&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

make show_boards`

.....
mega         Arduino Mega (ATmega1280)
**mega2560     Arduino Mega 2560 or Mega ADK**
mini         Arduino Mini w/ ATmega168

....

&lt;/pre&gt;

&lt;p&gt;In the present case the board was Arduino Mega hence the board name in the Makefile should be entered as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mega2560&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The ISP Program should contain the protocol used to commuicate with the Arduino Bootloader&lt;/p&gt;

&lt;p&gt;Arduino MEGA (1280 and 2560)  bootloader  uses the stk500v2 protocol operating at baud rate of 115200 while Arduino Uno uses stk500v1 protocol.The boot loaders and baud rate are  are board specific.The exact configuration for the board can be found by programming the Arduino from IDE with verbose upload output enabled from Preferences Menu.&lt;/p&gt;

&lt;p&gt;Below is verbose output for Arduino-Mega board&lt;/p&gt;
&lt;pre class=&quot;brush:python&quot;&gt;

/usr/share/arduino/hardware/tools/avrdude -C/usr/share/arduino/hardware/tools/avrdude.conf -v -v -v -v -patmega2560 -cstk500v2 -P/dev/ttyACM0 -b115200 -D -Uflash:w:/tmp/build6124755445833805312.tmp/test.cpp.hex:i

&lt;/pre&gt;

&lt;p&gt;we can now use the command make to compile your sketch, and make upload to send it to the board.
The makefile uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;avrdude&lt;/code&gt; utility to upload the hex file to arduino.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

sudo make
sudo make upload

&lt;/pre&gt;

&lt;p&gt;If the arduino utility is waiting from user inputs it can be passed through the serial interface.
A serial listner can be opened on the Raspberry pi for programming pruposes&lt;/p&gt;

&lt;p&gt;The code for the same is found below&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

 #! /usr/bin/python

 #    Serial Reader for ARDUINO
 #    usefull when tail -f /dev/ttyXXXX doesn't work
 #
 #    Change ttyACM0 for your own tty interface

import serial
import time

 # The second argument is the baudrate, change according to the baudrate you gave to your Serial.begin command
ser = serial.Serial(&quot;/dev/ttyACM0&quot;, 115200)

 # To avoid communication failure due to bad timings
ser.setDTR(True);
time.sleep(1);
ser.setDTR(False)

while True:
    print ser.readline()


&lt;/pre&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;http://playground.arduino.cc/Linux/All&lt;/li&gt;
  &lt;li&gt;https://github.com/sudar/Arduino-Makefile&lt;/li&gt;
  &lt;li&gt;http://hardwarefun.com/tutorials/compiling-arduino-sketches-using-makefile&lt;/li&gt;
  &lt;li&gt;http://baldwisdom.com/bootloading/&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Starting Programs at Boot using Supervisor Process Control Utility</title>
   <link href="http://localhost:4000/linux/2014/12/05/17/"/>
   <updated>2014-12-05T00:00:00+00:00</updated>
   <id>http://localhost:4000/linux/2014/12/05/17</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In this article we look at how to automatically to automatically start a set of programs upon startup using ‘Supervisor’ process control tool for embedded&lt;/p&gt;

&lt;p&gt;Typically in any embedded application we require a set of services to be running upon startup or boot into operating system.&lt;/p&gt;

&lt;p&gt;sudo apt-get install supervisor&lt;/p&gt;

&lt;p&gt;Supervisor is a client/server system that allows its users to control of processes on UNIX-like operating systems. It can be user to start/stop/restart/monitor a single or group of processes&lt;/p&gt;

&lt;p&gt;Supervisor consists of following important components&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;supervisord - Server&lt;/li&gt;
  &lt;li&gt;supervisorctl - commandline client&lt;/li&gt;
  &lt;li&gt;Web Server&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;installing&quot;&gt;Installing&lt;/h3&gt;

&lt;p&gt;If the Python interpreter you’re using has Setuptools installed, and the system has internet access, you can download and install supervisor in one step using easy_install.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

easy_install supervisor

&lt;/pre&gt;

&lt;p&gt;Depending on the permissions of your system’s Python, you might need to be the root user to install Supervisor successfully using easy_install.&lt;/p&gt;

&lt;p&gt;Or you can download the supervisor package from http://pypi.python.org/pypi/supervisor and install it.
After unpacking the downloaded software archive, run&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python setup.py install&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It will download and install all distributions depended upon by Supervisor and finally install Supervisor itself.&lt;/p&gt;

&lt;h3 id=&quot;starting-supervisor-upon-startup&quot;&gt;Starting supervisor upon startup&lt;/h3&gt;
&lt;p&gt;In Unix-based computer operating systems, init (short for initialization) is the first process started during booting of the computer system. Init is a daemon process that continues running until the system is shut down&lt;/p&gt;

&lt;p&gt;Init scripts are the scripts located in /etc/init.d. These scripts are part of the bootup sequence of Ubuntu.&lt;/p&gt;

&lt;p&gt;During boot, they are not called directly, but through a structure of symbolic links which manage the services which are to be started in a particular runlevel.  The scripts which are symlinked from /etc/rcS.d are executed first. Then the scripts in /etc/rcN.d/ are executed.&lt;/p&gt;

&lt;p&gt;The naming convention of symlinks are   &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/rc[L].d/[S/K][NN]name &lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;L is the chosen runlevel (default 2)&lt;/p&gt;

    &lt;p&gt;runlevel S  is  one  of  the  runlevels  supported   by   init,   namely,
  `      0123456789S`&lt;/p&gt;

    &lt;p&gt;Runlevel 0 is used to halt the system and 6 to reboot the system
 and 2 is default runlevel ,1 is runlevel to boot into single user more&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;NN  is the two-digit sequence number that determines where in the sequence init will run the scripts.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The K links are responsible for killing services and the S link for starting services upon entering the runlevel.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After installation we can find the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;supervisor&lt;/code&gt; in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/init.d/&lt;/code&gt; directory indicating that it can be accessed as a service&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

root@prasad-TA790GX-A3:/media/sda2# ls -alrt /etc/rc2.d/S20supervisor
lrwxrwxrwx 1 root root 20 Dec  5 13:01 /etc/rc2.d/S20supervisor -&amp;gt; ../init.d/supervisor
root@prasad-TA790GX-A3:/media/sda2# 

&lt;/pre&gt;

&lt;p&gt;we can also see that symlink for supervisor is present in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rc2.d&lt;/code&gt; directory.Runlevel 2 is default runlevel in Ubuntu OS.Indicating that the service will start upon boot.And through supervisor we can control various processes being managed by supervisor.This include starting a set of processes at boot and stopping a set of processes at shutdown/reboot.&lt;/p&gt;

&lt;p&gt;####Creating a Configuration File
Once the Supervisor installation has completed, run echo_supervisord_conf. This will print a “sample” Supervisor configuration file to your terminal’s stdout.&lt;/p&gt;

&lt;p&gt;Once you see the file echoed to your terminal&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;echo_supervisord_conf &amp;gt; supervisord.conf. &lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Once you have a configuration file on your filesystem, you can begin modifying it to your liking.&lt;/p&gt;

&lt;p&gt;####Adding a Program&lt;/p&gt;

&lt;p&gt;To add a program, you’ll need to edit the supervisord.conf file.
The section relevant to present article is ` [program:x] Section ` of configuration file .&lt;/p&gt;

&lt;p&gt;The program section will define a program that is run and managed when you invoke the supervisord command.&lt;/p&gt;

&lt;p&gt;The configuration file must contain one or more program sections in order for supervisord to know which programs it should start and control&lt;/p&gt;

&lt;p&gt;The header value is composite value. It is the word “program”, followed directly by a colon, then the program name&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

[program:foo]
command=&quot;/bin/echo %(program_name)s_%(process_num)02d &quot;
numprocs=1 
process_name=%(program_name)s
redirect_stderr: true
autorestart: true

&lt;/pre&gt;

&lt;p&gt;In the above example a header value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[program:foo]&lt;/code&gt; describes a program with the logical name of “foo”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Command&lt;/strong&gt; 
The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;command&lt;/code&gt; section value will contain the command that will be run when this program is started.
command line arguments can be passed to program .In the above configuration the command line arguments are predefined  string expressions supported by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;supervisor&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now starting the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;supervisor&lt;/code&gt; service will create a file in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tmp&lt;/code&gt; directory&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;foo-stdout---supervisor-XXX.log and foo-stderr---supervisor-a35d4Z.log&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The contents of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp/foo-stdout---supervisor-XXX.log&lt;/code&gt; will contain the output of echo command which is pushed onto the stdout&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;logfile=/tmp/supervisord.log ; (main log file;default $CWD/supervisord.log)&lt;/code&gt;
specified in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;supervisord&lt;/code&gt; section of the configuration file defines where the log files will be created&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
foo_00
foo_00
foo_00
foo_00

&lt;/pre&gt;

&lt;p&gt;Thus we can see that logical program name is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;foo&lt;/code&gt; and value assigned to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;process_num&lt;/code&gt; is a 2 digit decimal value 00 due to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%(process_num)02d&lt;/code&gt; string literal expression.&lt;/p&gt;

&lt;p&gt;The section also include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autorestart&lt;/code&gt; parameter  which If set to true will cause the process to be unconditionally restarted when it exits, without regard to its exit code.We can see that it will restart the above program a fixed number of times .However if the program has some fatal error due to which it restarts too many times,it will stop the restart process .&lt;/p&gt;

&lt;p&gt;Since echo command terminates immediately,it will stop after some attempts.In present case after 4 attempts.We can observe the following lines in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tmp/supervisord.log&lt;/code&gt; file which indicate why program was terminated&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

2014-12-05 14:39:43,153 CRIT Supervisor running as root (no user in config file)
2014-12-05 14:39:43,155 INFO daemonizing the supervisord process
2014-12-05 14:39:43,157 INFO supervisord started with pid 5344
2014-12-05 14:39:44,161 INFO spawned: 'foo' with pid 5345
2014-12-05 14:39:44,172 INFO exited: foo (exit status 0; not expected)
2014-12-05 14:39:45,176 INFO spawned: 'foo' with pid 5346
2014-12-05 14:39:45,188 INFO exited: foo (exit status 0; not expected)
2014-12-05 14:39:47,193 INFO spawned: 'foo' with pid 5347
2014-12-05 14:39:47,206 INFO exited: foo (exit status 0; not expected)
2014-12-05 14:39:50,211 INFO spawned: 'foo' with pid 5349
2014-12-05 14:39:50,221 INFO exited: foo (exit status 0; not expected)
2014-12-05 14:39:51,222 INFO gave up: foo entered FATAL state, too many start retries too quickly

&lt;/pre&gt;

&lt;h3 id=&quot;multiprocess-applications&quot;&gt;Multiprocess applications&lt;/h3&gt;

&lt;p&gt;A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[program:x]&lt;/code&gt; section can also be used to represents a “homogeneous process group” to supervisor (as of 3.0). The members of the group are defined by the combination of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numprocs&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;process_name &lt;/code&gt;parameters in the configuration&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

[program:foo1]
command=/bin/echo %(program_name)s_%(process_num)02d 
numprocs=3 
process_name=%(program_name)s_%(process_num)02d
redirect_stderr: true
autorestart: false
startretries=0  

&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;numprocs&lt;/strong&gt;
Supervisor will start as many instances of this program as named by numprocs.This can be used in case of multiprocess applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;process_name&lt;/strong&gt;
A Python string expression that is used to compose the supervisor process name for this process. This is mandatory if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numproc&lt;/code&gt;  section value &amp;gt;1.&lt;/p&gt;

&lt;p&gt;The default value is :%(program_name)s&lt;/p&gt;

&lt;p&gt;It can contain string expression values : group_name, host_node_name, process_num, program_name&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;But for instance, if you have a [program:foo1] section with a numprocs of 3 and a process_name expression of %(program_name)s_%(process_num)02d, the “foo1” group will contain three processes, named foo1_00, foo1_01, and foo1_02 as seen below&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

-rw-------  1 root    root       16 Dec  5 15:08 foo1_02-stdout---supervisor-AkgbT1.log
-rw-------  1 root    root       16 Dec  5 15:08 foo1_01-stdout---supervisor-LYqA26.log
-rw-------  1 root    root       16 Dec  5 15:08 foo1_00-stdout---supervisor-xRoT5o.log

&lt;/pre&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;startretries=0&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autorestart=false&lt;/code&gt; cause the program to not be restarted upon exit.&lt;/p&gt;

&lt;p&gt;This makes it possible to start a multiprocess application from a single program configuration&lt;/p&gt;

&lt;p&gt;####Running supervisord&lt;/p&gt;

&lt;p&gt;To start supervisord, run supervisord. The resulting process will daemonize itself and detach from the terminal. It keeps an operations log at supervisor.log file in the directory specified by configuration files&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

sudo supervisord -c &quot;$HOME/supervisord.conf&quot;

&lt;/pre&gt;

&lt;p&gt;While starting the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;supervisord&lt;/code&gt; service using init.d scripts the command line arguments cannot be passed.In which case it searches the configuration file in default location &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/supervisord.conf&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;web-server&quot;&gt;Web Server&lt;/h3&gt;

&lt;p&gt;The various processes managed by the supervisor can be accessed by webservice.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

[inet_http_server]         ; inet (TCP) server disabled by default
port=*:9001       		   ; (ip_address:port specifier, *:port for all iface)
username=user              ; (default is no username (open server))
password=123               ; (default is no password (open server))

&lt;/pre&gt;

&lt;p&gt;The following section must be present in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;supervisord.conf&lt;/code&gt; file so that webserver is started when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;supervisord service&lt;/code&gt; is started&lt;/p&gt;

&lt;p&gt;The webservice can be accessed from the browser by accessing the url &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://${hostname}:9001/&lt;/code&gt;
for example in the present case http://192.168.1.3:9001&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/others/d1.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The sample &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;supervisord.conf&lt;/code&gt; files and sample script used in article can be found at tutorials/supervisor tree path of github &lt;a href=&quot;pi19404.github.io/pyVision/&quot;&gt;pyVision&lt;/a&gt; repository&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Installing OS on Raspberry PI using NOOBS</title>
   <link href="http://localhost:4000/raspberry%20pi/2014/12/04/16/"/>
   <updated>2014-12-04T00:00:00+00:00</updated>
   <id>http://localhost:4000/raspberry%20pi/2014/12/04/16</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In this article we will look at installing operating system on Raspberry Pi using NOOBS operating system install manager&lt;/p&gt;

&lt;h3 id=&quot;noobs&quot;&gt;NOOBS&lt;/h3&gt;

&lt;p&gt;To get started with Raspberry Pi you need an operating system. NOOBS (New Out Of the Box Software) is an easy operating system install manager for the Raspberry Pi.&lt;/p&gt;

&lt;p&gt;The New Out Of Box Software (NOOBS) system not only makes it extremely simple to go from a blank SD card to an installed copy of Rasbian, but it also comes prepackaged with alternative Raspberry Pi operating systems like Pidora (a Fedora-based build), RISC OS (a modern repackaging of the speedy 1990s ARM-based desktop operating system), Arch (an Arch Linux port for the Pi), and two distributions of XBMC: Raspbmc and OpenELEC.&lt;/p&gt;

&lt;p&gt;The following Operating Systems are currently included in NOOBS:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Raspbian&lt;/li&gt;
  &lt;li&gt;Pidora&lt;/li&gt;
  &lt;li&gt;OpenELEC&lt;/li&gt;
  &lt;li&gt;RaspBMC&lt;/li&gt;
  &lt;li&gt;RISC OS&lt;/li&gt;
  &lt;li&gt;Arch Linux&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As of NOOBS v1.3.10 (September 2014), only Raspbian is installed by default in NOOBS. The others can be installed with a network connection.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt;
NOOBS can be  download  for free and installed it on your SD card.&lt;/p&gt;

&lt;p&gt;NOOBS is available for download on the Raspberry Pi website: &lt;a href=&quot;raspberrypi.org/downloads&quot;&gt;raspberrypi.org/downloads&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;NOOBS is available in two forms: offline and network install; or network install only.&lt;/p&gt;

&lt;p&gt;The full version has the images of each of the operating systems included, so they can be installed from the SD card while offline, whereas NOOBS Lite requires an internet connection to download the selected operating system.&lt;/p&gt;

&lt;p&gt;In this artile the NOOBS we will install OS on Raspberry PI using NOOBS offline version is used .&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/16/r1.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Download the NOOBS offline and network install package.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Install NOOBS on SD CARD&lt;/strong&gt;
Raspberry PI requires a  SD card that is 4GB or greater in size .&lt;/p&gt;

&lt;p&gt;We will be performing the operations on Linux OS
Download and install the following filesystem partition management utilities&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;gparted - Partition Editor&lt;/li&gt;
  &lt;li&gt;pysdm - Storage device manager&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Formatting the SD Card&lt;/strong&gt;
Using Tools like gparted format the entire SD Card as FAT32 Filesystem&lt;/p&gt;

&lt;p&gt;Delete any existing partitions so that entire space on SDCard in unallocated&lt;/p&gt;

&lt;p&gt;RightClick on the unallocated space and select the “New” options to create a new parition&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/16/r5.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Select the filesystem type and FAT32&lt;/p&gt;

&lt;p&gt;Assign the filesystem lable ( not assiging a lablel causes issues while installing OS on Raspberry PI)&lt;/p&gt;

&lt;p&gt;Make sure all the space on the SD Card is selected by setting “New Size” to “Maximum Size”&lt;/p&gt;

&lt;p&gt;Click on Add to configure the formatting options&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/16/r2.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click on  the “Apply” option to format the SD Card or select the menu option
“Edit -&amp;gt; Apply All Operations” to begin formatting the SD Card&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assigning filesystem permissions&lt;/strong&gt;
Now open the pysdm utility as root by typing “sudo pysdm” on the terminal&lt;/p&gt;

&lt;p&gt;Choose the suitable Device from the list .
In the present case the SD card is identified as “/dev/sdc”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/16/r6.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Select the Assistant menu&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mounting Menu :set the umask for “directory”,”file” and “file and directory” permissions  to 000 indicating that any user can read,write and execute files on the filesystem.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/16/r7.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Special Files : select all the options which enables any user to execute binary on the filesystem&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/16/r8.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Copying the NOOBS File&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Extract the files contained in this NOOBS zip file.(NOOBS_v1_3_10.zip)&lt;/li&gt;
  &lt;li&gt;Copy extracted files onto the SD Card so that files are placed in the root directory of the SD card&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Starting Installation Process&lt;/strong&gt;
Insert the SD card into your Pi and connect the power supply.&lt;/p&gt;

&lt;p&gt;The first thing done on the first boot is  “RECOVERY” FAT partition will be automatically resized to a minimum, and a list of OSs that are available to install will be displayed.&lt;/p&gt;

&lt;p&gt;If filesytem is not assigned a lable or filesystem permissions are not set then resizing operation fails and it may lead to different types of issues  .&lt;/p&gt;

&lt;p&gt;Once the initialization process is done, you’ll be kicked into the NOOBS installation menu is displayed
Select the “Raspbian” and click on Install to start with installation&lt;/p&gt;

&lt;p&gt;It will take some time to unpack and install the operating system on the SD Card
Once the process is finished, you can boot right into your installed operating systems&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Approximation of Piecewise Polynomial Using Wavelets</title>
   <link href="http://localhost:4000/signal%20processing/2014/11/29/14/"/>
   <updated>2014-11-29T00:00:00+00:00</updated>
   <id>http://localhost:4000/signal%20processing/2014/11/29/14</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In this article we look at Approximation of Piecewise Polynomial Using Wavelets&lt;/p&gt;

&lt;h3 id=&quot;wavelet-approximation-of-polynomials&quot;&gt;Wavelet Approximation of polynomials&lt;/h3&gt;

&lt;p&gt;In many application,one needs to approximate a signal using scaling function ie using projection on $V_{m}$ subspace.&lt;/p&gt;

&lt;p&gt;The support of a function is the set of points where the function is not zero-valued,The support of $\phi(t),\psi(t)$ which is defined over unit interval is 1&lt;/p&gt;

&lt;p&gt;The basic analysis starts with considering a set of monomials  ${1,t,t^2,\ldots,t^{k} }$ and asking the question till what degree $k$ can these be reproduced exactly using the scaling function.&lt;/p&gt;

&lt;p&gt;Let us assume that $t^{p}$ can be represented exactly using the scaling function
$t^{p} = \sum_{k} d_{k} \phi(t - k)$&lt;/p&gt;

&lt;p&gt;Orthonormality of basis function impies&lt;/p&gt;

&lt;p&gt;$d_{k} = \int t^{p} \phi(t -k ) dt$&lt;/p&gt;

&lt;p&gt;To achieve this the scaling function should possess a certain properties which is dependent on the filter coefficients .&lt;/p&gt;

&lt;h3 id=&quot;restrictions-on-scaling-and-wavelet-function&quot;&gt;Restrictions on scaling and wavelet function&lt;/h3&gt;

&lt;p&gt;This assumption will impose certain restriction on the scaling and wavelet function&lt;/p&gt;

&lt;p&gt;$\int t^{p} \psi(t) dt = \int \sum_{k} d_{k} \phi(t - k)\psi(t) dt = \sum_{k} d_{k} \int \phi(t - k)\psi(t) dt $&lt;/p&gt;

&lt;p&gt;$\psi(t) ,\phi(t)$ are orthogonal ,meaning that $\phi(t)$ is capable of expressing polynomials upto degree $p$  exactly.&lt;/p&gt;

&lt;p&gt;The projection of $t^{p} $ on $W_{m}$  subspace is 0.
The projection of $t^{p} $ on $W_{m}$  subspace will not be zero only when it cannot be expressed completely by the scaling function.&lt;/p&gt;

&lt;p&gt;if $p=0$,The condition implies&lt;/p&gt;

&lt;p&gt;$\int \psi(t) dt =0$&lt;/p&gt;

&lt;p&gt;Indicating that a  constant function can always be expressed completely by scaling function.&lt;/p&gt;

&lt;p&gt;we know that&lt;/p&gt;

&lt;p&gt;$\psi(t) = \sum_{k} g[k] \phi(2t - k)$&lt;/p&gt;

&lt;p&gt;$\phi(t) = \sum_{k} h[k] \phi(2t -k )$&lt;/p&gt;

&lt;p&gt;We will use a result here which will be derived in later articles&lt;/p&gt;

&lt;p&gt;$\int \psi(t) \phi(t) dt =0$ for this to hold true&lt;/p&gt;

&lt;p&gt;$g[k]=(-1)^{N-k-1}h(N-k-1)$ and&lt;/p&gt;

&lt;p&gt;$\psi(t) = \sum_{k} (-1)^{N-k-1}h(N-k-1) \phi(2t + k -N+1)$&lt;/p&gt;

&lt;p&gt;for even N&lt;/p&gt;

&lt;p&gt;$g[k]=(-1)^{k}h(N-k-1)$&lt;/p&gt;

&lt;p&gt;$\int \sum_{k} (-1)^{N-k-1}h(N-k-1) \phi(2t + k -N+1) dt=0$&lt;/p&gt;

&lt;p&gt;$\sum_{k} (-1)^{k} h(k) \int \phi(y) dy=0$&lt;/p&gt;

&lt;h3 id=&quot;vanishing-momemnt-constraint-on-wavelet-filter-coefficients&quot;&gt;Vanishing Momemnt Constraint on wavelet filter coefficients&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Zero order vanishing moment constraint
$\sum_{k} (-1)^{k} h(k) =0$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;pth order vanishing moment constraint
$\sum_{k} (-1)^{k} k^{p}h(k) =0$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;These vanishing moment constraint imposed on scaling and wavelet function help solve for the filter coefficients.&lt;/p&gt;

&lt;h3 id=&quot;projection-of-polynomials-on-subspace-defined-by-wavelets&quot;&gt;Projection of polynomials on subspace defined by Wavelets&lt;/h3&gt;

&lt;p&gt;Wavelet function $\psi(t)$ having N vanishing moments will kill polynomial upto degree $p-1$&lt;/p&gt;

&lt;p&gt;Let us look at a Haar wavelets function and projection of increasing order of polynomials on Haar wavelet basis.Haar wavelet has 1 vanishing moment.&lt;/p&gt;

&lt;p&gt;Thus it can only kill polynomial of order 1 or constant function&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/14/w11.png&quot; alt=&quot;Haar Waveletvanishing moment &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Daubechies -2 wavelet has vanishing moment of 2 ,Thus it can kill polynomial upto degree of 2
A constant and linear function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/14/w12.png&quot; alt=&quot;Daubechies -2 wavelet  vanishing moment &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Daubechies -4 has vanishing moment of 4 ,Thus it can kill polynomials upto a degree of 3&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/14/w14.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What this means is that projection on $W_{o}$ subspace is zero.Wavelet coefficients will have low magnitude .Typically threshold should be less than $1^{-7}$.This will imply signal can be reconstructed from the projection on $V_{0}$ subspace to a great degree of accuracy&lt;/p&gt;

&lt;p&gt;We can see the projection on the  $V_{0}$ and  $W_{0}$ subspace in the below figures&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/14/w17.png&quot; alt=&quot;Haar Waveletvanishing moment &quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/14/w18.png&quot; alt=&quot;Daubechies -2 wavelet  vanishing moment &quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/14/w19.png&quot; alt=&quot;Daubechies -4 wavelet  vanishing moment &quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;projection-of-piecewise-polynomials-on-subspace-defined-by-wavelets&quot;&gt;Projection of Piecewise polynomials on subspace defined by Wavelets&lt;/h3&gt;

&lt;p&gt;It is important to note that it is sufficient that function behaves like a polynomial of degree $k$ over support of function for it to be approximated by scaling function.&lt;/p&gt;

&lt;p&gt;Below figures show piecewise polynomials and projection on $V_{0}$ and $W_{0}$ subspace&lt;/p&gt;

&lt;p&gt;We can see that piece-wise polynomials of degree $p$ within the support of wavelet functions with vanishing moment $p$  have zero wavelet coefficients except at points of discontinuity&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/14/w20.png&quot; alt=&quot;Haar wavelet piecewise polynomial &quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/14/w21.png&quot; alt=&quot;Daubechies -2 piecewise polynomial &quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;
&lt;p&gt;The function &lt;strong&gt;plotwaveletProjection&lt;/strong&gt; plots the projection of signal onto the $V_{m}$ and $W_{m}$ subspace&lt;/p&gt;

&lt;pre lang=&quot;brush:python&quot;&gt;

def plotWaveletProjection(x,coeff,w,level=1,mode=0):
    &quot;&quot;&quot; function plots the projection of signal on the scaling and wavelet functions subspace
    
    Parameters
    -----------
    x           : numpy-array,
                  The input signal
             
    coeff      : numpy=-array
                 The wavelet or scaling coefficient
                 
    w           : pywt.Wavelet
                  wavelet object
                  
    level       : integer
                  decomposition level
                  
    mode        : integer
                  scaling or wavelet coefficient
             
        
    Returns
    --------
    out : numpy-tuple
          (time,reconstruction,signal)
    
    &quot;&quot;&quot;
    
    #generate the scaling and wavelet functions
    s1,w1,t2=w.wavefun(level=20)
    #setup 1D interpolating function for scaling and wavelet functions
    wavelet=interpolate.interp1d(t2, w1)
    scaling=interpolate.interp1d(t2, s1)
    
    
    time=[]
    sig=[]
    s1=np.array(s1,float)
    
   
    #compute the dydactic scale
    l=2**level
    
    #find the support
    end1=math.floor(t2[len(t2)-1])
    d=abs(float(len(coeff))*float(l)-len(x))
    d=int(d)
    
    #range over each element of coefficient
    for i in range(len(coeff)-1):
       
       #define the interpolation points
       t=np.linspace(l*i,l*i+l*end1,l*end1*(len(x)));
       
       t1=np.linspace(0,end1,end1*(len(x))*l);
       
       #multiply the coefficient value with scaling or interpolation function
       if mode==0:
           val=coeff[i]*scaling(t1)
       else:
           val=coeff[i]*wavelet(t1)
           

       ratio=end1
       #compute the translation
       inc=len(t)/ratio
       
       
       if i==0:
               sig=np.append(sig,val)
               time=np.append(time,t)
               
       else:
               #compute the incremental sum of signals
               v1=val[0:len(t)-inc]
               
               if  inc &amp;amp;lt len(t):
                   v2=val[len(t)-inc:len(t)]
                   sig[i*inc:i*inc+len(t)]=sig[i*inc:i*inc+len(t)]+v1
                   sig=np.append(sig,v2)
                   time=np.append(time,np.linspace(l*i+l*end1-l,l*i+l*end1,inc))       
               else:
                   sig=np.append(sig,val)
                   time=np.append(time,t)      
    
    #flatten the arrays
    sig=np.array(sig).flatten()
    time=np.array(time).flatten().ravel()

    #scale the values due to didactic decomposition
    sig=sig/(math.sqrt(2)**level)

    #upsamples the value of signal
    x=np.repeat(x,len(sig)/len(x))

    #return the signals,which can be plotted
    return time,sig,x
    
 &lt;/pre&gt;

&lt;p&gt;A class called “PiecewiseContinuous “ encapsulates all the methods that define a piece-wise continuous function.The function is modified version of the function from sagemath library&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

 # piecewise function
def f1(x):return 10
def f2(x):return 5*x
def f3(x):return 2*(x)**2
def f4(x):return (x)**3+(x)**2
def f5(x):return 20*((x)**5)

f = Piecewise([[(0,1),f5],[(1,2),f4],[(2,3),f3],[(3,4),f2],[(4,5),f1]])

f(1) will give the value the value of piecewise function at 1

&lt;/pre&gt;

&lt;p&gt;All the plots and results presented in the article can be generated by running the wavelet4.py files&lt;/p&gt;

&lt;h4 id=&quot;files&quot;&gt;Files&lt;/h4&gt;
&lt;p&gt;The code can be found in &lt;a href=&quot;http://pi19404.github.io/pyVision/&quot;&gt;pyVision&lt;/a&gt; github repository&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;pyVision/pySignalProc/tutorials/wavelet4.py&lt;/li&gt;
  &lt;li&gt;pyVision/pySignalProc/wavelet/pywtUtils.py&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Haar Wavelet Filter Bank</title>
   <link href="http://localhost:4000/signal%20processing/2014/11/26/13/"/>
   <updated>2014-11-26T00:00:00+00:00</updated>
   <id>http://localhost:4000/signal%20processing/2014/11/26/13</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In this article we will look at discrete time signal processing using wavelets,more specificially we will look at the concept of Haar Filterbank&lt;/p&gt;

&lt;p&gt;In the previous article &lt;a href=&quot;http://pi19404.github.io/pyVision/2014/11/20/12/&quot;&gt;Haar Wavelets&lt;/a&gt; we saw the basics of harr wavelet and how a continuous time/discrete time signal can be expressed as sum of projection onto sub-spaces defined by wavelet and scaling function.&lt;/p&gt;

&lt;h3 id=&quot;analysisdecomposition-filters&quot;&gt;Analysis/Decomposition Filters&lt;/h3&gt;

&lt;p&gt;We saw that if we assume that the sampled signal is a projection onto subspace $V_{0}$
then using Harr scaling and wavelet function we can express the signal in terms of its projection on the subspaces $V_{-1} $ and $ W_{0}$&lt;/p&gt;

&lt;p&gt;The process of expressing the signal by projecting on to subspaces of lower resolution is called as wavelet decomposition.&lt;/p&gt;

&lt;p&gt;At each decomposition stage we project signal in subspace $V_{m}$ to subspaces $V_{m-1}$ and $W_{m}$
which are defined by scaling and wavelet functions.&lt;/p&gt;

&lt;p&gt;We can see that once a we consider a projection on $V_{m}$ subspace we are dealing with a discrete time sequence.Let sequence be represented by  $x[n]$&lt;/p&gt;

&lt;p&gt;The projection of signal from $V_{m}$  to $V_{m-1}$ using haar scaling function can be&lt;/p&gt;

&lt;p&gt;$\displaystyle y[n] = \frac{1}{\sqrt2}  x[2n] + x[2n+1]$&lt;/p&gt;

&lt;p&gt;Let $y_{o}[n]= \frac{1}{\sqrt2}  x[n] + x[n+1]$&lt;/p&gt;

&lt;p&gt;$\displaystyle _{l}[n] = y_{o}[2n]$&lt;/p&gt;

&lt;p&gt;The projection operation can be obtained by performing following steps&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apply discrete time filter $h_{l}[n]$ to discrete time signal which represents the projection of signal onto $V_{m}$ subspace&lt;/li&gt;
  &lt;li&gt;Downsample the signal by a factor of 2 to get the projection on $V_{m-1}$ subspace&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The projection of signal from  $V_{m}$ to $W_{m}$ subspace using haar wavelet function can be expressed as&lt;/p&gt;

&lt;p&gt;$\displaystyle y[n] = \frac{1}{\sqrt2}  x[2n] - x[2n+1]$&lt;/p&gt;

&lt;p&gt;Let $y_{o}[n]= \frac{1}{\sqrt2}  x[n] - x[n+1]$&lt;/p&gt;

&lt;p&gt;$\displaystyle y_{h}[n] = y_{o}[2n]$&lt;/p&gt;

&lt;p&gt;The projection operation can be obtained by performing following steps&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apply discrete time filter $h_{u}[n]$ to discrete time signal which represents the projection of signal onto $V_{m}$ subspace&lt;/li&gt;
  &lt;li&gt;Downsample the signal by a factor of 2 to get the projection on $W_{m}$ subspace&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\displaystyle h_{l}[n]= \frac{1}{\sqrt2} [ 1 , 1 ] $ or $h_{l}[z] = \frac{1}{\sqrt2}[ 1 + z^{-1}]$&lt;/p&gt;

&lt;p&gt;$\displaystyle h_{u}[n]= \frac{1}{\sqrt2} [- 1 , 1 ] $ or $h_{u}[z] = \frac{1}{\sqrt2}[ 1- z^{-1}]$&lt;/p&gt;

&lt;p&gt;$h_{l}[n]$ computes the average of neighborhood samples acting as a low pass filter.The filter that projects onto subspace $V_{m-1}$ is called as low pass decomposition filter or low pass analysis filter&lt;/p&gt;

&lt;p&gt;$h_{h}[n]$ computes the difference between neighborhood samples acting as a high pass filter.The filter that projects onto subspace $W_{m}$ is called as high pass pass decomposition filter or high pass analysis filter&lt;/p&gt;

&lt;p&gt;The projection operation can be represented graphically as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w2.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reconstructionsynthesis-filters&quot;&gt;Reconstruction/Synthesis Filters&lt;/h3&gt;

&lt;p&gt;Given projection on signal on $V_{m-1}$ and $W_{m}$,given&lt;/p&gt;

&lt;p&gt;$\displaystyle y_{l}[n] = \frac{1}{\sqrt2}  x[2n] + x[2n+1]$&lt;/p&gt;

&lt;p&gt;$\displaystyle y_{h}[n] = \frac{1}{\sqrt2}  x[2n] - x[2n+1]$&lt;/p&gt;

&lt;p&gt;we can see that
$\displaystyle x_{1}[n]=x[2n] = \frac{1}{\sqrt2} [y_{l}[n] + y_{h}[n] ]$&lt;/p&gt;

&lt;p&gt;$\displaystyle x_{1}[n]=x[2n+1] = \frac{1}{\sqrt2} [y_{l}[n] - y_{h}[n] ]$&lt;/p&gt;

&lt;p&gt;To get $x[n]$ we need to place  samples of $x_{1}[n]$ are even integer location 
and $x_{2}[n] $ at  odd integer location of $x[n]$&lt;/p&gt;

&lt;p&gt;As example if $x_{1}[n]={1,2,3,4} $ and $x_{2}[n]={5,6,7,8}$
the desired sequence is $x[n]={1,5,2,6,3,7,4,8}$&lt;/p&gt;

&lt;p&gt;This can be accomplished by first upsampling both the signals 
This will create a sequence new sequences $x_{1}[n],x_{2}[n]$ with zeros at odd integer locations&lt;/p&gt;

&lt;p&gt;${1, 0 ,2 ,0, 3 ,0 4, 0, }$ and ${5, 0 ,6 ,0, 7 ,0 8, 0, }$&lt;/p&gt;

&lt;p&gt;no  we delay $x_{2}[n]$ ,this will lead to sequence  with zeros at even integer locations&lt;/p&gt;

&lt;p&gt;$\displaystyle {0,5, 0 ,6 ,0, 7 ,0 ,8}$
$\displaystyle {1, 0 ,2 ,0, 3 ,0 ,4, 0, }$&lt;/p&gt;

&lt;p&gt;and adding both these signals will give us desired $x[n]$&lt;/p&gt;

&lt;p&gt;$\displaystyle {1,5,2,6,3,7,4,8}$&lt;/p&gt;

&lt;p&gt;This can be graphically represented as
&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w4.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can as well upsample the original signals and then perform addition and subtraction and get the same result.&lt;/p&gt;

&lt;p&gt;The equivalent block diagram can be expressed as
&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/wave6.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\displaystyle x_{1}[n] = \frac{1}{\sqrt2} [y_{l}[\frac{n}{2}] + y_{h}[\frac{n}{2}] ]$&lt;/p&gt;

&lt;p&gt;$\displaystyle x_{2}[n] = \frac{1}{\sqrt2} [y_{l}[\frac{n}{2}] - y_{h}[\frac{n}{2}] ]$&lt;/p&gt;

&lt;p&gt;$\displaystyle x[n] = x_{1}[n] + x_{2}[n-1]$&lt;/p&gt;

&lt;p&gt;The delay element can be shifted to the source&lt;/p&gt;

&lt;p&gt;$\displaystyle x[n] = \frac{1}{\sqrt2} [y_{l}[\frac{n}{2}] + y_{h}[\frac{n}{2}] ]+ [y_{l}[\frac{n-1}{2}] - y_{h}[\frac{n-1}{2}] ]$&lt;/p&gt;

&lt;p&gt;$\displaystyle x[n] = \frac{1}{\sqrt2} [y_{l}[\frac{n}{2}] + y_{l}[\frac{n-1}{2}] ]+ [y_{h}[\frac{n}{2}] - y_{h}[\frac{n-1}{2}] ]$&lt;/p&gt;

&lt;p&gt;$\displaystyle x[n] =  y_{l}[\frac{n}{2}] * h_{r,l}[n]+y_{h}[\frac{n}{2}] *h_{r,h}[n]$&lt;/p&gt;

&lt;p&gt;where 
$\displaystyle h_{r,l}[n]= \frac{1}{\sqrt2} [ \delta[n] + \delta[n-1] ]$ is called low pass reconstruction/synthesis filter&lt;/p&gt;

&lt;p&gt;$\displaystyle h_{r,h}[n]= \frac{1}{\sqrt2} [ \delta[n] - \delta[n-1] ]$ is called high pass reconstruction/synthesis filter&lt;/p&gt;

&lt;p&gt;$\displaystyle h_{r,l}[n]= \frac{1}{\sqrt2} [ 1, 1] $&lt;/p&gt;

&lt;p&gt;$\displaystyle h_{r,l}[n]= \frac{1}{\sqrt2} [ 1, -1] $&lt;/p&gt;

&lt;p&gt;Thus we can see that we can recover the decomposed signal using above synthesis process&lt;/p&gt;

&lt;h3 id=&quot;haar--filter-bank&quot;&gt;Haar  Filter Bank&lt;/h3&gt;

&lt;p&gt;The entire process can be graphically represented as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w7.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let us look at what the analysis and synthesis filters are doing&lt;/p&gt;

&lt;h3 id=&quot;analysis-of-decomposition-filter&quot;&gt;Analysis of Decomposition filter&lt;/h3&gt;

&lt;p&gt;The decomposition filter coefficients are given by&lt;/p&gt;

&lt;p&gt;$\displaystyle h_{d,l}[n]= \frac{1}{\sqrt2} [ 1 , 1 ] $ or $h_{l}[z] = \frac{1}{\sqrt2}[ 1 + z^{-1}]$&lt;/p&gt;

&lt;p&gt;$\displaystyle h_{d,u}[n]= \frac{1}{\sqrt2} [- 1 , 1 ] $ or $h_{u}[z] = \frac{1}{\sqrt2}[ 1- z^{-1}]$&lt;/p&gt;

&lt;p&gt;The Haar decomposition process can be graphically represented as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w8.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\displaystyle H_{0}[z] = \frac{1}{\sqrt2} [ 1 + z^{-1} ] $&lt;/p&gt;

&lt;p&gt;$\displaystyle H_{1}[z] = \frac{1}{\sqrt2} [ 1 - z^{-1} ] $&lt;/p&gt;

&lt;p&gt;Let us look at the frequency response of these filter
$\displaystyle H_{0}(w) = \frac{1}{\sqrt2} [ 1 + e^{-j\omega} ] $&lt;/p&gt;

&lt;p&gt;$\displaystyle H_{0}(w) = \sqrt{2}e^{-j\frac{\omega}{2}} cos(\frac{\omega}{2}) $&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w9.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The phase response of the filter is $-\frac{\omega}{2}$ .The filter has linear phase
response.&lt;/p&gt;

&lt;p&gt;We can see from magnitude response that filter has high gain near 0 frequencies while gain falls
to 0 at frequencies near $\omega= \pi$ .This acts like a crude low pass filter&lt;/p&gt;

&lt;p&gt;Let us look at the frequency response of these filter
$\displaystyle H_{1}(w) = \frac{1}{\sqrt2} [ 1 - e^{-j\omega} ] $&lt;/p&gt;

&lt;p&gt;$\displaystyle H_{1}(w) = \sqrt{2}e^{-j\frac{\omega}{2}+\frac{j\pi}{2}} sin(\frac{\omega}{2}) $&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w10.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see from magnitude response that filter has high gain near $\pi$ rads while gain falls
to 0 near frequencies of 0 .This acts like a crude high pass filter
and also has linear phase of $\frac{\pi}{2} - \frac{\omega}{2}$&lt;/p&gt;

&lt;p&gt;we can see that&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\displaystyle&lt;/td&gt;
      &lt;td&gt;H_{0}(w)&lt;/td&gt;
      &lt;td&gt;^2 +&lt;/td&gt;
      &lt;td&gt;H_{1}(w)&lt;/td&gt;
      &lt;td&gt;^2=1$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;we can say that the filters are power complementary&lt;/p&gt;

&lt;p&gt;$\displaystyle H_{0}(w) + H_{1}(w)=\sqrt{2}$&lt;/p&gt;

&lt;h3 id=&quot;analysis-of-reconstruction-filter&quot;&gt;Analysis of Reconstruction filter&lt;/h3&gt;

&lt;p&gt;The reconstruction filter coefficients are given by&lt;/p&gt;

&lt;p&gt;$\displaystyle h_{r,l}[n]= \frac{1}{\sqrt2} [ 1 , 1 ] $ or $h_{l}[z] = \frac{1}{\sqrt2}[ 1 + z^{-1}]$&lt;/p&gt;

&lt;p&gt;$\displaystyle  h_{r,u}[n]= \frac{1}{\sqrt2} [- 1 , 1 ] $ or $h_{u}[z] = \frac{1}{\sqrt2}[ 1- z^{-1}]$&lt;/p&gt;

&lt;p&gt;The Haar reconstruction process can be graphically represented as&lt;/p&gt;

&lt;p&gt;$\displaystyle  G_{0}[z] = \frac{1}{\sqrt2} [ 1 + z^{-1} ] $&lt;/p&gt;

&lt;p&gt;$\displaystyle  G_{1}[z] = \frac{1}{\sqrt2} [ 1 - z^{-1} ] $&lt;/p&gt;

&lt;p&gt;The synthesis filters have identical frequency reponse as corresponding analysis filters .
$G_{0}[z] $ represents a crude low pass filter while $G_{1}[z]$ represents a crude high pass filter&lt;/p&gt;

&lt;h3 id=&quot;realizable-two-band-filter-bank&quot;&gt;Realizable two band filter bank&lt;/h3&gt;

&lt;p&gt;One question to ask is that if we had used ideal filters would the decomposition and reconstruction still be possible.Instead of using crude filter banks why not use ideal filter banks&lt;/p&gt;

&lt;p&gt;We have seen in the article &lt;a href=&quot;http://pi19404.github.io/pyVision/page2/2014/10/11/DFT1/&quot;&gt;“Discrete Time Fourier Transform for Frequency Analysis”&lt;/a&gt;
that ideal low pass filter has the infinite impulse response,they are infinitely non causal hence cannot be realized in practice&lt;/p&gt;

&lt;p&gt;The filters $H_{0}[z],G_{0}[z]$ aspire to be ideal low pass filter with cutoff frequencies of $\frac{\pi}{2}$&lt;/p&gt;

&lt;p&gt;The filters $H_{1}[z],G_{1}[z]$ aspire to be ideal high pass filter with cutoff frequencies of $\frac{\pi}{2}$&lt;/p&gt;

&lt;p&gt;They cannot however be ideal filters to be practically realizable ,hence we would want them to be as close to the ideal frequency response as possbile.Have a frequency response close to ideal frequency response and impulse response that is realizable.&lt;/p&gt;

&lt;p&gt;The different wavelet families  essentially have the decomposition and reconstruction filters that are close to desired ideal frequency response.&lt;/p&gt;

&lt;p&gt;Another way to look at the filters is thought the scaling and wavelet functions&lt;/p&gt;

&lt;p&gt;$\phi(t)$ is basis of subspace $V_{m-1}$,$\phi(2t)$ defines basis of subspace $V_{m}$
we know that $V_{m-1} \subset V_{m}$,Thus basis of $V_{M-1}$ can be expressed in terms of basis 
of $V_{m}$&lt;/p&gt;

&lt;p&gt;$\displaystyle  \phi(t) = [\phi(2t) + \phi(2t -\frac{T}{2})]$&lt;/p&gt;

&lt;p&gt;In the earlier article on Haar wavelets we say that
$\displaystyle  \psi(t) = \phi(t)−\frac{1}{2}\phi(2t−\frac{T}{2}) $&lt;/p&gt;

&lt;p&gt;$\displaystyle  \psi(t) = \frac{1}{\sqrt{2}}[\phi(2t) - \phi(2t +1)]$&lt;/p&gt;

&lt;p&gt;This can be seen from the below figures
&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w11.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w12.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thus the filter coefficients are nothing but the projection coefficients of basis function $\phi(t)$ in $V_{m}$ on subspace $V_{m-1}$ and $W_{m}$ whose basis are defined by set ${\phi(2t)}$&lt;/p&gt;

&lt;p&gt;$\displaystyle  \phi(t) = \sum_{n} h[n] \phi(2t -n)$
$\displaystyle  \psi(t) = \sum_{n} g[n] \phi(2t -n)$&lt;/p&gt;

&lt;h3 id=&quot;frequency-analysis-of-wavelet-decomposition&quot;&gt;Frequency analysis of wavelet decomposition&lt;/h3&gt;

&lt;p&gt;The question we ask here is what does wavelet decomposition achieve&lt;/p&gt;

&lt;p&gt;we again look at the decomposition filter
The decomposition filter coefficients are given by&lt;/p&gt;

&lt;p&gt;$\displaystyle  h_{d,l}[n]= \frac{1}{\sqrt2} [ 1 , 1 ] $ or $h_{l}[z] = \frac{1}{\sqrt2}[ 1 + z^{-1}]$&lt;/p&gt;

&lt;p&gt;$\displaystyle  h_{d,u}[n]= \frac{1}{\sqrt2} [- 1 , 1 ] $ or $h_{u}[z] = \frac{1}{\sqrt2}[ 1- z^{-1}]$&lt;/p&gt;

&lt;p&gt;The Haar decomposition process can be graphically represented as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w8.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\displaystyle  H_{0}[z] = \frac{1}{\sqrt2} [ 1 + z^{-1} ] $&lt;/p&gt;

&lt;p&gt;$\displaystyle  H_{1}[z] = \frac{1}{\sqrt2} [ 1 - z^{-1} ] $&lt;/p&gt;

&lt;p&gt;The first step performed in the decomposition process is low pass and high pass filtering
Thus the we are essentially observing the low pass and high pass components idependently&lt;/p&gt;

&lt;p&gt;Now we perform downsampling
we known that scaling in the time domain leads to expansion in the frequency domain&lt;/p&gt;

&lt;p&gt;for example let us consider the signal&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w10.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we downsample the signal and observe its frequency response&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w11.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we can see that original signal had highest frequency content of 40Hz while the downsampled signal
has highest frequency content of 80Hz&lt;/p&gt;

&lt;p&gt;Thus by dowsampling we have zoomed in on the frequency domain ,while zoomed out in the time domain
Thus we have increased the frequency resolution while reduced the time resolution of the signal&lt;/p&gt;

&lt;p&gt;Thus by performing decomposition we are first dividing the frequency spectrum of signal into $0-\frac{pi}{2}$
and $\frac{\pi}{2}- \pi$ and zooming in each frequency interval by expanding the $0-\frac{pi}{2}$ interval  and $\frac{\pi}{2}- \pi$ interval to $0-pi $ respectively&lt;/p&gt;

&lt;p&gt;At the next stage of decompostion,we only consider the output of the low pass filter,which is projection of signal onto $V_{m-1}$ subspace.That is essentially we are looking at the interval $0-\frac{pi}{2}$ of the original signal and again performing the same operation&lt;/p&gt;

&lt;p&gt;They by splitting the interval into $0-\frac{pi}{4},\frac{pi}{4}-\frac{pi}{2}$ and so on.
Thus at each subspace projection $V_{m-1}$ we are essentially trying to zoom in the frequency domain of the signal while zooming out in the time domain .&lt;/p&gt;

&lt;p&gt;Thus we are having a closer look at the frequency spectrum of the signal at each stage of the decomposition.However in standard decompostion process we only zoom into the low pass spectrum of signal. However the same can also be done for high pass spectrum ,ie we want to zoom into the high pass spectrum of the signal.&lt;/p&gt;

&lt;p&gt;Another way to look at it is we are succesively dividing the frequency spectrum of the signal at each stage&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Stage 1  - $[0,\frac{\pi}{2}][\frac{\pi}{2},\pi]$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stage 2  - $[0,\frac{\pi}{4}][\frac{\pi}{4},\frac{\pi}{2}][\frac{\pi}{2},\pi]$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stage 3  - $[0,\frac{\pi}{8}][\frac{\pi}{8},\frac{\pi}{4}][\frac{\pi}{4},\frac{\pi}{2}][\frac{\pi}{2},\pi]$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At each stage we are looking at smaller region in frequency spectrum and conversely large region in the time domain&lt;/p&gt;

&lt;p&gt;Lets look at the following signal and its decomposition&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w12.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lets look at the frequency response&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w13.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see piece-wise constant approximation as well as frequency response&lt;/p&gt;

&lt;p&gt;The first subplot is the output of the low pass filter or the approximation coefficient of last stage of decomposition and remaining plots are for detail coefficients at each stage of decomposition&lt;/p&gt;

&lt;p&gt;we can see that since the filter being used is not ideal low/high pass in decomposition.
even though the signal has no high frequency components,the high pass decomposition filter
as produced a response.&lt;/p&gt;

&lt;p&gt;We can also observe the zooming effect in the frequency response&lt;/p&gt;

&lt;p&gt;Below is a plot for sinc squared signal&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w14.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w15.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The same results can be observed for large stages of decomposition
&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w16.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/13/w17.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;
&lt;p&gt;The code for generating all the results and plots can be found the &lt;a href=&quot;http://pi19404.github.io/pyVision/&quot;&gt;pyVision&lt;/a&gt; github repository&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;wavelet2.py&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;running-the-code&quot;&gt;running the code&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;The code uses pyWavelets python package.This needs to be installed before running the code.&lt;/li&gt;
  &lt;li&gt;To run the code,download the entire pyVision repository.&lt;/li&gt;
  &lt;li&gt;Go to the pySignalProc/tutorials directory and execute the wavelet2.py code&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Remote Accessing Raspberry PI</title>
   <link href="http://localhost:4000/raspberry%20pi/2014/11/21/13/"/>
   <updated>2014-11-21T00:00:00+00:00</updated>
   <id>http://localhost:4000/raspberry%20pi/2014/11/21/13</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In this article we will look at accessing Raspberry PI remotely&lt;/p&gt;

&lt;h3 id=&quot;testing-ssh&quot;&gt;Testing SSH&lt;/h3&gt;
&lt;p&gt;openssh-server is insalled by default on raspbian installation
ssh client is installed by default on most linux distribution&lt;/p&gt;

&lt;p&gt;test the ssh connection&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;ssh -X  192.168.1.4 -l pi&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;where 192.168.1.4 is IP Address of Raspberry PI&lt;/p&gt;

&lt;p&gt;This will give remote terminal access to Raspberry PI&lt;/p&gt;

&lt;h3 id=&quot;graphical-access&quot;&gt;Graphical Access&lt;/h3&gt;
&lt;p&gt;X is an architecture-independent system for remote graphical user interfaces and input device capabilities&lt;/p&gt;

&lt;p&gt;X uses a client–server model: an X server communicates with various client programs. The server accepts requests for graphical output (windows) and sends back user input (from keyboard, mouse, or touchscreen). Each person using a networked terminal has the ability to interact with the display with any type of user input device.&lt;/p&gt;

&lt;p&gt;X provides display and I/O services to applications, so it is a server; applications use these services, thus they are clients.The X client sends commands to the X Server about what kind of things to put on the screen and the X server does it&lt;/p&gt;

&lt;p&gt;The X server will run on the host machine while X server client will run on the Raspberry PI
Let us consider a hostmachine running Ubuntu12.04 and Raspberry PI running raspbian OS.&lt;/p&gt;

&lt;p&gt;The X window system introduces a abstraction  for graphical display and input devices .One can run a program on the X client and have all the graphical bit appear on the screen of the X Server somewhere else on the network. As long as X window communication protocols are followed display for any  client can be rendered on any server irrespective of whether host/client machine run the same operating systems,have same architecture etc&lt;/p&gt;

&lt;p&gt;This is particularly useful because this removes a large amount of processor demand from the Raspberry PI for rendering.Also Input devices need not be connected to the PI directly as input device commands are passed for host X Server to the X Client machine.&lt;/p&gt;

&lt;p&gt;The most common X Window standard used presently is called X11.&lt;/p&gt;

&lt;p&gt;The X11 Server needs to be installed on the server (Ubuntu12.04 ) while X11 client needs to be installed on the Raspberry PI .&lt;/p&gt;

&lt;p&gt;By default X11 server and clients are installed on most linux and Raspberry PI compatible linux distributions.&lt;/p&gt;

&lt;h3 id=&quot;x11-forwarding-for-gui-application-over-ssh&quot;&gt;X11 Forwarding for GUI Application over SSH&lt;/h3&gt;
&lt;p&gt;The first method is to use X switch&lt;/p&gt;

&lt;p&gt;To enable the use of X commands forwarding,we need to enable the feature on Raspberry PI&lt;/p&gt;

&lt;p&gt;Open sshd config file with a text editor.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$ sudo vi /etc/ssh/sshd_config&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Add the following line in the bottom line of the configuration file.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;X11Forwarding yes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Restart sshd&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$ sudo /etc/init.d/ssh restart&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;connect to Raspberry Pi over SSH with “-X” option.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;ssh -X 192.168.1.4 -l pi&lt;br /&gt;
leafpad&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The command uses the X switch to tell ssh to send the X commands to the X server on your host.&lt;/p&gt;

&lt;p&gt;The window corresponding to the application &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;leafpad&lt;/code&gt; will be rendered on Ubuntu12.04
Thus we can access graphicall application on Raspberry PI remotely&lt;/p&gt;

&lt;h3 id=&quot;x11-forwarding-for-desktop-over-ssh&quot;&gt;X11 Forwarding for Desktop over SSH&lt;/h3&gt;
&lt;p&gt;Another option is to get the complete desktop instead of Window for specific application&lt;/p&gt;

&lt;p&gt;On ubuntu the desktop session is running on virtual terminal 7 which can be accessed by typing 
CTRL+ALT+F7&lt;/p&gt;

&lt;p&gt;we can run the remote RPi desktop in the a virtual terminal  via X11 forwarding
change to root user.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$ sudo su&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The xinit program allows a user to manually start an X display server.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;$ xinit – :1 &amp;amp;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This will start a X display server session on virtual termnial 8 on Ubuntu 12.04 OS which can be accessed by  typing CTRL+ALT+F8 .You can switch back to the original terminal  by pressing CTRL+ALT+F7.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$DISPLAY=:1 ssh -X pi@192.168.2.5 lxsession&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After switching to virtual terminal 8, the following command will launch the Raspberry PI desktop remotely.&lt;/p&gt;

&lt;p&gt;You can move between first and second virtual terminals by pressing CTRL+ALT+F7 or CTRL+ALT+F8.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Haar Wavelets</title>
   <link href="http://localhost:4000/signal%20processing/2014/11/20/12/"/>
   <updated>2014-11-20T00:00:00+00:00</updated>
   <id>http://localhost:4000/signal%20processing/2014/11/20/12</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In this article we will look at basic introduction to wavelets using Haar wavelets&lt;/p&gt;

&lt;h3 id=&quot;haar-wavelets&quot;&gt;Haar Wavelets&lt;/h3&gt;

&lt;p&gt;Lets take the consider a family of wavelets called Haar Wavelets.The Haar wavelet
are simplest form of wavelets.&lt;/p&gt;

&lt;p&gt;The idea of wavelets is to express  contininuous singal  linear combination of wavelet function.
Let is first see what Haar wavelets look like&lt;/p&gt;

&lt;h3 id=&quot;piece-wise-constant-approximation&quot;&gt;Piece-wise constant approximation&lt;/h3&gt;

&lt;p&gt;The idea of piecewise constant approximation is to represent signal in duration of interval T by a constant.
One of measures that can be used is to represent the signal using mean value of signal over the interval.&lt;/p&gt;

&lt;p&gt;Let consider a continuous time signal $x(t)$ and look at piecewise constant approximation of the signal.&lt;/p&gt;

&lt;p&gt;We will perform picewise contant approximation of signal over interval length of $T=8$
$\displaystyle x_{T}(t)= \frac{1}{T}\int_{T} x(t) dt $&lt;/p&gt;

&lt;p&gt;We can see this in the below figure
&lt;img src=&quot;http://pi19404.github.io/pyVision/images/12/wave2.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let $\phi(t)$ represent a unit step function,The approximated signal can also be expressed as
$x_{a}(t) = \sum_{i} x_{T}(t) \phi(t - iT)$&lt;/p&gt;

&lt;p&gt;$\psi(t - iT)$ represents constant integral translations of the unit step function&lt;/p&gt;

&lt;p&gt;Thus any arbitrary function can be expressed can be expressed as linear combination piecewise constant integer translates of function $\phi(t)$&lt;/p&gt;

&lt;h3 id=&quot;basic-function&quot;&gt;Basic function&lt;/h3&gt;
&lt;p&gt;The set of function ${\phi(t)}$ and its interger translates are said to span a linear space $\mathcal{F}$ if any function in the space $\mathcal{F}$ can be expressed as linear combination of this set.&lt;/p&gt;

&lt;p&gt;Let $V_{o}$ represent such a space where any function can be expressed as linear combination of ${\phi(t)}$ and its interger translates.Any function in $V_{o}$ can be expressed as $\sum_{n \in \mathcal{Z}} C_{n} \phi(t- n)$&lt;/p&gt;

&lt;p&gt;$V_{0}$ is a space of piecewise constant function on  interval of size $T$&lt;/p&gt;

&lt;p&gt;The set of function ${\phi(t)}$ are called the basis of space $\mathcal{F}$&lt;/p&gt;

&lt;p&gt;we can also see that $\int \phi(t-kT) \phi(t-iT)=0,i\neq k$ and $\int \phi(t-kT) \phi(t-iT)=1,i= k$&lt;/p&gt;

&lt;p&gt;The basics also forms a orthonormal set&lt;/p&gt;

&lt;h3 id=&quot;subspaces-of-piecewise-constant-functions&quot;&gt;Subspaces of piecewise constant functions&lt;/h3&gt;

&lt;p&gt;Now let us consider piecewise constant approximation over interval length of $T/2=4$ where $s=1/2$&lt;/p&gt;

&lt;p&gt;This can be seen in below figure&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/12/wave3.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$x_{s,a}(t) = \sum_{i} x_{T}(t) \phi_{s}(t - iT)$&lt;/p&gt;

&lt;p&gt;and $\phi_{s}(t) = \phi(\frac{t}{s})$&lt;/p&gt;

&lt;p&gt;$x_{s,a}(t) = \sum_{i} x_{T}(t) \phi(\frac{t - iT}{s})$&lt;/p&gt;

&lt;p&gt;Thus function is expressed as linear combination of integral translates of scaled version of function $\phi(t)$
Let   $V_{1}$ denote the linear space spanned by $\phi(\frac{t}{s})=\phi(2t)$ and its integer translates&lt;/p&gt;

&lt;p&gt;$x_{s=2,a}(t) = \sum_{n} x_{T}(t) \phi(\frac{t}{s} - n)$&lt;/p&gt;

&lt;p&gt;Any  space $V_{m}(t)$ can be similarily constructed&lt;/p&gt;

&lt;p&gt;$V_{m} = span_{n \in \mathcal{Z}} { \phi(\frac{t}{s} - n)}$&lt;/p&gt;

&lt;p&gt;Lets take a signal $x(t)$ and its projection in subspace $V_{0}$ and $V_{1}$
in the present example $T=8$&lt;/p&gt;

&lt;p&gt;We can see as we take smaller intervals or projection of subspace $V_{1},V_{2},\ldots V_{m}$ we get better approximation of the signal&lt;/p&gt;

&lt;p&gt;we plot both the projection and red region indicates the difference between the projection.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/12/wave5.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we can see that the mean square error reduces as we take projection of higher subspaces 
As we take smaller and smaller intervals,we can better approximate the continuous time signal&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/12/wave6.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we can see that X_{T}(t) is a sequence and  equivalence between $x(t)$ and sequence $X_{T}(t)$
we will write $X_{T}(t)$ as $x[n]$ which are the components of projection along orthonornal basis
${ \phi(t)}$&lt;/p&gt;

&lt;p&gt;$x_{s,a}(t) = \sum_{n} x[n]\phi(\frac{t}{s} - n)$&lt;/p&gt;

&lt;p&gt;Thus $x(t) \in \mathcal{L}_{2}(\mathcal{R})$ implies $x[n] \in l_{p}(\mathcal{Z})$
where $l_{p}(\mathcal{Z})$ is linear space of set of integers&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Since $ \int&lt;/td&gt;
      &lt;td&gt;x(t)&lt;/td&gt;
      &lt;td&gt;^2  dt &amp;lt; \infty $ it also implies $\sum_{n}&lt;/td&gt;
      &lt;td&gt;x[n]&lt;/td&gt;
      &lt;td&gt;^2 &amp;lt; \infty $&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;ladder-of-subspaces&quot;&gt;Ladder of subspaces&lt;/h3&gt;
&lt;p&gt;we can also see that $x_{s=1,a}(t)$ can also be expressed as a linear combination of $\phi(\frac{t}{s} )$
and its integral translates.&lt;/p&gt;

&lt;p&gt;Thus linear space $V_{0}$ is also spanned by $\phi(\frac{t}{s} )$ and its integer translates.
It can also been seen that $V_{o}$ is a subspace of $V_{1}$.&lt;/p&gt;

&lt;p&gt;where $s=2^{M}$ for case we take didactic scale of $T,T/2,T/4 \ldots $&lt;/p&gt;

&lt;p&gt;$V_{m} = span_{n \in \mathcal{Z}} { \phi(st - n)}$&lt;/p&gt;

&lt;p&gt;and $\ldots  \subset V_{-1} \subset V_{o} \subset V_{1} \subset V_{2} \ldots \subset V_{m} \ldots   \mathcal{L}_{2}(\mathcal{R})$&lt;/p&gt;

&lt;p&gt;Thus there exits a ladder of subspaces which allow us to represent the signal using appriopriate basis at desired resolution&lt;/p&gt;

&lt;p&gt;In wavelet terminalogy $\phi(t)$ is called a scaling function and basis of ladder of subspaces are defined in terms of scaled version of scaling function&lt;/p&gt;

&lt;h3 id=&quot;wavelet-function&quot;&gt;Wavelet function&lt;/h3&gt;

&lt;p&gt;Let us consider the subspaces $V_{0}$ and $V_{1}$ and observe the difference between the signals approximated by these two subspaces&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/12/wave7.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can observe a pattern here ,The shape of the difference function in all the intervals is the same&lt;/p&gt;

&lt;p&gt;Let us denote the subspace spanned by this difference as $W_{0}$&lt;/p&gt;

&lt;p&gt;$V_{1} = V_{0} + W_{0}$&lt;/p&gt;

&lt;p&gt;Let’s look at the reason why this is so&lt;/p&gt;

&lt;p&gt;$x_{1,a}(t) = \sum_{n} x[n]\phi({t} - n)$&lt;/p&gt;

&lt;p&gt;$x_{2,a}(t) = \sum_{n} z[n]\phi(2t - n)$&lt;/p&gt;

&lt;p&gt;where $\phi(t)$ is defined over interval T and $\phi(2t)$ is defined over interval of T/2&lt;/p&gt;

&lt;p&gt;Let us look at single interval over T and projection $V_{0},V_{1}$&lt;/p&gt;

&lt;p&gt;we can express the these measures as&lt;/p&gt;

&lt;p&gt;$\displaystyle x_{1,\frac{T}{2}}(t)=  \frac{2}{T}\int_{0}^{\frac{T}{2}} x(t)dt =\frac{2}{T}\int x(t) \phi(2t)dt$&lt;/p&gt;

&lt;p&gt;$\displaystyle x_{2,\frac{T}{2}}(t)=  \frac{2}{T}\int_{\frac{T}{2}}^{T} x(t)dt =\frac{2}{T}\int x(t) \phi(2t-\frac{T}{2})$&lt;/p&gt;

&lt;p&gt;$\phi(st) = 1 $ for $t\in [0,\frac{T}{s}]$&lt;/p&gt;

&lt;p&gt;Now we need to relate $x_{T}(t)$ piecewise constant approximation over the entire duration T with measures $x_{1,\frac{T}{2}}(t),x_{2,\frac{T}{2}}(t)$ which are piecewise constant approximation over duration $\frac{T}{2}$ of the signal.&lt;/p&gt;

&lt;p&gt;$\displaystyle x_{T}(t) = \frac{1}{2} [ x_{1,\frac{T}{2}}(t) +x_{2,\frac{T}{2}}(t) ]$&lt;/p&gt;

&lt;p&gt;$\displaystyle x_{T}(t) = \frac{1}{2} \sum_{i} x_{i,\frac{T}{2}}(t) $&lt;/p&gt;

&lt;p&gt;$\displaystyle x_{T}(t) = \frac{1}{T} [ \int x(t)\phi(2t)dt +\int x(t)\phi(2t-\frac{T}{2})dt ]$&lt;/p&gt;

&lt;p&gt;$\displaystyle x_{T}(t) = \frac{1}{2^N} \sum_{i\in 2^N} x_{i,\frac{T}{2^N}}(t) $&lt;/p&gt;

&lt;p&gt;Let us look at difference between approximation in the projection on $V_0$ and $V_{1}$ over interval of $[0,\frac{T}{2}$ and $[\frac{T}{2},T]$&lt;/p&gt;

&lt;p&gt;$e_{0}(t) = x_{1,\frac{T}{2}}(t) - x_{1,T}(t) $&lt;/p&gt;

&lt;p&gt;$e_{0}(t) =\frac{2}{T}\int_{0}^{\frac{T}{2}} x(t) dt -\frac{1}{T} \int_{0}^{T} x(t) dt$&lt;/p&gt;

&lt;p&gt;$e_{0}(t) =\frac{1}{T}\int_{0}^{\frac{T}{2}} x(t) dt -\frac{1}{T} \int_{\frac{T}{2}}^{T} x(t) dt$&lt;/p&gt;

&lt;p&gt;Similarily 
$e_{1}(t) =\frac{2}{T}\int_{\frac{T}{2}}^{T} x(t) dt -\frac{1}{T} \int_{0}^{T} x(t) dt$&lt;/p&gt;

&lt;p&gt;$e_{1}(t) =\frac{1}{T}\int_{\frac{T}{2}}^{T} x(t) dt -\frac{1}{T} \int_{0}^{\frac{T}{2}} x(t) dt$&lt;/p&gt;

&lt;p&gt;the error over interval of T can be written as&lt;/p&gt;

&lt;p&gt;$e(t) = A \phi(2t) - A \phi(2t -\frac{T}{2})$&lt;/p&gt;

&lt;p&gt;$\psi(t) = \phi(2t ) - \phi (2t -\frac{T}{2})$&lt;/p&gt;

&lt;p&gt;$e(t) = A \psi(t)$&lt;/p&gt;

&lt;p&gt;This is exactly the shape of function we observed in each interval&lt;/p&gt;

&lt;p&gt;This is called as wavelet function.&lt;/p&gt;

&lt;p&gt;The wavelet function for the basis of subspace $W_{0}$.
we can also see that basis  $\psi(t)$ can be expressed as linear combination of $\phi(t)$.&lt;/p&gt;

&lt;p&gt;Thus $W_{0} \subset V_{1}$ and Thus $V_{0} \subset V_{1}$&lt;/p&gt;

&lt;p&gt;The coefficients of projection on $V_{1}$ subspace can be computed using the wavelet function&lt;/p&gt;

&lt;p&gt;$V_{1} = V_{0} + W_{0}$&lt;/p&gt;

&lt;p&gt;$V_{1} = V_{-1} + W_{-1} +  W_{0}$&lt;/p&gt;

&lt;p&gt;$V_{1} = V_{-2} +W_{-2} + W_{-1} +  W_{0}$&lt;/p&gt;

&lt;p&gt;$V_{0} $ is subspace with $\phi(t)$ defined over unit interval.&lt;/p&gt;

&lt;h3 id=&quot;haar-wavelet&quot;&gt;Haar Wavelet&lt;/h3&gt;

&lt;p&gt;The wavelet and scaling function of Haar Wavelet is as below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/12/wave1.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;They can be expressed as
$ [ \frac{1}{\sqrt2} ,\frac{1}{\sqrt2} ] $
$ [\frac{1}{\sqrt2},-\frac{1}{\sqrt2} ] $&lt;/p&gt;

&lt;h3 id=&quot;signal-decomposition-using-wavelets&quot;&gt;Signal Decomposition Using Wavelets&lt;/h3&gt;

&lt;p&gt;Where $V_{1}$ can be considered as sampled sequence,Now we can express this signal in terms of scaling and wavelet functions.&lt;/p&gt;

&lt;p&gt;Let $x[n]$ represent the discrete time sequence .This can be considered as projection coefficients on the $V_{0}$ subspace.&lt;/p&gt;

&lt;p&gt;Thus we can represent the projection on $V_{-1}$ subspace as 
$y[n]=\frac{1}{\sqrt2}\frac{1}{2} x[2n]+x[2n+1]$&lt;/p&gt;

&lt;p&gt;we can represent the project on the $W_{0}$ subspace as 
$y[n]=\frac{1}{\sqrt2} \frac{1}{2} x[2n]-x[2n+1]$&lt;/p&gt;

&lt;p&gt;let us consider a dummy input sequence $x=[1,1,1,1,1,1]$&lt;/p&gt;

&lt;p&gt;$V_{-1}=[ 1.41421356 , 1.41421356,  1.41421356]$&lt;/p&gt;

&lt;p&gt;$W_{0}=[ 0.,  0,  0.]$&lt;/p&gt;

&lt;p&gt;Now let us look at projection on subspace $V_{-2},W_{-1}$&lt;/p&gt;

&lt;p&gt;$V_{-2}=[ 2.,  2.]$&lt;/p&gt;

&lt;p&gt;$W_{-1}=[ 0. , 0.]$&lt;/p&gt;

&lt;p&gt;The process of expressing a signal in terms of the wavelet basis is called signal decomposition&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;

&lt;p&gt;The code for generating the plot can be found in the github pyVision repository&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;pyVision/pySignalProc/tutorial/wavelet1.py&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Download the entire repository and run the wavelet1.py files&lt;/p&gt;

&lt;p&gt;The code uses &lt;a href=&quot;http://www.pybytes.com/pywavelets/&quot;&gt;pyWavelets&lt;/a&gt; python package.This needs to be installed before running the code.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Shperical Wave Simulator</title>
   <link href="http://localhost:4000/signal%20processing/2014/11/03/WaveSimulator/"/>
   <updated>2014-11-03T00:00:00+00:00</updated>
   <id>http://localhost:4000/signal%20processing/2014/11/03/WaveSimulator</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In this article we will look at how to simulate a spherical wave produced from point source .&lt;/p&gt;

&lt;h3 id=&quot;wave-propagation-and-spreading-loss&quot;&gt;Wave Propagation and Spreading Loss&lt;/h3&gt;

&lt;p&gt;According to wave theory a point source produces a spherical wave in an ideal isotropic (uniform) medium such as air.&lt;/p&gt;

&lt;p&gt;If wave propagates at velocity $t$,time taken for the wave to propagate from point A to B is given by&lt;/p&gt;

&lt;p&gt;$\displaystyle \tau=\frac{d(A,B)}{v}$&lt;/p&gt;

&lt;p&gt;If $x(t)$ is signal associated with the source,then the signal y(t) at the listening point can be expressed as&lt;/p&gt;

&lt;p&gt;$\displaystyle y(t) = \alpha * x(t - \tau)$&lt;/p&gt;

&lt;p&gt;Thus these is a phase delay between the signal observed at source and destination locations.&lt;/p&gt;

&lt;p&gt;This can be easily simulated by delay line.&lt;/p&gt;

&lt;p&gt;In digital domain the analog wave is sampled at frquency $F_{s}$.Thus a time duration of $\tau$ sec will correspond to $\tau*F_{s}$ samples.&lt;/p&gt;

&lt;p&gt;Thus signal observed at listening point is time delay version of signal associated with the source&lt;/p&gt;

&lt;p&gt;$ \displaystyle y[n] = \alpha * x [ n- \tau N] = \alpha x [ n - \beta]$&lt;/p&gt;

&lt;p&gt;where $\beta$ can be fractional.&lt;/p&gt;

&lt;p&gt;In the previous article `` we saw how to implement fractional delays.The same function will be used here to introduce a delay corresponding to wave propagation time between points A and B.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

        def propagation_delay(self,source):
            &quot;&quot;&quot; the function completes the sample delay of source signal 
            
            Parameters
            -----------
            source - numpy array ,shape (Nxdimension)
                     source location            
            
            Returns
            ----------
            diff - numpy array ,shape (Nx1)
                   distance of receiver to source
            
            &quot;&quot;&quot;
            diff1=[]
            for i in range(len(source)):
                diff1.append(Utils.distance(self.position,source[i]))
            diff1=np.array(diff1)
            return diff1

        def sample_delay(self,source):
            &quot;&quot;&quot;&quot; computes the sample delay of source signal observed 
            at receiver 
            
            Parameters
            -----------
            source - numpy array ,shape (Nxdimension)
                     source location            
            
            Returns
            ----------
            delay - numpy array ,shape (Nx1)
                    sample delay corresponding to propagationtime        
            
            &quot;&quot;&quot;
            dist=propagation_delay(source)
            delay=dist*self.Fs/self.velocity
            return delay
            
  &lt;/pre&gt;

&lt;p&gt;Wave energy is conserved as it propagates through the air. In a spherical pressure wave of radius $ r$ , the energy of the wavefront is spread out over the spherical surface area $ 4\pi r^2$ . Therefore, the energy per unit area of an expanding spherical pressure wave decreases as $ 1/r^2$ . This is called spherical spreading loss.&lt;/p&gt;

&lt;p&gt;Energy is proportional to amplitude squared, an inverse square law for energy translates to a $ 1/r$ decay law for amplitude.&lt;/p&gt;

&lt;p&gt;Thus the amplitude of wave reduces by a factor of $1/r$ for a traversal distance of $r$&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

        def propagation_loss(self,source):
            &quot;&quot;&quot; the function completes the spreading loss of signal 
            from source to present location
            
            Parameters
            -----------
            source - numpy array ,shape (Nxdimension)
                     source location            
            
            Returns
            ----------
            loss - numpy array ,shape (Nx1)
                   spreading propagation loss
            
            &quot;&quot;&quot;
            dist=self.propagation_delay(source)
            loss=1.0/dist
            return loss
            
&lt;/pre&gt;

&lt;p&gt;Thus the wave at receiver can be simulated wrt to wave associated with source by introducing a time delay and attenucation corresponding to distance travelled&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

        def run(source,signal):
            &quot;&quot;&quot;&quot; simulates the signal at the wave receiver
            
            Parameters
            -----------
            source - numpy array ,shape (Nxdimension)
                     source location       
                     
            signal - numpy array,shape (1xN)
                     signal associated with source
            
            Returns
            ----------
            delay - numpy array ,shape (Nx1)
                    wave at the receiver due to multiple sources        
            
            &quot;&quot;&quot;            
            
            delay=self.sample_delay(source)
            loss=self.propagation_loss(source)
            signal=Utils.fdelay(signal,delay)
            signal=signal1*loss*absorbtion            
            
               
            for i in range(len(source)):
                signal[i]=Utils.addNoise(signal[i],self.noise)        
                seed=int(np.random.uniform(0,1)*10000);   
                np.random.seed(seed)                 
                
            signal=np.sum(signal)   
            
            
            if self.phase_shift!=0:
                signal=Utils.phase_sift(signal,self.phase_shift)
                   
            return signal
&lt;/pre&gt;

&lt;p&gt;For a modular implementation we describe &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WaveSource&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WaveSink&lt;/code&gt; classes
that encapsulate the properties of wave source and receiver.&lt;/p&gt;

&lt;p&gt;The WaveSource contains simply the location of source,propagation properties of wave
and source signal&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

class WaveSource(object):
    def __init__(self,position,attenuation,phase,velocity,carrier,Fs):
        self.position=np.array(position)
        self.attenuation=attenuation
        self.phase=phase
        self.carrier=carrier
        
        #generate default modulated sinusiodal waveform
        to=100;
        t1=100+(100*Fs/3000);
        l=1000*Fs/3000;       
        
        waveform=4*Utils.sinepulse(l,to,t1,carrier,Fs)
        
        self.signal=waveform

&quot;&quot;&quot; The Wave souce is initialized as follows &quot;&quot;&quot;'
    
    #souce signal properties
    Fs=48000 
    carrier=1000
    velocity=300
    
    #souce location
    source=[[5,3],[5,-3],[-5,3],[5,17],[15,3]]
    phase =[0,math.pi,math.pi,math.pi,math.pi]
    attenuation=[1,0.5,0.5,0.5,0.5]
    
    sources=[]
    for i in range(len(source)):
        s=WaveSource(source[i],attenuation[i],phase[i],velocity,carrier,Fs)
        sources.append(s)
        
&lt;/pre&gt;

&lt;p&gt;The WaveSink compute the propagation time.sample delay,propagation loss and performs computation on each WaveSink object to generate a wave output corresponding to each source.Then it adds all outputs dues to individual sources to get a combined output at the receiver due to all the sources&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

    receivers=[10,10]          
    sink=WaveSink(receivers,Fs,velocity,0,1.0,0.001)
    signals=sink.run(sources)    
    
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://pi19404.github.io/pyVision/images/image1.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;

&lt;p&gt;The code for the same can be found in file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WaveSink.py&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WaveSouce.py&lt;/code&gt; in pyVision github repository&lt;/p&gt;

&lt;p&gt;To run the code clone the entire &lt;a href=&quot;https://github.com/pi19404/pyVision&quot;&gt;pyVision&lt;/a&gt; github repository
The code to generate the waveform can be executed by going to the pySignalProc directory of the repository and executing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WaveSink.py&lt;/code&gt; file&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pi19404/pyVision&quot;&gt;Github Repo Link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Links to files &lt;a href=&quot;https://github.com/pi19404/pyVision/blob/master/pySignalProc/WaveSink.py&quot;&gt;WaveSink.py&lt;/a&gt; and &lt;a href=&quot;https://github.com/pi19404/pyVision/blob/master/pySignalProc/WaveSource.py&quot;&gt;WaveSource.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Fraction Delays using Linear Interpolation and Resampling</title>
   <link href="http://localhost:4000/signal%20processing/2014/11/01/FractionalDelay/"/>
   <updated>2014-11-01T00:00:00+00:00</updated>
   <id>http://localhost:4000/signal%20processing/2014/11/01/FractionalDelay</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In this article we look at implementing fractional delays using Linear Interpolation and Resampling Techniques&lt;/p&gt;

&lt;p&gt;A integral delay of $M$ is obtained by a delay line of length $M$.A factional  delay is implemented cascading the integral part of delay with a block which can approximate a constant phase delay equal to fractional part of $m$&lt;/p&gt;

&lt;p&gt;Fractional delay filters receive a sequential stream of input samples and produce a corresponding sequential stream of interpolated output values.&lt;/p&gt;

&lt;p&gt;The most intuitive way of obtaining fractional delay is interpolation	.&lt;/p&gt;

&lt;p&gt;For an ideal fractional-delay filter, the frequency response should be equal to that of an ideal delay&lt;/p&gt;

&lt;p&gt;$\displaystyle H^\ast(e^{j\omega}) = e^{-j\omega\Delta}$&lt;/p&gt;

&lt;p&gt;where  $ \Delta = N+ \eta$ denotes the total desired delay of the filter
Thus, the ideal desired frequency response is a linear phase term corresponding to a delay of $ \Delta$ samples.&lt;/p&gt;

&lt;h3 id=&quot;linear-interpolation&quot;&gt;Linear Interpolation&lt;/h3&gt;
&lt;p&gt;Linear interpolation works by effectively drawing a straight line between two neighboring samples and returning the appropriate point along that line.&lt;/p&gt;

&lt;p&gt;More specifically, let $ \eta$ be a number between 0 and 1 which represents how far we want to interpolate a signal $ y$ between time $ n$ and time $ n+1$ . Then we can define the linearly interpolated value  $ \hat y(n+\eta)$ as follows:&lt;/p&gt;

&lt;p&gt;$\displaystyle \hat y(n+\eta) = (1-\eta) \cdot y(n) + \eta \cdot y(n+1) $&lt;/p&gt;

&lt;p&gt;$\displaystyle \hat y(n+\eta) = y(n) + \eta\cdot\left[y(n+1) - y(n)\right].$&lt;/p&gt;

&lt;p&gt;Thus, the computational complexity of linear interpolation is one multiply and two additions per sample of output.&lt;/p&gt;

&lt;p&gt;In case of delay filter $\eta$ is the fractional part of the delay.Thus we pass the sequence throught a filter&lt;/p&gt;

&lt;p&gt;for example for a delay of 1/4
$\displaystyle {\hat y}\left(n-\frac{1}{4}\right)
\;=\;\frac{3}{4} \cdot y(n) + \frac{1}{4}\cdot y(n-1) $&lt;/p&gt;

&lt;p&gt;The python code for implementing fractional delay by interpolation can be found below&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;


def convolution(signal,h):
    &quot;&quot;&quot; function that performs linear convolution &quot;&quot;&quot;
    output=scipy.convolve(signal,h,&quot;same&quot;)    
    return output
    
    
def fdelay(signal,N):
    &quot;&quot;&quot; function introduces a fractional delay of N samples 
    
    Parameters
    -----------
    signal : numpy-array,
             The input signal
             
    N      : factional
             delay
        
    Returns
    --------
    out : numpy-array
          delayed signal
    
    &quot;&quot;&quot;    
    f,i=math.modf(N)
    #perform integral delay
    signal=delay(signal,i)

    #perform linear interpolation for fractional delay    
    output=convolution(signal,[f,i-f])

    return output

def delay(signal,N):
    &quot;&quot;&quot; function introduces a circular delay of N samples 
    
    Parameters
    -----------
    signal : numpy-array,
             The input signal
             
    N      : integer
             delay
        
    Returns
    --------
    out : numpy-array
          delayed signal
    
    &quot;&quot;&quot;
    if N==0:
        return signal;
        
    if N &amp;gt;= len(signal):
        N=N-len(signal)
    
    if N &amp;lt;0:
        N=N+len(signal)
    
   
    d=signal[len(signal)-N:len(signal)];#numpy.zeros((1,N+1));    
    signal1=numpy.append(d,signal[0:len(signal)-N])
    return signal1;
    
    &lt;/pre&gt;

&lt;h3 id=&quot;upsampling-technique&quot;&gt;Upsampling Technique&lt;/h3&gt;

&lt;p&gt;Let us assume we can express the fractional delay as rational number $\frac{M}{N}$&lt;/p&gt;

&lt;p&gt;The steps to introduce fractional delay are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;upsample the sequence by a factor $N$ which simple inserts N zeros between adjacent samples.&lt;/li&gt;
  &lt;li&gt;interpolate the zero values using low pass filter&lt;/li&gt;
  &lt;li&gt;Delay the signal by $M$ samples&lt;/li&gt;
  &lt;li&gt;downsample by a factor $N$&lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

def fdelay(signal,N,mode=&quot;upsample&quot;):
    &quot;&quot;&quot; function introduces a fractional delay of N samples 
    
    Parameters
    -----------
    signal : numpy-array,
             The input signal
             
    N      : factional
             delay
        
    mode   : &quot;linear &quot; - linear interpolation
             &quot;upsample &quot; - upsampling technique
    Returns
    --------
    out : numpy-array
          delayed signal
    
    &quot;&quot;&quot;    
 
    if mode==&quot;linear&quot;:      
        f,i=math.modf(N)
        #perform integral delay
        signal=delay(signal,i)           
        #perform linear interpolation for fractional delay    
        output=convolution(signal,[f,i-f])
    if mode==&quot;upsample&quot;:
        N=math.ceil(N*100)/100
        #get rational approximation
        result=fractions.Fraction(N).limit_denominator(20)
        num=result.numerator;
        den=result.denominator
        #upsample the signal and interpolate

        out1=scipy.signal.resample(signal,den*len(signal))
        #delay the signal
        out1=delay(out1,int(num))        
        #downsample the signal

        out1=scipy.signal.resample(out1,len(signal))
        
        output=out1
         
    return output

&lt;/pre&gt;

&lt;p&gt;The Rational number has been chosse such that denominator is limited to 20 ,so that we are not
upscaling by a large factor.&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;
&lt;p&gt;The code for the same can be found in the pyVision github repository in files&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pi19404/pyVision/blob/master/pySignalProc/Utils.py&quot;&gt;Utils.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fdelay&lt;/code&gt; implements fractional delay while function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delay&lt;/code&gt; implements integer delay.
The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mode&lt;/code&gt; parameter of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fdelay&lt;/code&gt; function specified which method to use to perform fractional delay operations.
presently it support &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linear&lt;/code&gt; - Linear Interpolation and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;upsample&lt;/code&gt; - Upsamling technique&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Time Delay Estimation Techniques - part 2</title>
   <link href="http://localhost:4000/signal%20processing/2014/10/19/TODA2/"/>
   <updated>2014-10-19T00:00:00+00:00</updated>
   <id>http://localhost:4000/signal%20processing/2014/10/19/TODA2</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In this article we will look at improving the Time Delay estimation in presense of noise using information of signals coming in from multiple receivers or observing multiple samples
intervals of the signal.&lt;/p&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;Let us consider an array of N receiver ,the signal $x_{m}[n]$ at receiver is given by&lt;/p&gt;

\[x\_{m}[n] = s[n]*h[n]+w[n]\]

&lt;p&gt;Let us say that by choosing the appropriate  delays $\delta_{m}$ we can make all the receivers aligned in time and them summing the time aligned signals together.&lt;/p&gt;

&lt;p&gt;The aim of such techniques is to boost SNR of the signal by combining the signals from multiple receivers.&lt;/p&gt;

&lt;p&gt;Since the signal is periodic,we can observe multiple instances of the same signal.Again if we are sure that the signals are coherent in sampled intervals we can combine multiple observation of the signal.The aim of such techniques is to boost the SNR of the signal by observing multiple instances of the singal.&lt;/p&gt;

&lt;p&gt;Both techniques have the same effect of enhancing the signal and averaging the noise,therby increasing the SNR of the signal.&lt;/p&gt;

&lt;h3 id=&quot;reducing-environmental-noise&quot;&gt;Reducing Environmental Noise&lt;/h3&gt;

&lt;p&gt;Mathematically this can be expressed as&lt;/p&gt;

\[y[n] = \sum\_{m=1}^{M} x\_{m}[n-\delta\_{m}]\]

\[y[n] = \sum\_{m=1}^{M} s\_{m}[n-\delta\_{m} ] + w\_{m}[n]\]

&lt;p&gt;The noise due to sensor or amplification process can be assumed to be independent.
The noise introduced dues to environment will be same in all the receivers.
The noise can be assumed to be sample of stationary white random process.&lt;/p&gt;

&lt;p&gt;Let us assume for now that signals are time aligned&lt;/p&gt;

&lt;p&gt;We can see that the our signal contains information plus additive noise.Most of region
contains just noise and only a small section of the time duration contains the information.&lt;/p&gt;

&lt;p&gt;The basic idea is that when we add the signals ,information will get enhanced while the noise will get averaged.In this way we can boost signal strength and achieve higher SNR.&lt;/p&gt;

&lt;p&gt;If the noise from independent receivers are considered to uncorrleated.
When we add noise components,we are  effectively adding two independent gaussian
random variables.Thus resultant is also a Gaussian random variable.&lt;/p&gt;

&lt;p&gt;The sum of N Gaussian random variables is the Gaussian random variable with the standard deviation
\(\sqrt{ \sum\_{i=0}^N\sigma\_{i }^2}\)&lt;/p&gt;

&lt;p&gt;Thus if we consider that noise power is same in each of receivers then by adding the signals
the resultant noise power is&lt;/p&gt;

\[P\_{N}=  \sigma^2 N\]

&lt;p&gt;However the signal is a deterministic ,the resultant signal power of adding N signals is&lt;/p&gt;

&lt;p&gt;$P_{M} = N^2 P_{S} $&lt;/p&gt;

&lt;p&gt;Thus the SNR of resultant signal is&lt;/p&gt;

\[SNR = \frac{P\_{s}}{P\_{n}} N\]

&lt;p&gt;The SNR is boosted by $N$ times.&lt;/p&gt;

&lt;p&gt;Let us see if we can observe this via simulations&lt;/p&gt;

&lt;p&gt;to generated uncorrelated random sequences,we change the seed every time we generate the random noise.This will simulate condition of noise generated from multiple sensors.&lt;/p&gt;

&lt;p&gt;In the first case we observe only the noise signal&lt;/p&gt;

&lt;p&gt;Let us observe the variance of sum signal for N={2,4,8 …. 128}&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;N&lt;/th&gt;
      &lt;th&gt;Simulation&lt;/th&gt;
      &lt;th&gt;Theory&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.14&lt;/td&gt;
      &lt;td&gt;0.14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.20&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0.28&lt;/td&gt;
      &lt;td&gt;0.28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0.39&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;0.56&lt;/td&gt;
      &lt;td&gt;0.56&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;0.78&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;1.144&lt;/td&gt;
      &lt;td&gt;1.13&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;we can see from the table that simulated and theoretical results agrees with each other&lt;/p&gt;

&lt;p&gt;we consider the case where $M=4$ independent receivers are used .The signal is corrupted by additive environmental  noise of variance $\sigma^2=0.2$&lt;/p&gt;

&lt;pre&gt;
signal strength 1.6
noise power  0.04
Input SNR  40.0
estimated noise power  0.16
estimated signal power  6.4
estimated SNR  40.0
estimated improvement  1.0
emprical signal power  6.56051311603
emprical noise power  0.160339040093
emprical SNR  40.9165048776
emprical SNR improvement  1.02291262194
&lt;/pre&gt;
&lt;p&gt;The empirical results agree with our theoretical analysis.
Thought the improvement factor is not very significant&lt;/p&gt;

&lt;h3 id=&quot;sensor-noise&quot;&gt;Sensor Noise&lt;/h3&gt;

&lt;p&gt;The next case comes where we observe signal from the same receiver over N periods.&lt;/p&gt;

&lt;p&gt;Again the noise observed at each sample instance $[n]$ is sample of Gaussian random variable
since we assume stationary noise process.Thus noise variance is $\sigma^2$.Thus the sum signal will have noise power of $(N * \sigma^2)$&lt;/p&gt;

&lt;p&gt;As we add the signals the signal power increases by factor $N^2$.&lt;/p&gt;

&lt;p&gt;This comes directly from central limit theorem and law of larger numbers that the sum of N iid samples will approach gaussian distribution.Thus noise variance will be $\sigma^2$.&lt;/p&gt;

\[SNR =  \frac{P\_{s}}{P\_{n}} {N}\]

&lt;p&gt;The SNR is boosted by N Times.&lt;/p&gt;

&lt;p&gt;we consider the case where $M=2$ independent receivers are observed over a duration of $N=4$ periods.The signal is corrupted by additive  sensor noise of variance $\sigma^2=0.4$&lt;/p&gt;

&lt;p&gt;let us consider the case of $N=1$ ,number of periods&lt;/p&gt;
&lt;pre&gt;
signal strength 1.6
noise power  0.16
Input SNR  10.0
estimated noise power  0.32
estimated signal power  6.4
estimated SNR  20.0
estimated improvement  2.0
emprical signal power  7.04582360211
emprical noise power  0.321264927044
emprical SNR  21.931505773
emprical SNR improvement  2.1931505773
&lt;/pre&gt;
&lt;p&gt;Let us consider the case of $N=4$&lt;/p&gt;
&lt;pre&gt;
signal strength 1.6
noise power  0.16
Input SNR  10.0
estimated noise power  1.28
estimated signal power  102.4
estimated SNR  80.0
estimated improvement  8.0
emprical signal power  105.0704936
emprical noise power  1.27294915151
emprical SNR  82.5409981816
emprical SNR improvement  8.25409981816
&lt;/pre&gt;

&lt;p&gt;The empirical estimates again agree with our theoretical analysis.
We get a significant improvement in SNR,just by increasing the number of periods of observation.The SNR improves linearly by factor of $N$.&lt;/p&gt;

&lt;h3 id=&quot;snr-improvement-in-presence-of-sensor-and-environmental-noise&quot;&gt;SNR Improvement In Presence of sensor and environmental noise&lt;/h3&gt;

&lt;p&gt;Let us say that the signal is corrupted by both environmental and receiver noise&lt;/p&gt;

&lt;p&gt;There are  N receivers .The same environmental noise is observed in each of the receivers.
Let us assume that the sequences are time aligned.Each of receivers has uncorrelated noise components.
We observe each receiver channel for M periods .&lt;/p&gt;

&lt;p&gt;Thus in total we have M*N Signals&lt;/p&gt;

&lt;p&gt;Thus total noise power in a single period be considered as $2 \sigma^2$&lt;/p&gt;

&lt;p&gt;The environmental noise component which is present in all the sensors cannot
be reduced by adding the sensor signals.This component can only be reduced by observing the signal over longer duration&lt;/p&gt;

&lt;p&gt;When signals from different receivers are added,the environmental noise power is also amplified
along with the signal.This noise power is amplified by a factor $N^2$&lt;/p&gt;

&lt;p&gt;Now we have M periods of the signal,Thus noise power after adding M periods of the signal is given by
$M*N^2$.&lt;/p&gt;

&lt;p&gt;The factor by which total sensor noise power increases by adding all the signals is given by $MN$&lt;/p&gt;

&lt;p&gt;Thus the total noise power is given by $M N^2 \sigma_{1}^2 + MN \sigma_{2}^2$&lt;/p&gt;

&lt;p&gt;The factor by which total signal power increases by adding all the signals is given by $M^2N^2$&lt;/p&gt;

&lt;p&gt;Thus SNR of sum of signals is given by $\frac{M^2N^2 P_{M}}{M N^2 \sigma_{1}^2 + MN \sigma_{2}^2}$&lt;/p&gt;

&lt;p&gt;When both variances are same,the SNR Improvement can be expressed as 
$\frac{2M^2N^2}{N^2M+NM} =\frac {2MN}{N+1}$&lt;/p&gt;

&lt;p&gt;we consider the case where $N=2$ independent receivers are observed over a duration of $M=4$ periods.The signal is corrupted by additive  environmental and sensor noise of variance $\sigma^2=0.4$&lt;/p&gt;

&lt;p&gt;Theoretical improvement by a factor of   5.33&lt;/p&gt;

&lt;pre&gt;
signal strength 1.6
noise power  0.32
Input SNR  5.0
estimated noise power  3.84
estimated signal power  102.4
estimated SNR  26.6666666667
estimated improvement  5.33333333333
emprical signal power  107.678534403
emprical noise power  3.82682954561
emprical SNR  28.1377921644
emprical SNR improvement  5.62755843289
&lt;/pre&gt;
&lt;p&gt;The simulations give a improvement by 5.62&lt;/p&gt;

&lt;p&gt;In terms of decibles its improvement of about 25db just by averaging signals.&lt;/p&gt;

&lt;p&gt;Also it can be seen that improvement is larger by considering longer duration signals than adding more receivers.&lt;/p&gt;

&lt;h3 id=&quot;time-delay-estimation&quot;&gt;Time Delay Estimation&lt;/h3&gt;

&lt;p&gt;Given signals form  the receviers,time period to analyze signals we compute the SNR for all possible delays between the signals.The delay which gives the maximum SNR is considered to be the time delay between the signals.&lt;/p&gt;

&lt;p&gt;In the present article we only consider pairs of signals for analysis.We can perform the delay and sum operation wrt to ideal signal or noisy signal from one of the receivers.&lt;/p&gt;

&lt;h3 id=&quot;ideal-signal&quot;&gt;Ideal Signal&lt;/h3&gt;
&lt;p&gt;In the first method we  construct an ideal signal and perform this computation wrt signals obtained from both the receivers.&lt;/p&gt;

&lt;p&gt;The ideal signal is not plagued by environmental and sensor noise.When we add ideal and noisy version of the signal,the environmental and sensor noise are averaged.&lt;/p&gt;

&lt;p&gt;However the improvement is only between a pair of signals.The environmental signal can only be reduced by a factor of 4.&lt;/p&gt;

&lt;p&gt;Let us first consider the case of noise variance of $0.4$ and use a ideal signal to perform the computation&lt;/p&gt;

&lt;pre&gt;
signal strength 1.6
noise power  0.32
Input SNR  5.0
estimated noise power  1.28
estimated signal power  102.4
estimated SNR  80.0
estimated improvement  16.0
emprical signal power  103.442427564
emprical noise power  1.26823592579
emprical SNR  81.5640256366
emprical SNR improvement  16.3128051273
mean delay  5.0
std deviation  0.0
&lt;/pre&gt;

&lt;p&gt;Now the same computation is performed using a signal received from one of the receivers.
$MN^2 \sigma_{1}^2 + MN \sigma_{2}^2$ and for N=2 we get $4M  \sigma_{1}^2 + 2M \sigma_{2}^2$
and the total signal power increases by $4 M^2$&lt;/p&gt;

&lt;pre&gt;
signal strength 1.6
noise power  0.32
Input SNR  5.0
estimated noise power  3.84
estimated signal power  102.4
estimated SNR  26.6666666667
estimated improvement  5.33333333333
emprical signal power  104.34546039
emprical noise power  3.07193469288
emprical SNR  33.9673433265
emprical SNR improvement  6.79346866529
mean delay  5.0
std deviation  0.0
&lt;/pre&gt;

&lt;p&gt;Improvement factor is greater is the case of using ideal signal.In both the methods
since pairwise operations are performed,increasing the number of receivers has not effect
on the analysis.&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;
&lt;p&gt;A class  &lt;strong&gt;&lt;em&gt;TimeDelaySimulator&lt;/em&gt;&lt;/strong&gt; encapsulates all the methods for generating time delayed signals with additive environmental and sensor noise components and computing the statistics.&lt;/p&gt;

&lt;p&gt;All the results mentioned in the article can be simulate by executing the script.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

    def run(self,isignal):
        &quot;&quot;&quot; main function that generates noisy,time delayed signals 
        
        Parameters 
        --------
        isignal : numpy array
                  ideal signal
        Returns
        -------
        out : numpy array shape=(length,channel,period)
            
        
        &quot;&quot;&quot;

        l=len(isignal)
        #array for storing signal and noise
        signal=numpy.zeros((l,self.nchannels,self.periods));
        noise=numpy.zeros((l,self.nchannels,self.periods));    
        
        #generate environmental noise for each period and across all channels
        for k in range(self.periods):
            n=self.GenerateNoise(self.enoise,l)           
            #simulating environmental noise
            for i in range(self.nchannels):
                noise[:,i,k]=n;
            
            
        #for each channel add sensor noise
        for k in range(self.nchannels):
            
            #dummpy call to change the random seed 
            #so that we can generate uncorrelated sensor noise
            self.GenerateNoise(self.snoise,l,1)
            for i in range(self.periods):
                signal[:,k,i]=self.delay(isignal,(k+1)*self.tdelay)+noise[:,k,i]+self.GenerateNoise(self.snoise,l)    
        
        #change the seed everytime the function is called
        self.seed=int(np.random.uniform(0,1)*10000);   
            
        #flag to print the statistics 
        if self.once==1:
            self.once=0;
            PS=np.mean(isignal*isignal);
            PN=self.enoise*self.enoise+self.snoise*self.snoise;
           
            if self.tde_method==0:
                PNT=(self.periods*self.enoise*self.enoise)
                PNT=PNT+(self.periods*self.nchannels*self.snoise*self.snoise/self.nchannels)
                PST=(self.periods*self.periods)*PS*4              
                
            if self.tde_method==1:
                PNT=(self.periods*4*self.enoise*self.enoise)
                PNT=PNT+(self.periods*2*self.snoise*self.snoise)
                PST=(self.periods*self.periods)*PS*4
            
            print &quot;signal strength&quot;,PS
            print &quot;noise power &quot;,PN
            self.SNRI=PS/PN
            print &quot;Input SNR &quot;,self.SNRI          
            print &quot;estimated noise power &quot;,PNT
            print &quot;estimated signal power &quot;,PST
            self.SNR0=PST/PNT
            print &quot;estimated SNR &quot;,self.SNR0
            print &quot;estimated improvement &quot;,self.SNR0/self.SNRI
        
        #returns the signal        
        return signal

&lt;/pre&gt;

&lt;p&gt;A class  &lt;strong&gt;&lt;em&gt;TimeDelayEstimator&lt;/em&gt;&lt;/strong&gt; encapsulates all the methods for computing the time delay estimation mentioned in the article.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

    def run(self,signal,isignal,enoise,snoise,method=0):
        &quot;&quot;&quot; the main function that performs time delay estimation
        
        Parameters
        -----------
        signal : numpy array shape=(length,channel,periods)
                 input noisy signals for time delay estimation
                 
        isgignal : numpy array shape=(length,)
                   ideal signal
                   
        enoise,snoise : float 
                        environmental and sensor noise standard deviation
        
        method      : integer
                      0 - for pariwise delay sum with ideal signal
                      1 - pariwise delay sum with noisy signals
        
        Returns
        -------
        tdelay : numpy integer
                 estimated time delay
             
        
        &quot;&quot;&quot;
        
        size1=signal.shape;
        ch=size1[1];
        length=size1[0];
        periods=size1[2];
        enoise=enoise*enoise;
        snoise=snoise*snoise;
        PS=np.mean(isignal*isignal);      
 


        if method==0:
            iloop=ch-1
        if method==1:
            iloop=ch-1;         
        
        #compute the SNR threshold
        if method==0:
            vnoise=(periods**enoise/4)+(periods*iloop*snoise/iloop);
            vsnoise=[-2*vnoise,2*vnoise]    
            ispower=(periods*periods*PS);
            spower=np.mean(signal*signal)*periods*periods
            OSNR=(periods*periods*4*PS)+(1*vsnoise);
            OSNR=OSNR/(periods*periods)
        if method==1:
            vnoise=(periods*enoise)+(periods*iloop*snoise);
            vsnoise=[-2*vnoise,1*vnoise];
            spower=np.mean(signal*signal)*periods*periods
            ispower=(periods*periods*iloop*iloop*PS);
            OSNR=(periods*periods*4*PS)+vsnoise
            OSNR=OSNR/(periods*periods)
           
        
        
        #add the signals along periods
        s=np.mean(signal,axis=2);
        
  
        SNR=np.zeros((iloop,length))
        for i in range(iloop):
            #delay sum operation for various delays
            for k in range(length):
                if method==0:
                
                    su=s[:,0]+self.delay(s[:,i+1],k)
                if method==1:
                    su=s[:,0]+self.delay(s[:,i+1],k)
                
                #apply SNR Thresholding
                SNR[i,k]=np.mean(su*su);
                if(SNR[i,k] &amp;amp;lt ;OSNR[0]):
                   SNR[i,k]=0
        
        #find the index of maximum SNR
        m1=np.argmax(SNR,axis=1)          
   

        #average all the time delays        
        if method==0:
            tdelay=0;
            
            
            for i in range(iloop-1):
               
                tdelay=tdelay+abs(m1[0]-m1[i+1])/(i+1)
            tdelay=tdelay/(iloop-1)
        if method==1:
            
            tdelay=0;
            for i in range(iloop):
                tdelay=tdelay+((length-(abs(m1[i])+1)))/(i+1);
              
            
            tdelay=tdelay/iloop;
           
            

        #return the result  
        return tdelay   

&lt;/pre&gt;

&lt;p&gt;Change the mode from 0-4 to test various algorithms.The parameters like delay and noise can also
be changed and the various results mentioned in the article can be generated.&lt;/p&gt;

&lt;p&gt;The code can be found in the pyVision github repository &lt;a href=&quot;https://github.com/pi19404/pyVision&quot;&gt;https://github.com/pi19404/pyVision&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Files&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;pySignalProc/TimeDelayEstimation.py&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Time Delay Estimation Techniques - Part 1</title>
   <link href="http://localhost:4000/signal%20processing/2014/10/13/TDOA1/"/>
   <updated>2014-10-13T00:00:00+00:00</updated>
   <id>http://localhost:4000/signal%20processing/2014/10/13/TDOA1</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In this article we will look at signal processing techniques for time delay estimation.&lt;/p&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;Time delay estimation has been a research topic of significant practical importance in
many fields like radar, sonar, seismology, geophysics, ultrasonic’s, hands-free communications,Doppler positioning systems etc and is of fundamental importance in a variety of signal-processing applications.&lt;/p&gt;

&lt;p&gt;The problem boils down to computing the location of a digitized “signature” waveform residing within a larger time-slice.&lt;/p&gt;

&lt;p&gt;Let $s(n)$ be the sound source signal.
Let us consider two  spatially separated sensors and $x_{1}(t)$ and $x_{2}(t)$ be result of propagation of $s(t)$ through different paths to reach the respective sensors.&lt;/p&gt;

&lt;p&gt;A simple propagation model is that the signals just encounter attenuation and delay and are corrupted by additive noise&lt;/p&gt;

&lt;p&gt;$x_{i}(t) =  a_{i} s(t- \tau_{i}) + b_{i}(t)$&lt;/p&gt;

&lt;p&gt;where ,&lt;/p&gt;

&lt;p&gt;$b_{i}(t)$ is the additive noise uncorrelated with the signal&lt;/p&gt;

&lt;p&gt;We make a assumption that the distance from the sensors to the source is very large compared to the spatial separation between the sensors.Thus signal $s(t)$ received by both the receivers is the same,there is no significant change in the signal as it travels thorough different paths to reach the spatially separated sensors.&lt;/p&gt;

&lt;h3 id=&quot;cross-correlation&quot;&gt;Cross-correlation&lt;/h3&gt;

&lt;p&gt;One of the simplest method of time delay estimation is cross-correlation .
The cross-correlation of two signals is a measure of similarity between the two sequences.
The cross-correlation function is maximized when both the signals have significant overlap.&lt;/p&gt;

&lt;p&gt;$R_{xy}(k) = \sum_{i} {x(i) y(i+k)} = x[n]*y[-n]$&lt;/p&gt;

&lt;p&gt;Compute maximum absolute value of the correlation function to estimate the lag.&lt;/p&gt;

&lt;p&gt;To test the results we create  create two sequences,one a delayed version of another.
We add white noise to the delayed sequence and use sample correlation to detect the lag.
while performing correlation we normalize the signals,so that correlation measure is bounded
between [0,1]&lt;/p&gt;

&lt;p&gt;Now we increase the noise to try to get estimate of noise co-variance at which this technique fails.
we run the experiment 100 times and estimate the number of times we get the proper answer.&lt;/p&gt;

&lt;h4 id=&quot;testing-with-different-additive-noise-covariance&quot;&gt;Testing with Different Additive Noise Covariance&lt;/h4&gt;
&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.1
SNR 15.4390474505
Correct  98  Incorrect  3
mean  9.90099009901 Std  1.0000490136
&lt;/pre&gt;
&lt;p&gt;We see that we are always correct,when the noise levels are low or SNR is high&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image9.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.3
SNR 5.89662235611
Correct  48  Incorrect  53
mean  9.79207920792 Std  1.2767287382
&lt;/pre&gt;
&lt;p&gt;The SNR has dropped to about 6db and error in estimation is about 50%. The accuracy is linearly
related with the SNR&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image10.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.5
SNR 1.45964736379
Correct  36  Incorrect  65
mean  9.80198019802 Std  1.57969668714

&lt;/pre&gt;
&lt;p&gt;At 2 db SNR we can see the variance of estimated time delay increasing with rising noise levels.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image11.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see in the auto-correlation plot the difference between the peak and its neighbors is not significant and depending on the random noise levels introduced,we may not always get the right answer.&lt;/p&gt;

&lt;p&gt;As noise increases,we can see variance in the estimated time delay increases and error in estimation also increases.As the level of noise increases, the uncertainty in the time-delay estimate increases&lt;/p&gt;

&lt;p&gt;We need a reliable time delay estimator in presence of additive noise.&lt;/p&gt;

&lt;p&gt;If is always difficult to estimate the time delays for a base-band signal illustrated above.The due to the noise,we are not reliably able to estimate the delay corresponding to the maximal overlap between the noisy and ideal signal.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
def delay(signal,N):
    &quot;&quot;&quot; function introduces a delay of N samples 
    
    Parameters
    -----------
    signal : numpy-array,
             The input signal
             
    N      : integer
             delay
        
    Returns
    --------
    out : numpy-array
          delayed signal
    
    &quot;&quot;&quot;
    d=numpy.zeros((1,N+1));    
    signal=numpy.append(d,signal)
    return signal;

def addNoise(s,variance):
    &quot;&quot;&quot; function add additive white gaussian noise to the input signal 
    
    Parameters
    -----------
    s : numpy-array,
             The input signal
             
    N      : float
             noise covariance
        
    Returns
    --------
    out : numpy-array
          noisy signal    
    
    &quot;&quot;&quot;
    noise = np.random.normal(0,variance,len(s))                    
    s=s+noise;                              
    return s;


if __name__ == &quot;__main__&quot;:  
            
        Fs=1000;
        
        mode=0
        if mode==0:
            x = triang(20);
            x2=x;

		......
            
        tdelay=10;
        varnoise=0.001;
        loop=1000

        
        #delay the signal
        dx=delay(x,tdelay);
        result=numpy.zeros((1,loop));
        for i in range(loop):
            s=dx;
            #add noise
            s=addNoise(s,varnoise);
            
            #normalize the signals
            s=s/np.linalg.norm(s);
            x1=x2/np.linalg.norm(x2);
            
            unfiltered_signal=s;
            
            r=numpy.correlate(s,x1,mode=&quot;full&quot;)
            arg=np.argmax(r)
            result[0,i]=abs(arg-len(x))     

		#plot the results
	    c=np.sum(result==tdelay)
        ic=np.sum(result!=tdelay)
        #10*np.log(np.sum(np.abs(x*x))/
        print &quot; *********** Information ************ &quot;
        print 'time delay : ',tdelay
        print 'Noise :',varnoise
        print &quot;SNR&quot;,10*np.log(np.mean(abs(x*x))/(varnoise*varnoise))/np.log(10);
        print &quot;Correct &quot;,str(c),&quot; Incorrect &quot;,str(ic)
        print &quot;mean &quot;,np.mean(result),&quot;Std &quot;,np.std(result)
        print result
        plt.figure(1)
        subplot(2,2,1) 
        plt.plot(range(len(x)),x)
        xlabel('Time')
        ylabel('Amplitude')

        
        subplot(2,2,2) 
        plt.plot(range(len(unfiltered_signal)),unfiltered_signal)
        xlabel('Time')
        ylabel('Amplitude')

        subplot(2,2,3) 
        plt.plot(range(len(r)),r)
        xlabel('Time')
        ylabel('Amplitude')            
&lt;/pre&gt;

&lt;h3 id=&quot;rectangular-pulse-signals&quot;&gt;Rectangular Pulse signals&lt;/h3&gt;

&lt;p&gt;Let us look at the results for a different signal in the form of a rectangular pulse .
we will consider the wave of same duration 30.&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.5
SNR 0.791812460476
Correct  86  Incorrect  15
mean  9.92079207921 Std  1.11411438271
&lt;/pre&gt;
&lt;p&gt;we see that signal has a lower SNR,but accuracy and estimated time delay variance is lower.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image12.png&quot; alt=&quot;enter image description here&quot; /&gt;
Thus the type of pulse we use has a impact on the accuracy of auto-correlation function.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
def rectangular(N,start,end):
        &quot;&quot;&quot; fuction generates a rectangular pulse 
        
        Parameters
        ---------
        N : integer
            length of signal
            
        start,end: integer
                starting and ending index of pulse
        
        &quot;&quot;&quot;
        x = np.zeros((1,N))
        x[:,start:end]=1;
        x=x.flatten();    
        return x;
 &lt;/pre&gt;

&lt;h3 id=&quot;coded-pulses&quot;&gt;Coded Pulses&lt;/h3&gt;
&lt;p&gt;Broadband  techniques have a sequence of code pulses that increase the accuracy of time delay estimation in the presence of noise.&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
time delay :  10
Noise : 0.5
SNR 2.55272505103
Correct  95  Incorrect  6
mean  9.93069306931 Std  1.01725144864

&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image13.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
x=rectangular(20,8,14)+rectangular(20,2,5)
&lt;/pre&gt;

&lt;p&gt;But most of the time we do not have control of the source pulse.&lt;/p&gt;

&lt;p&gt;In the remainder of the article we will assume that it is a rectangular pulse of duration 1000
with impulse of duration 50 starting at 100.&lt;/p&gt;
&lt;pre class=&quot;brush : python &quot;&gt;
time delay :  10
Noise : 0.5
SNR -6.98970004336
Correct  856  Incorrect  144
mean  9.991 Std  0.561176442841
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image14.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
x=rectangular(1000,100,150)
&lt;/pre&gt;

&lt;p&gt;Now given source signal,we need to see if we can do better in the presence of additive noise atleast.In real life situations there will be a host of other distortions and effects which will  increase the estimation errors apart from noise.&lt;/p&gt;

&lt;p&gt;For pulses like the ones observed above,noise is a dominant factor,signal energy is low compared to noise energy.&lt;/p&gt;

&lt;h3 id=&quot;modulated-pulses&quot;&gt;Modulated Pulses&lt;/h3&gt;

&lt;p&gt;Often the pulses are modulated by sinusoidal waves for longer range transmissions.
&amp;lt;/pre&amp;gt;
time delay :  10
Noise : 0.5
SNR -6.98970004336
Correct  988  Incorrect  12
mean  10.004 Std  0.109471457467
Actual time delay 10
&amp;lt;/pre&amp;gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image15.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
def sinepulse(N,start,end,f,Fs=1000,tau=0):
        &quot;&quot;&quot; function generates modulated rectangular pulse

        Parameters
        ---------
        N : integer
            length of signal
            
        start,end: integer
                starting and ending index of pulse
                
        f   : integer
              modulated carrier freuency
              
        Fs  : integer
              Sampling freuency

        tau : integer
              carrier phase delay in samples.
              
        Returns
        --------
        out : numpy-array
              modulated rectangular  signal               
          
        &quot;&quot;&quot;
        x = np.zeros((1,N))
        t=np.asarray(range(0,end-start));
        x[:,start:end]=np.cos(2*np.pi*f*(t+tau)/Fs);
        
        return x.flatten();
&lt;/pre&gt;

&lt;h3 id=&quot;carrier-synchronization-issues&quot;&gt;Carrier Synchronization Issues&lt;/h3&gt;

&lt;p&gt;we can synchronize exactly with the carrier ,then like broadband techniques we can achieve a significantly enhanced SNR. However synchronizations are never possible.&lt;/p&gt;

&lt;p&gt;we introduce a phase delay of 1 sample to check the effect of carrier phase errors&lt;/p&gt;
&lt;pre class=&quot;brush : python &quot;&gt;
*********** Information ************ 
time delay :  10
Noise : 0.3
SNR -2.55272505103
Correct  2  Incorrect  998
mean  229.036 Std  244.973840857

&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image16.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In such cases another approach might be to use rectangular envelope
but in the present case that also does not seem to help&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.3
SNR -2.55272505103
Correct  0  Incorrect  1000
mean  325.336 Std  272.058543523

&lt;/pre&gt;
&lt;h4 id=&quot;envelope-detection&quot;&gt;Envelope Detection&lt;/h4&gt;
&lt;p&gt;We perform envelope detection on the signal and then apply correlation.
This improves the situation considerably&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.3
SNR -2.55272505103
Correct  661  Incorrect  339
mean  9.746 Std  0.793400277288
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image17.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
            if mode==5 or mode ==6 or mode==7:                
                s=abs(scipy.signal.hilbert(s))
&lt;/pre&gt;

&lt;h3 id=&quot;filtering&quot;&gt;Filtering&lt;/h3&gt;

&lt;p&gt;If we known the carrier frequency ,we can filter the noise outside the signal  bandwidth before performing the correlation operation.&lt;/p&gt;

&lt;p&gt;For the remaining examples we consider signal with carrier frequency of 1KHz and Sampling rate
of 5Khz. The reason for that is explained below&lt;/p&gt;

&lt;p&gt;Let us first look at the frequency characteristics of the rectangular pulse&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image26.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The sharp transition at the edges of the rectangular pulse give rise to high frequency components.
Thus while filtering we need to take these into account,else we may end up distorting the edge
leading to errors in time delay estimation.&lt;/p&gt;

&lt;p&gt;The width of the mainlobe in frequency domain is inversely proportional to width of rectangular pulse
in the time domain.Thus a wider pulse in the time domain will provide higher spectral compression in the frequency domain.&lt;/p&gt;

&lt;p&gt;we can see that main lobe of the rectangular pulse would have normalize freuency of 0.05
Few of the side lobes also contain significant information.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

def plotFrequency(b,a=1):
    &quot;&quot;&quot; the function plots the frequency and phase response &quot;&quot;&quot;
    w,h = signal.freqz(b,a)
    h_dB = abs(h);#20 * np.log(abs(h))/np.log(10)
    subplot(211)
    plot(w/max(w),h_dB)
    #plt.ylim(-150, 5)
    ylabel('Magnitude (db)')
    xlabel(r'Normalized Frequency (x$\pi$rad/sample)')
    title(r'Frequency response')
    subplot(212)
    h_Phase = np.unwrap(np.arctan2(np.imag(h),np.real(h)))
    plot(w/max(w),h_Phase)
    ylabel('Phase (radians)')
    xlabel(r'Normalized Frequency (x$\pi$rad/sample)')
    title(r'Phase response')
    plt.subplots_adjust(hspace=0.5)
&lt;/pre&gt;

&lt;p&gt;We try with a normalized frequency components of 0.05 which contains around 3 adjacent side-lobes&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image31.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we can see that the signal has been attenuated near the edges.Thus the distortion in the region where the pulse starts will lead  to ambiguity of time delay computation.&lt;/p&gt;

&lt;p&gt;Thus we need to consider the freuency components to retain so that sharp transition in time domain are not affected&lt;/p&gt;

&lt;p&gt;The carrier freuency is 1Khz .The normalized carrier frequency at sampling rate of 5Khz is 0.4.
Thus the spectrum of modulated rectangular pulse is centered at 0.4&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image30.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thus significant frequency band lies from 0.3 to 0.5.&lt;/p&gt;

&lt;p&gt;If we bandpass filter frequencies in this band,we should get a relatively noiseless waveform,which is expected to improve the performance of correlation.&lt;/p&gt;

&lt;p&gt;We apply band-pass  butter worth filter of order 2 with normalized cutoff frequencies are 0.2 and 0.6.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image33.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
def butter_bandpass(lowcut, highcut, fs, order=5):
    &quot;&quot;&quot; function returns the bandpass butterworth filter coefficients 
    
    Parameters
    -------------    
    lowcut,highcut : integer
                     lower and higher cutoff freuencies in Hz
                     
    Fs : Integer
         Samping freuency in Hz

    order : Integer
            Order of butterworth filter                     
        
    Returns
    --------
    b,a - numpy-array
          filter coefficients 
          
    &quot;&quot;&quot;
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low,high], btype='bandpass')
    return b, a
    
    
def bandpass_filter(data, lowcut, highcut, fs, order,filter_type='butter_worth'):

 &quot;&quot;&quot; the function performs bandpass filtering 
    
    Parameters
    -------------
    data : numpy-array
           input signal
           
    lowcut,highcut : integer
                     lower and higher cutoff freuencies in Hz
                     
    Fs : Integer
         Samping freuency in Hz

    order : Integer
            Order of butterworth filter                     
        
    Returns
    --------
    out : numpy-array
          Filtered signal
    
    &quot;&quot;&quot;
    global once
    if filter_type=='butter_worth':
        b, a = butter_bandpass(lowcut, highcut, fs, order=order)            
        if once==0:
            plt.figure(2)
            plotFrequency(b,a)
            once=1
        y = filtfilt(b, a, data)
        return y
    
&lt;/pre&gt;

&lt;p&gt;The butter-worth band-pass filter is not a zero phase .It will introduce different phase delay depending of frequency. The.The phase plot of the band pass filter is not linear,&lt;/p&gt;

&lt;p&gt;To see the phase delay effects of bandpass butter-worth filter,we consider the case of no noise .
we can see that instead of 10,delay is introduced as 12 due to butter-worth filter. However we have a stable singular correlation peak being detected&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.001
SNR 46.9897000434
Correct  0  Incorrect  1000
mean  12.0 Std  0.0
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image34.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
.....
        #introduce delay
        dx=delay(x,tdelay);
        result=numpy.zeros((1,loop));
        for i in range(loop):
            s=dx;

            #add noise            
            if varnoise!=0:
                s=addNoise(s,varnoise)

            #normalize signals                
            s=s/np.linalg.norm(s);
            x1=x2/np.linalg.norm(x2);
            if carrier!=None:
                unfiltered_signal=s;
                if mode==1:
                    s = bandpass_filter(s, carrier-500, carrier+500, Fs,order,filter_type)
                if mode==2:
                    s = bandpass_filter(s, carrier-750, carrier+750, Fs,order,filter_type)

                filtered_signal=s;
            else:
                unfiltered_signal=s;
             
            #perform envelope detection
            s=abs(scipy.signal.hilbert(s))
            if mode==1 or mode==2:
                r=numpy.correlate(s,x1,mode=&quot;full&quot;)

.......
&lt;/pre&gt;
&lt;p&gt;To compensate for that we need to use zero phase filtering techniques like forward backward filtering.&lt;/p&gt;

&lt;p&gt;To achieve zero phase the linear filter is applied twice, once forward and once backwards. The combined filter has zero phase.In general  forward-backward filtering squares the amplitude response and zeros the phase response if phase of filter is linear.&lt;/p&gt;

&lt;p&gt;we see that there are few errors inspire of applying the forward backward algorithm due to distortions introduced by bandpass filtering of high frequency components.Though the mean is closer to actual delay of 10.&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0
Correct  0  Incorrect  1000
mean  9.0 Std  0.0
&lt;/pre&gt;
&lt;p&gt;This tells us that we cannot eliminate the effects introduced due to filtering.&lt;/p&gt;

&lt;p&gt;we now introduce noise and compare the performance against envelope detection.&lt;/p&gt;
&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.3
SNR -2.55272505103
Correct  442  Incorrect  558
mean  9.49 Std  0.678159273327
&lt;/pre&gt;
&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Envelope Detection ************ 
time delay :  10
Noise : 0.3
SNR -2.55272505103
Correct  661  Incorrect  339
mean  9.746 Std  0.793400277288
&lt;/pre&gt;

&lt;p&gt;Compared to just envelope detection,we can see that standard deviation has reduced.Thus the neighborhood of estimated TDOA values are reduced,though the mean is shifted away from 10 may be due to the phase delay effects of bandpass filter.&lt;/p&gt;

&lt;p&gt;Though band-pass filtering did not lead to a significant improvement  due no the noise within the frequency bandwidth .It is essential to keep out unwanted frequency components due to environmental noise and other factors.&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;

&lt;p&gt;The code used in the article can be found in github repository &lt;a href=&quot;https://github.com/pi19404/pyVision/tree/master/pySignalProc&quot;&gt;“Github Link”&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Files&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TDOA1.py&lt;/li&gt;
  &lt;li&gt;TODA2.py - Band Pass Filtering&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All the plots included in the article can generated by changing the mode variable in the files&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Discrete Fourier Transform For Freuency Analysis</title>
   <link href="http://localhost:4000/signal%20processing/2014/10/11/DFT1/"/>
   <updated>2014-10-11T00:00:00+00:00</updated>
   <id>http://localhost:4000/signal%20processing/2014/10/11/DFT1</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Fourier analysis&lt;/strong&gt; is generally concerned with the analysis and synthesis of functions. The decomposition of signal into easy-to-analyze components and the reconstruction from such components.&lt;/p&gt;

&lt;p&gt;In this article we will look at Fourier analysis of discrete time signals.&lt;/p&gt;

&lt;p&gt;Discrete time signal are defined at only particular set of time instances and are represented as sequence of real numbers that have continuous range of values&lt;/p&gt;

&lt;h3 id=&quot;fourier-series-representation-of-signal&quot;&gt;Fourier Series Representation of signal&lt;/h3&gt;

&lt;p&gt;A discrete time complex exponential is periodic in nature&lt;/p&gt;

&lt;p&gt;$ e^{j k\omega (n+N)} = e ^{j k \omega n} $
where  , $\omega=\frac{2\pi}{N}$&lt;/p&gt;

&lt;p&gt;The set of all discrete time time complex exponential signals that are periodic with period N is given by
$\phi_{k}[n] =  e ^{j k \frac{2\pi}{N} n} , \forall k \in \mathcal{Z}$&lt;/p&gt;

&lt;p&gt;\(\phi\_{k+rN}[n] = e ^{j (k+rN) \frac{2\pi}{N} n}  = e ^{j k \frac{2\pi}{N} n}  * e ^{j r N \frac{2\pi}{N} n}\)
\(\phi\_{k+rN}[n] = e ^{j k \frac{2\pi}{N} n} e ^{j r  2\pi n} =e ^{j k \frac{2\pi}{N} n}\)&lt;/p&gt;

&lt;p&gt;When k is changed by integral multiples of N ,we get identical sequence.
Thus there are only N unique values of k for which we can define unique discrete time complex exponential sequence.&lt;/p&gt;

\[\begin{align} x[n]=\sum\_{k} x[k] e^{j k \omega n} \end{align}\]

&lt;p&gt;$e^{j k \omega n} $ are only unique over N successive values of k,summation is only considered
over this range&lt;/p&gt;

\[\begin{align} x[n]=\sum\_{k={N}} x[k] e^{j k \omega n} \end{align}\]

&lt;p&gt;$x[n]$ is  periodic with period N.&lt;/p&gt;

&lt;h3 id=&quot;discrete-time-fourier-series&quot;&gt;Discrete time Fourier Series&lt;/h3&gt;

&lt;p&gt;Let us consider a arbitrary periodic signal $x[n]$ with period N and that  $x[n]$ can be expressed in the form&lt;/p&gt;

\[\begin{align} x[n]=\sum\_{k} X[k] \phi\_{k}[n] \end{align}\]

&lt;p&gt;Thus we need to find out if $x[n]$ can be expressed in the above form and if so the coefficients $x[k]$ for which this representation is valid.&lt;/p&gt;

&lt;p&gt;The theoretical derivations for the same can be found in all signal processing references.The result are as follows&lt;/p&gt;

&lt;p&gt;\(\begin{align} X[k] =\sum\_{n=0}^{N-1}x[n] e^{- j k \frac{2\pi}{ N} n } \end{align}\) and 
\(\begin{align} x[n] =\frac{1}{N}\sum\_{k=0}^{N-1}X[k]\,e^{j k \frac{2\pi}{ N} n } \end{align}\)&lt;/p&gt;

&lt;p&gt;$X[k]$ are called as &lt;strong&gt;Fourier series coefficients&lt;/strong&gt;.The coefficients are also referred to as spectral coefficients of signal $x[n]$.&lt;/p&gt;

&lt;p&gt;The representation of signal $x[n]$ in terms of spectral  coefficients is called as the &lt;strong&gt;Fourier series representation&lt;/strong&gt; of the signal&lt;/p&gt;

&lt;p&gt;These coefficient specify the decomposition of signal $x[n]$ into sum of N harmonically related complex exponential .&lt;/p&gt;

&lt;p&gt;Any periodic discrete time signal $x[n]$ can be represented using the Fourier series representation 
and Fourier series representation enable us to represent any periodic signal as weighted sum of complex exponential s.&lt;/p&gt;

&lt;p&gt;Let us look at some examples to understand what information can Fourier series representation of a signal give us.&lt;/p&gt;

&lt;pre class=&quot;brush: python&quot;&gt;
def FourierSeries(input,N=None):
    &quot;&quot;&quot; computes the fourier series coefficients of input signal
    
    Parameters
    -----------
    input : numpy array
            input discrete time signal
            
    Returns
    --------
    out : complex,list
          fourier series coefficients
    &quot;&quot;&quot;
    
    N=len(input);

    w=2*cmath.pi/N;
    input=input[0:N];
    n=numpy.arange(0,N);    
    r=cexp(-1j*w*n);

    output = [complex(0)] * N    
    for k in range(N):        
        r=input*cexp(-1j*w*n*k)          
        output[k]=np.sum(r);
        
   
    return output;

&lt;/pre&gt;

&lt;p&gt;Let us consider a sinusoidal signal with frequency $f_{s}=10Hz$,for a duration of 1sec
Lets sample this waveform at $F_{s}=150Hz$ and observe the Fourier series coefficients of the signal&lt;/p&gt;

&lt;p&gt;we sample at 150Hz,In the frequency spectrum,just consider a single period of the cosine waveform,ie 
$F_{s}/f_{s}=15$ samples.The output can be seen in subplot 2&lt;/p&gt;

&lt;p&gt;we take complete signal,and compute the Fourier series coefficients,output can be seen in subplot 3&lt;/p&gt;

&lt;p&gt;Let us consider  magnitude plot of Fourier series&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image2.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
def FourierSinusoids(F,w,Fs,synthesis=None):    
    &quot;&quot;&quot; the function generates a discrete time sinusoid,computes
	the Fourier series coefficients and plots the time and frequency
	data 
 
     Paraneters
     ----------
     F : numpy array
         sinuoidal frequency components
         
     w  : numpy array
          the weights associated with freuency components
         
     Fs : Integer
          sampling frequency
          
     synthesis : int
                 if 1 ,reconstructs signal and plots the original and reconstructed
                 signal
 
    &quot;&quot;&quot;
    if synthesis==None:
        synthesis=0;
        
    Ts=1.0/Fs;   
    xs=numpy.arange(0,1,Ts) 
    
    signal=numpy.zeros(np.shape(xs));
    for i in range(len(F)):
        omega=2*np.pi*F[i];
        signal = signal+ w[i]*numpy.cos(omega*xs);
    #plot the time domain signal    
    subplot(2,1,1)
    plt.plot(range(0,len(signal)),signal)
    xlabel('Time')
    ylabel('Amplitude')
    title('time doman')
    #plt.ylim(-2, 2)
    
    #compute fourier series coefficients
    r1=FourierSeries(signal)
    a1=cabs(r1)
    
    if synthesis==0:
        #plot the freuency domain signal
        L=len(a1);
        fr=np.arange(0,L);
        subplot(2,1,2)
        plt.stem(fr,a1,'r') # plotting the spectrum
        xlabel('Freq (Hz)')
        ylabel('|Y(freq)|')
        title('complete signal')
        ticks=np.arange(0,L+1,25);
        plt.xticks(ticks,ticks);     
        show() 
        
    if synthesis==1:
        rsignal=IFourierSeries(r1);
        print np.allclose(rsignal, signal)    
        subplot(2,1,2) 
        plt.stem(xs,signal)
        xlabel('Time')
        ylabel('Amplitude')
        title('reconstructed signal')
        show() 


if __name__ == &quot;__main__&quot;:     

    mode =0
    
    if mode==0:
        F=[10];
        F=np.array(F);
        w=numpy.ones(F.shape);
        #plot the time domain signal and fourier series component
        FourierSinusoids(F,w,150);
&lt;/pre&gt;

&lt;p&gt;we can see that the frequency corresponding to 10Hz we can observe peak.The total number of Fourier
series coefficients are equal to the total number of input samples.&lt;/p&gt;

&lt;p&gt;Now let us consider a combination of sinusoidal signals at freuency 10 and 15Hz.The signal is periodic with frequency which is LCM of 10 and 15 ie 5Hz.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image4.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    if mode==1:
        F=[10,20];
        F=np.array(F);
        w=numpy.ones(F.shape);
        FourierSinusoids(F,w,150); 
&lt;/pre&gt;
&lt;p&gt;Now lets keep on adding frequency components.The below signal is periodic with period of 1 sec&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image5.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;

    if mode==2:
        F=range(1,10);
        F=np.array(F);
        w=numpy.ones(F.shape);	    
        FourierSinusoids(F,w,150);    
&lt;/pre&gt;

&lt;h3 id=&quot;synthesis-of-signal&quot;&gt;Synthesis of Signal&lt;/h3&gt;
&lt;p&gt;In the earlier section we saw the process of decomposition of aperiodic signal into its frequency components.&lt;/p&gt;

&lt;p&gt;we will now look at the &lt;strong&gt;synthesis of signal&lt;/strong&gt; from its Fourier series coefficients.&lt;/p&gt;

\[{x[n] =
\frac{1}{N}\sum\_{k=0}^{N-1}X[k]\,e^{j k \frac{2\pi}{ N} n }}\]

&lt;p&gt;Below is plot of the original and reconstructed signal and python code for the&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image6.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
def IFourierSeries(input):
    &quot;&quot;&quot; function reconstructs the signal from fourier series coefficients
    
    Parameters
    ---------
    input : cmath list
            fourier series coefficients    
    
    Returns
    -----------
    out : numpy arrary
          reconstructed signal
    
    &quot;&quot;&quot;
    N=len(input);
    w=2*cmath.pi/N;
    k=numpy.arange(0,N);    
    output = [complex(0)] * N   
    for n in range(N):  
        r=input*cexp(-1j*w*n*k);
        output[n]=np.mean(r);

    print output.__class__    
    return output;


    if mode==3:
        F=range(1,10);
        F=np.array(F);
        w=numpy.ones(F.shape);
        FourierSinusoids(F,w,150,1); 
&lt;/pre&gt;

&lt;h3 id=&quot;discrete--fourier-transform&quot;&gt;Discrete  Fourier Transform&lt;/h3&gt;

&lt;p&gt;Now arises the situation what do we do for a-periodic signals.After a lot of theorotical analysis
on Discrete time Fourier transform and sampling in the frequency domain,it turns out
we just assume periodic extension of aperiodic signal and compute Fourier series as above.&lt;/p&gt;

&lt;p&gt;The Fourier series coefficients for a periodic signal are also periodic with same period N&lt;/p&gt;

\[X[k+N]=X[k]\]

&lt;p&gt;If we consider a single period of N values of Fourier series coefficient ,we obtain a finite duration sequence
which is called Discrete Fourier Transform (DFT).&lt;/p&gt;

&lt;p&gt;Thus by computing the DFT we obtain the Fourier series coefficients for single period.&lt;/p&gt;

&lt;p&gt;It is upto us to choose a period of the signal.Let us consider a aperiodic impulse of length 150 and on-duty cycle of 5.&lt;/p&gt;

&lt;p&gt;Let us consider N=150,450 and observe the results.
&lt;img src=&quot;/pyVision/images/blog/images/image7.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At we increase the period of the signal we can see that resolution in the freuency domain increases.&lt;/p&gt;

&lt;p&gt;As $N \rightarrow \infty$ ,the samples in the freuency domain will be placed closer and closer .
If samples in frequency domain are spaced infinitely closely,it can be considered a continuous signal.
This gives us Discrete Time Fourier Transform representation of the signal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/image8.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
def FourierRect(N):     
        &quot;&quot;&quot; the function generates rectangular aperiodic pulse and computer the DFT coefficients
        
        Parameters
        ----------
        N : period of aperiodic signal
             
        &quot;&quot;&quot;
        x = np.zeros((1,N))
        x[:,0:30]=1;
        x=x.flatten();
    
        
        #compute the DFT coefficients
        r1=FourierSeries(x)
        #magnitude of DFT coefficients
        a1=cabs(r1)

        #plot the time domain signal
        subplot(2,1,1)
        plt.plot(range(0,len(x)),x)
        xlabel('Time')
        ylabel('Amplitude')
        title('time doman')
        plt.ylim(-2,2);
        
        #plot the DFT coefficients
        L=len(a1);
        fr=np.arange(0,L);
        subplot(2,1,2)
        plt.stem(fr,a1,'r') # plotting the spectrum
        xlabel('Freq (Hz)')
        ylabel('|Y(freq)|')
        title('complete signal')
        ticks=np.arange(0,L+1,25);
        plt.xticks(ticks,ticks);     
        show() 

    if mode==4:
       FourierRect(150);

    if mode==5:
       FourierRect(150*3);        
&lt;/pre&gt;
&lt;p&gt;It samples in frequency domain are spaced infinitely closely,it can be considered a continuous signal.
This representation of is called as Fourier Transform.&lt;/p&gt;

\[X(k) = \sum x[n] exp(-j\omega k n)\]

&lt;p&gt;distance between adjacent samples is $\omega$ .As $ w \rightarrow 0 $ samples are placed infinitely closely with each other .&lt;/p&gt;

\[X(w) = \sum x[n] exp(-j\omega  n)\]

&lt;p&gt;Fourier transform is continuous in nature and cannot be used for numeral computation .&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discrete Fourier transform&lt;/strong&gt; is sampled version of Discrete Time Fourier transform of a  signal and in in a form that is suitable for numerical computation on a signal processing unit.&lt;/p&gt;

&lt;p&gt;A fast Fourier transform (FFT) is an algorithm to compute the discrete Fourier transform (DFT) and its inverse.It is a efficient way to compute the DFT of a signal.&lt;/p&gt;

&lt;p&gt;we will use the  python FFT routine can compare the performance with naive implementation&lt;/p&gt;

&lt;p&gt;Using the inbuilt FFT routine :Elapsed time was 6.8903e-05 seconds&lt;/p&gt;

&lt;p&gt;Using the naive code :Elapsed time was 0.0653119 seconds&lt;/p&gt;

&lt;p&gt;we can see improvement of order of 1000&lt;/p&gt;

&lt;p&gt;The naive algorithm has complexity of $o(N^2)$ and FFT algorithm as complexity of $O(N log N)$&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    if mode==6:
        Fs=150;
        F=range(1,10);
        F=np.array(F);        
        w=numpy.ones(F.shape);

        
        Ts=1.0/Fs;   
        xs=numpy.arange(0,1,Ts) 
    
        signal=numpy.zeros(np.shape(xs));
        for i in range(len(F)):
            omega=2*np.pi*F[i];
            signal = signal+ w[i]*numpy.cos(omega*xs);        
            
            
        start_time = time.time()
        FourierSeries(signal)
        end_time = time.time()
        print(&quot;Elapsed time naive algo  %g seconds&quot; % (end_time - start_time)) 

        start_time = time.time()
        fft(signal)
        end_time = time.time()
        print(&quot;Elapsed time of fft algo  %g seconds&quot; % (end_time - start_time)) 
&lt;/pre&gt;

&lt;h4 id=&quot;circular-convolution-and-dft&quot;&gt;Circular Convolution and DFT&lt;/h4&gt;

&lt;h5 id=&quot;circular-shift-of-a-sequence&quot;&gt;Circular Shift of a sequence&lt;/h5&gt;
&lt;p&gt;Let $x[n]$ denote the finite length time domain sequence&lt;/p&gt;

&lt;p&gt;Once we take DFT ,in time domain we are constructing the periodic extension of the signal
Thus a time shit of signal is actually implies circular shit of the signal.&lt;/p&gt;

&lt;p&gt;\(x[n] \Leftrightarrow X[k]\)
\(x[(n-n\_{o})\_{N}]  \Leftrightarrow e^{-j k \omega n\_{o}} X[k]\)&lt;/p&gt;

&lt;p&gt;One of the most basic application in signal processing is linear convolution.&lt;/p&gt;

&lt;p&gt;It can be used to represents the output of discrete time LTI system,correlation and cross correlation,
filtering and host of other signal processing operations.&lt;/p&gt;

&lt;p&gt;Linear Convolution in discrete time system is represented as&lt;/p&gt;

\[y[n]=\sum\_{i} x[i] h[n-i]\]

&lt;p&gt;The signals $x[n]$ and $h[n]$ are finite duration discrete time signals .&lt;/p&gt;

&lt;p&gt;Linear convolution of 2 sequences of length M,L  results in sequence of length M+L-1&lt;/p&gt;

&lt;p&gt;An associated discrete time Fourier transform property&lt;/p&gt;

\[Y\left(\Omega\right) = X\left(\Omega\right)H\left(\Omega\right)\]

&lt;p&gt;Let us consider 2 sequences,take their DFT,multiply them and then take the inverse
&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Circular convolution in time domain leads to multiplication of DFT coefficients in the frequency domain&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;lets take the seuences &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1,1,1,1,1]&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[1,2,3,4]&lt;/code&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
The result of linear convolution is `[ 1  3  6 10 10  9  7  4 ]`
The result of inverse of product of DFT coefficients is `[ 10.  10.  10.  10.  10.]`
&lt;/pre&gt;

&lt;p&gt;we take N point DFT of signals,N=max(M,L)&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
...|1 1 1 1 1|1 1 1 1 1|1 1 1 1 1|....
...|1 2 3 4 0|1 2 3 4 0|1 2 3 4 0|....
Result 
...|10|10|10| ...
&lt;/pre&gt;

&lt;p&gt;Taking the DFT leads to periodicity in time domain,now we perform convolution as usual.
the $h[n-n_{o}]$ will be circularly shifted&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
...|1 1 1 1 1|1 1 1 1 1|1 1 1 1 1|....
...|0 1 2 3 4|0 1 2 3 4|0 1 2 3 4|....

Result 
...|10|10|10| ...
&lt;/pre&gt;
&lt;pre class=&quot;brush : python &quot;&gt;
...|1 1 1 1 1|1 1 1 1 1|1 1 1 1 1|....
...|4 0 1 2 3 |4 0 1 2 3|4 0 1 2 3|....

Result 
...|10|10|10| ...
&lt;/pre&gt;

&lt;p&gt;After N shifts we are repeating the sequence
Thus we perform the convolution of N shifts equivalent to the period of sequence in time domain&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
Circular convolution
[ 10.  10.  10.  10.  10.]
&lt;/pre&gt;

&lt;p&gt;Circular shift  is consequence of periodicity introduced by the DFT  and results in circular convolution operation&lt;/p&gt;

&lt;p&gt;We can consider this due to the time aliasing being introduced due to sampling in the frequency domain.
The sampling theorem states that to avoid aliasing in frequency domain the sampling rate must be greater than twice the maximum frequency of signal being sampled.&lt;/p&gt;

&lt;p&gt;We had mentioned earlier that DFT is the sampled version of DTFT .The question arises that how do we increase the sampling rate to avoid time domain aliasing.&lt;/p&gt;

&lt;p&gt;We saw that zero padding the sequence leads to samples of Fourier series are placed more closely together.Equivalent to saying increases the sampling rate of DTFT in frequency domain,&lt;/p&gt;

&lt;p&gt;This gives us a intuition that if we zero pad the sequence,it will lead to increased sampling rate of the DTFT of the signal.&lt;/p&gt;

&lt;p&gt;For the result of circular convolution to be equal to the linear convolution,we simply zero pad the sequences to length of M+L-1.&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
...|1 1 1 1 1 0 0 0 0 |1 1 1 1 1 0 0 0 0 |
...|1 2 3 4 0 0 0 0 0 |1 2 3 4 0 0 0 0 0 |

result : 10

...|1 1 1 1 1 1 0 0 0 |1 1 1 1 1 0 0 0 0 |
...|0 1 2 3 4 0 0 0 0 |0 1 2 3 4 0 0 0 0 |

result : 10

...|1 1 1 1 1 1 0 0 0 |1 1 1 1 1 0 0 0 0 |
...|0 0 1 2 3 4 0 0 0 |0 0 1 2  3 4 0 0 0 |

result : 9
&lt;/pre&gt;

&lt;p&gt;thus we can see that due to zero padding,the circular shifted components are zeros
and the result obtained is equivalent to linear convolution result.&lt;/p&gt;

&lt;p&gt;The code for the above example is given below&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    if mode == 7:
        x=np.array([1,1,1,1,1]);
        h=np.array([1,2,3,4,0]);
        r=np.convolve(x,h)
        print 'Linear convolution'
        print r

		#take 5 point DFT of the sequence
        f1=fft(x,5);
        f2=fft(h,5);
        s=ifft(f1*f2);
        print 'Circular convolution'
        print abs(s)
		
		#take 9 point DFT of sequence
        f1=fft(x,9);
        f2=fft(h,9);
        s=ifft(f1*f2);
        print 'zero padded Circular convolution'
        print r
&lt;/pre&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;
&lt;p&gt;The code for all the examples can be found at github repository
&lt;a href=&quot;https://github.com/pi19404/pyVision/tree/master/pySignalProc/tutorial/fourierSeries.py&quot;&gt;https://github.com/pi19404/pyVision/tree/master/pySignalProc/tutorial/fourierSeries.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can change the mode variable in the file from 1-7 to generate all the plots shown in the article&lt;/p&gt;

&lt;p&gt;The file fourierSeries.py can also be downloaded from below link&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.codeproject.com/KB/Articles/828166/pySignalProc.zip&quot;&gt;Download Link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Multilayer Perceptron in Python</title>
   <link href="http://localhost:4000/2014/10/03/test/"/>
   <updated>2014-10-03T00:00:00+00:00</updated>
   <id>http://localhost:4000/2014/10/03/test</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this article we will look at supervised learning algorithm called Multi-Layer Perceptron (MLP) and implementation of single hidden layer MLP&lt;/p&gt;

&lt;p&gt;###Perceptron&lt;/p&gt;

&lt;p&gt;A perceptron is a  unit that computes a single output from multiple real-valued inputs by forming a linear combination according to its input weights and then possibly putting the output through some nonlinear function called the activation function&lt;/p&gt;

&lt;p&gt;Below is a figure illustrating the operation of perceptron&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/images1.jpg&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSPBshuqpGJBgvzx9ECppUv6QBg7ipgPH4XDEle3gZVn3Ku56MT&quot;&gt;figure taken from&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The output of perceptron can be expressed as&lt;/p&gt;

&lt;p&gt;$f(x) = G( W^T x+b)$&lt;/p&gt;

&lt;p&gt;$x$ is the input vector 
$(W,b)$ are the parameters of perceptron 
 $f$ is the non linear function&lt;/p&gt;

&lt;h3 id=&quot;multi-layer-perceptron&quot;&gt;Multi Layer Perceptron&lt;/h3&gt;
&lt;p&gt;The MLP network consists of input,output and hidden layers.Each hidden layer consists of numerous perceptron’s which are called hidden units&lt;/p&gt;

&lt;p&gt;Below is figure illustrating a feed forward neural network architecture for Multi Layer perceptron&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/mlp1.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;a href=&quot;http://www.deeplearning.net/tutorial/_images/mlp.png&quot;&gt;(figure taken from)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A single-hidden layer MLP  contains a array of perceptrons .
The output of hidden layer of MLP can be expressed as a function&lt;/p&gt;

&lt;p&gt;$f(x) = G( W^T x+b)$&lt;/p&gt;

&lt;p&gt;$f: R^D \rightarrow R^L$, 
where D is the size of input vector $x$ 
$L$ is the size of the output vector
$G$ is activation function.&lt;/p&gt;

&lt;p&gt;In case the activation function G is a sigmoid function then a single-layer MLP consisting of just the output layer is equivalent to a logistic classifier&lt;/p&gt;

&lt;p&gt;$\begin{align} f_{i}(x)=\frac{e^{W_{i}x+b_{i}}}{\sum_{j} e^{W_{j}x+b_{j}}} \end{align}$&lt;/p&gt;

&lt;p&gt;Each unit of input layer corresponds to element of input vector.
Each output unit of logistic classifier generate a prediction probability that input vector belong to a specified class.&lt;/p&gt;

&lt;p&gt;##Feed Forward Neural Network 
Let us first consider the most classical case of a single hidden layer neural network&lt;/p&gt;

&lt;p&gt;The number of inputs to hidden layer is $(d)$ and number of outputs of hidden layer are $(m)$
The hidden layer performs mapping  of vector of dimensionality $d$ to vector of dimensionality $m$.&lt;/p&gt;

&lt;p&gt;Each unit of hidden layer of a MLP can be parameterized by a  weight matirx and bias vector  $(W,b)$ and a activation function $(\mathcal{G})$.The output of a hidden layer is activation function applied to linear combination of input and weight vector.&lt;/p&gt;

&lt;p&gt;Dimensionality of weight matrix and bias vector are determined by desired number of output units.
If the number of  inputs to hidden layer/dimensionality of input is $\mathcal{M}$ and number of outputs is $\mathcal{N}$ then dimensionality of weight vector in $\mathcal{NxM}$ and that of  bias vector is $\mathcal{N}x1$.&lt;/p&gt;

&lt;p&gt;We can consider that hidden layer consists of $\mathcal{N}$ hidden units ,each of which accepts a $\mathcal{M}$ dimensional vector and produces a single output.&lt;/p&gt;

&lt;p&gt;The output is the affine transformation of the input layer followed by the appplication of function $f(x)$ ,which is typically a non linear function like sigmoid of inverse tan hyperbolic function.&lt;/p&gt;

&lt;p&gt;The vector valued function  $h(x)$  is the output of the hidden layer.&lt;/p&gt;

\[h(x) = f(W^T x + c )\]

&lt;p&gt;The output layer of MLP is typically Logistic regresson classifier,if probabilistic outputs are desired for classification purposes in which case the activation function is the softmax regression function.&lt;/p&gt;

&lt;p&gt;###Single Hidden Layer Multi Layer Perceptron’s
Let ,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$h_{i-1}$ denote the input vector to the i-th  layer&lt;/li&gt;
  &lt;li&gt;$h_{i}$ denote the output vector of the i-th layer.&lt;/li&gt;
  &lt;li&gt;$h_{0}$=x is vector that represents input layer&lt;/li&gt;
  &lt;li&gt;$h_{n}=y$ is output layer which produces the desired prediction output.&lt;/li&gt;
  &lt;li&gt;$f(x)$ denote the activation function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus we denote the output of each hidden layer as&lt;/p&gt;

&lt;p&gt;$h_{k}(x) = f(b_{k} + w_{k}^T h_{i-1}(x)) = f(a_{k}) $&lt;/p&gt;

&lt;p&gt;Considering sigmoid activation function,gradient of fundtion wrt arguments can be written as&lt;/p&gt;

&lt;p&gt;$\begin{align} \frac{\partial \mathbf{h}_{k}(x)  }{\partial \mathbf{a}_{k}}=  f(a_{k})(1- f(a_{k})) \end{align}$&lt;/p&gt;

&lt;p&gt;The computation associated with each hidden unit $(i)$ of the layer can be denoted as&lt;/p&gt;

\[h\_{k,i}(x) = f(b\_{k,i} + W\_{k,i}^T h\_{i-1}(x)) = f(a\_{k}(x))\]

&lt;p&gt;The output layer is a Logistic regression classifier.The output is a probabilistic output denoting the confident that input belongs to the predicted class.The cost function defined for the same is defined as negative log likelyhood over the training data&lt;/p&gt;

\[L = -log (p\_{y})\]

&lt;p&gt;The idea is to maximize $p_{y}= P( Y =y_{i} | x )$ as estimator of conditional probability of the class $y$ given that input is $x$.This is the cost function for training algorithm.&lt;/p&gt;

&lt;p&gt;###Back-Propagation Algorithm&lt;/p&gt;

&lt;p&gt;The Back-Propagation Algorithm is recursive gradient algorithm used to optimize the parameters MLP wrt to defined loss function.Thus our aim is that each layer of MLP the hidden units are computed so that cost function is maximized.&lt;/p&gt;

&lt;p&gt;Like in logistic regression we compute the gradients of weights wrt to the cost function . The gradient of the cost function wrt all the weights in various hidden layers are computed.Standard gradient based optimization is performed to obtain the parameters that will minimize the likelihood function.&lt;/p&gt;

&lt;p&gt;The output layer determines the cost function.Since we are using Logistic regression as output layer.The cost function is the softmax function.Let L denote the cost function.&lt;/p&gt;

&lt;p&gt;There is nothing different we do in backpropagation algorithm that any other optimization techniue.The aim is to determine how the weights and biases change in the network&lt;/p&gt;

&lt;p&gt;$ \begin{align} \frac{\partial L}{\partial W_{k,i,j} } \text{ and } \frac{\partial L}{\partial b_{k,i,j} } \end{align}$.&lt;/p&gt;

&lt;h4 id=&quot;output-layer&quot;&gt;output layer&lt;/h4&gt;

&lt;p&gt;$\begin{align} L = -log ( f(a_{k,i}) ) \end{align}$&lt;/p&gt;

&lt;p&gt;$\begin{align} \frac{\partial L  }{\partial \mathbf{a}_{k,i}} = \frac{\partial L  }{\partial \mathbf{h}_{k,i}} \frac{\partial \mathbf{h}_{k,i} }{\partial \mathbf{a}_{k,i}} = -\frac{1}{h_{k,i}} * h_{k,i}*(1-h_{k,i}) = (h_{k,i}-1)\end{align}  $&lt;/p&gt;

&lt;p&gt;$ \begin{align} \frac{\partial L  }{\partial \mathbf{a}_{k,i}} =\mathbf{h}_{k,j} - 1_{y=y_{i}} \end{align}$&lt;/p&gt;

&lt;p&gt;The above expression can be considered as the error in output.When $y=y_{i}$ the error is $(1-p_{i})$ and then $y \ne y_{i}$ the error in prediction is $p_{i}$.&lt;/p&gt;

&lt;h4 id=&quot;hidden-layer&quot;&gt;hidden layer&lt;/h4&gt;

&lt;p&gt;$\begin{align}\frac{\partial L }{\partial \mathbf{a}_{k-1,j}} =  \frac{\partial L }{\partial \mathbf{h}_{k-1,j}} \frac{\partial \mathbf{h}_{k-1,j} }{\partial \mathbf{a}_{k-1,j}} \end{align}$&lt;/p&gt;

&lt;p&gt;Thus the idea is to start computing gradients from the bottom most layer.To compute the gradients of the cost function wrt parameters at the i-th layer we need to know the gradients of cost function wrt parameters at $(i+1)$th layer.&lt;/p&gt;

&lt;p&gt;We start with gradient computation at the logistic classifier level.The propagate backwards,updating the parameters at each layer&lt;/p&gt;

&lt;p&gt;Let us consider the case of other other hidden layers&lt;/p&gt;

&lt;p&gt;$\begin{align} \frac{\partial L }{\partial \mathbf{h}_{k-1,j}} = \sum_{i} \frac{\partial L }{\partial \mathbf{a}_{k,i}}\frac{\partial \mathbf{a}_{k,i} }{\partial \mathbf{h}_{k-1,j}} = \sum_{i} \frac{\partial L }{\partial \mathbf{a}_{k,i}} W_{k,i,j}  \end{align} $&lt;/p&gt;

&lt;p&gt;The implementation of the above equation&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    def linear_gradient(self,weights,error):   
            &quot;&quot;&quot; The function compues gradient of likelihood function wrt output of hidden layer
            :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{h}\_{k-1,j}} \\end{align}`
            
            Parameters 
            ------------
            weights : ndarray,shape=(n_out,n_hidden)
                      weights of next hidden layer, :math:`\\begin{align} \mathbf{W}\_{k,i,j}  \\end{align}`
                      
            error   : ndarray,shape=(n_out,)
                      backpropagated error from next layer :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{a}\_{k,i}} \\end{align}`
        
            Returns 
            -----------     
            out : ndarray,shape=(n_hidden,)                
                  compute the backpropagated error, :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{h}\_{k-1,j}} \\end{align}`
            &quot;&quot;&quot;            
            
            return numpy.dot(error,weights);
&lt;/pre&gt;

&lt;p&gt;The gradients computation of parameters of hidden layers is as follows&lt;/p&gt;

&lt;p&gt;$\begin{align}\frac{\partial L }{\partial \mathbf{W}_{k-1,i,j}} =  \frac{\partial L }{\partial \mathbf{a}_{k-1,j}} \frac{\partial \mathbf{a}_{k-1,j} }{\partial \mathbf{W}_{k-1,i,j}}=\frac{\partial L }{\partial \mathbf{a}_{k-1,j}} \mathbf{h}_{k-2,j} \end{align}$&lt;/p&gt;

&lt;p&gt;$\begin{align}\frac{\partial L }{\partial \mathbf{b}_{k-1,i}} =  \frac{\partial L }{\partial \mathbf{a}_{k-1,i}} \frac{\partial \mathbf{a}_{k-1,i} }{\partial \mathbf{b}_{k-1,i}}=\frac{\partial L }{\partial \mathbf{a}_{k-1,i}}  \end{align}$&lt;/p&gt;

&lt;p&gt;This is implemented as below ,where the input&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$x$ represents $\begin{align} \frac{\partial \mathbf{h}_{k,j} }{\partial \mathbf{a}_{k,j}} \end{align}$ -output gradient&lt;/li&gt;
  &lt;li&gt;$y$ represents $\begin{align} h_{k-2,j} \end{align}$ -activation&lt;/li&gt;
  &lt;li&gt;$w$ represents $\begin{align} \frac{\partial L }{\partial \mathbf{a}_{k-1,i}}\end{align}$ -error&lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
   
    def compute_error(self,x,w,y):      
        &quot;&quot;&quot;                 
        function computes the gradient of the likelyhood function wrt to parameters  of the hidden layer for single input
        

        Parameters 
        -------------
        x : ndarray,shape=(n_hidden,)

        w : ndarray,shape=(n_hidden,)
            `w` represents :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{h}\_{k,i}}\end{align}` the gradient of the likelyhood fuction wrt output of hidden layer
            
        y : ndarray,shape=(n_in,)
            `y` represents :math:`\mathbf{h}\_{k-2,j}` the input hidden layer
        
        Returns
        ------------
        res : ndarray,shape=(n_in+1,n_hidden)        
              :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{W}\_{k-1,i,j}}  \\text{ and } \\frac{\partial L }{\partial \mathbf{W}\_{k-1,i}} \end{align}`
        &quot;&quot;&quot;        
       
        
        x=x*w;                
        #gradient of likelyhood function wrt input activation
        res1=x.reshape(x.shape[0],1);
        #gradient of likelyhood function wrt weight matrix
        res=np.dot(res1,y.reshape(y.shape[0],1).T);
        self.eta=0.0001
        #code for L1 and L2 regularization 
        if self.Regularization==2:
           res=res+self.eta*self.W;
        if self.Regularization==1:
           res=res+self.eta*np.sign(self.W);

        #stacking the parameters and preparing for returning            
        res=np.hstack((res,res1));
        return res.T;


    def cost_gradients(self,weights,activation,error):        
        &quot;&quot;&quot; function to compute the gradient of log 
        likelyhood function wrt the parameters of the hidden layer
        averaged over all the input samples.        
        
        Parameters 
        -------------
        weights : numpy,shape(n_out,n_hidden),
                  weight matrix of the next layer,W\_{k,i,j} 
                  
                  
        activation: numpy,shape=(N,n_in)
                    input to the hidden layer \mathbf{h}\_{k-2,j}
                    
        error : numpy,shape=(n_out,) 
                 \frac{\partial L }{\partial \mathbf{a}\_{k,i}}
        
        Returns
        
        -------------
        gW : ndarray,shape=(n_hidden,n_in+1)
             coefficient parameter matrix of next hidden layer,
             :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{W}\_{k-1,i,j}}  \\text{ and } \\frac{\partial L }{\partial \mathbf{W}\_{k-1,i}} \end{align}`
        &quot;&quot;&quot;                                       
        we=self.linear_gradient(weights,error)
        ag=self.activation_gradient()
        e=[ self.compute_error(a,we,b) for a,b in izip(ag,activation)]
        gW=np.mean(e,axis=0).T        
        return gW;        
&lt;/pre&gt;

&lt;p&gt;Once we have the gradients and have computed the new parameters,the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;function update&lt;/code&gt; is called to updated the new parameters in the model.&lt;/p&gt;

&lt;p&gt;This function is called by the Optimizer module that performs SGD based optimizations,all the optimization parameters like learning rate are handled by the optimizer methods.&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    def update_parameters(self,params):
        &quot;&quot;&quot; function to updated the learn parameters to the model
        
        Parameters
        ----------
        grads : ndarray,shape=(n_hidden,n_in+1)        
                coefficient parameter matrix                
        
        &quot;&quot;&quot;
        
        self.params=params;
        param1=self.params.reshape(-1,self.nparam);
        self.W=param1[:,0:self.nparam-1];
        self.b=param1[:,self.nparam-1];
        
&lt;/pre&gt;

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h3&gt;
&lt;p&gt;The class HiddenLayer encapsulates all the methods for prediction,classification,training,gradient computation and error propagation that are required&lt;/p&gt;

&lt;p&gt;The important attributes of the HiddenLayer class are&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    Attributes        
    -----------
    `out` : array-like ,shape=[n_out]
    The output of hidden layer 
    
    `params`:array-like ,shape=[n_out,n_in+1]        
     parameters of hidden layer
    
    `W,b`:array-like,shape=[n_out,n_int],shape=[n_out,1]
     parameters in the form of weight matrix and bias vector characterizing 
     the hidden layer
     
     `activation`:function
     the non linear activation function
     
    .. note :
    in the below functions to n_hidden denotes the number of output units of present hidden layer
    n_out denotes the number of output units of next hidden layer
    and n_in denotes the size of input vector to present hidden layer
    
    def compute(self,input):
        &quot;&quot;&quot;function computes the output of the hidden layer for input matrix
      
        Parameters
        ----------
        input   :   ndarray,shape=(N,n_in)
                    :math:`h\_{i-1}(x)` is the `input`

        Returns
        -----------
        output  : ndarray ,shape=(N,n_out)
                    :math:`f(b_k + w_k^T h\_{i-1}(x))` ,affine transformation over input
        &quot;&quot;&quot;                
        #performs affine transformation over input vector        
        linout=numpy.dot(self.W,input.T)+np.reshape(self.b,(self.b.shape[0],1));     
        #applies non linear activation function over computed linear transformation
        self.output=self.activation(linout).T;                 
        return self.output;

&lt;/pre&gt;

&lt;p&gt;A class MLP  encapsulates all the methods for prediction,classification,training,forward and back propagation,saving and loading models etc.
Below 3 important functions are displayed.The learn function is called at every optimizer loop.
This calls the forward and backward iteration methods and updated the parameters of each hidden layer&lt;/p&gt;

&lt;p&gt;the forward iteration simply computes the output of network and while propagate_backward fuctions
is responsible for passing suitable inputs and weights to each hidden layer so that it can execute the backward algorithm loop&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;

               
   def propagate_backward(self,error,weights,input):                 
        &quot;&quot;&quot; the function that executes the backward propagation loop on hidden layers
                
        Parameters 
        ----------------
        error : numpy array,shape=(n_out,)
                average prediction error over all the input samples in output layer
                :math:`\\begin{align}\frac{\partial L  }{\partial \mathbf{a}\_{k,i}} \\end{align}`


        weight : numpy array,shape=(n_out,n_hidden)        
                 parameter weight matrix of the output layer
        
        
        input : ndarray,shape=(n_samples,n_in)
                input training data
        Returns
        ----------------
        None 
        
        &quot;&quot;&quot;              


        #input matrix for the hidden layer    
        input1=input;
        for i in range(self.n_hidden_layers):                        
            prev_error=np.inf;
            best_grad=[];
            for k in range(1):
                &quot;&quot;&quot; computing the derivative of the parameters of the hidden layers&quot;&quot;&quot;
                hidden_layer=self.hiddenLayer[self.n_hidden_layers-i-1];
                hidden_layer.compute(input1);
          
                # computing the gradient of likelyhood function wrt the parameters of the hidden layer 
                grad=hidden_layer.cost_gradients(weights,input1,error);
                #update the parameter of hidden layer
                res=self.update(hidden_layer.params,grad.flatten(),0.13);
            
                &quot;&quot;&quot; update the parameters &quot;&quot;&quot;
                hidden_layer.update_parameters(res);
            #set the weights ,inputs and error required for the back propagation algorithm
            #for the next layer
            weights=hidden_layer.W;
            error=grad[:,hidden_layer.n_in];                                    
            self.hiddenLayer[self.n_hidden_layers-i-1]=hidden_layer;
            input1=hidden_layer.output;

   def propagate_forward(self,input):
       &quot;&quot;&quot;the function that performs forward iteration to compute the output
        
       Parameters
       -----------
       input : ndarray,shape=(n_samples,n_in)
               input training data
       
       &quot;&quot;&quot;
       self.predict(input)

                
  
   def learn(self,update):
        &quot;&quot;&quot; the main function that performs learning,computing gradients and updating parameters 
            this is called by the optimizer module for each iteration
        
        Parameters
        ----------
        update - python function
                 this represents the update function that performs the gradient descent iteration
        &quot;&quot;&quot;
        #set the training data
        x,y=self.args;
        #set the update function
        self.update=update;                        
        #execute the forward iteration loop
        self.propagate_forward(x)  
        #set the input for output layer
        args1=(self.hidden_output,y);
        #set the input for the output logistic regression layer
        self.logRegressionLayer.set_training_data(args1);
        #gradient computation and parameter updation of output layer
        [params,grad]=self.logRegressionLayer.learn(update);
        self.logRegressionLayer.update_params(params);
       
        #initialize the gradiients and weights for backward error propagation
        error=grad;
        weights=self.logRegressionLayer.W;
        
        #perform the backward iteration over the hidden layers
        if self.n_hidden_layers &amp;gt;0:   
             weights=self.logRegressionLayer.W;
             self.propagate_backward(error,weights,x)
             
        return [None,None];                        
&lt;/pre&gt;

&lt;h3 id=&quot;selecting-the-parameters-of-the-model&quot;&gt;Selecting the parameters of the model&lt;/h3&gt;
&lt;p&gt;As mentioned earlier that MLP consits of input,hidden and output layers.There is not fixed rule to determine the number of hidden units.The parameters are application specific and best parameters are often arrived at by emperical testing process.Less number of hidden units leads to increased generalization and training error while having a large number of training units leads to issues of with training of large number of parameters and significantly large training time.&lt;/p&gt;

&lt;h3 id=&quot;issues-with-mlp&quot;&gt;Issues with MLP&lt;/h3&gt;
&lt;p&gt;One of the issues observed in MLP training is the slow nature of learning.The below figure illustrates the nature of learning process when a small learning parameter or improper regularization constant is chosen.Various adaptive methods can be implemented which can improve the performance ,but slow convergence and large learning times is an issue with Neural networks based learning algorithms.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/save.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;
&lt;p&gt;The important files related to MLP are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MLP.py&lt;/li&gt;
  &lt;li&gt;LogisticRegression.py&lt;/li&gt;
  &lt;li&gt;Optimizer.py&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The latest version of the code can be found in github repository &lt;a href=&quot;https://www.github.com/pi19404/pyVision&quot;&gt;www.github.com/pi19404/pyVision&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The files used in the current article can be downloaded from below link&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pi19404/pyVision/archive/pyVision_alpha0.002.zip&quot;&gt;Github Release&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The dataset and model file can be found under the models and data repository&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MLP.pyvision - model file&lt;/li&gt;
  &lt;li&gt;mnist.pkl.gz - data file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;make suitable changes to the path in MLP.py file before running the code.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Jekyll A Static Website Generator</title>
   <link href="http://localhost:4000/linux/2014/10/03/Jekyll/"/>
   <updated>2014-10-03T00:00:00+00:00</updated>
   <id>http://localhost:4000/linux/2014/10/03/Jekyll</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this article we will look at &lt;strong&gt;Jekyll&lt;/strong&gt; static site generator to generate a static website and host the same on &lt;strong&gt;github&lt;/strong&gt;  using github pages.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;Jekyll is a simple, blog aware, static site generator.&lt;/p&gt;

&lt;p&gt;A static site generator is a utility that generates ready-to-publish static HTML pages  from a set of files usually in markdown or HTML which are suitable for deployment directory on any web-server .The blog-aware means that it can support and maintain website with content added in series like that of blogs.&lt;/p&gt;

&lt;p&gt;Jekyll is the engine behind GitHub Pages, which enables us to use Jekyll to host  the project’s page, blog, or website from GitHub’s servers for free&lt;/p&gt;

&lt;h2 id=&quot;github-pages-and-jekyll-installation&quot;&gt;Github Pages and Jekyll Installation&lt;/h2&gt;

&lt;p&gt;Let us consider the github project &lt;a href=&quot;https://github.com/pi19404/pyVision&quot;&gt;www.github.com/pi19404/pyVision.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github pages generator gives us the facility to create a website for the project using one of the standard defined themes .This is the easiest way to start a project without having to worry about the HTML themes and CSS files etc and just focus on content writing.&lt;/p&gt;

&lt;p&gt;To generate a static webpage of the project go to project settings on github.com
&lt;img src=&quot;/pyVision/images/blog/images/ss5.png&quot; alt=&quot;enter image description here&quot; /&gt;
Click on the &lt;strong&gt;&lt;em&gt;Automatic page generator&lt;/em&gt;&lt;/strong&gt; options&lt;/p&gt;

&lt;p&gt;This will provide you with a markdown editor where you can enter the contents for the main page of the website.&lt;/p&gt;

&lt;p&gt;Enter the name ,content and other details and click on &lt;strong&gt;&lt;em&gt;“Continue to Layouts Button”&lt;/em&gt;&lt;/strong&gt; to proceed to HTML theme selection
&lt;img src=&quot;/pyVision/images/blog/images/ss6.png&quot; alt=&quot;enter image description here&quot; /&gt;
Select the desired theme and click on &lt;strong&gt;&lt;em&gt;“Publish Page”&lt;/em&gt;&lt;/strong&gt; button to complete the process
&lt;img src=&quot;/pyVision/images/blog/images/ss7.png&quot; alt=&quot;enter image description here&quot; /&gt;
This will lead to github servers hosting the created website on “http://pi19404.github.io/pyVision”&lt;/p&gt;

&lt;p&gt;now we are ready to modify and edit the website.The sources of the website are also stored in github repository of the  project in the gh-pages branch.&lt;/p&gt;

&lt;p&gt;Thus all we have to do to access the sources is to checkout the gh-pages branch and start modifying content on local server and then push changes onto remote github repository.The github servers will regenerated the website and latest changes will be reflected on your website.&lt;/p&gt;

&lt;p&gt;The same process can be used to maintain blogs,post or any other content of the website.&lt;/p&gt;

&lt;p&gt;Every GitHub Page is run through Jekyll when you push content to gh-pages branch within your repository&lt;/p&gt;

&lt;h2 id=&quot;installing-jekyll&quot;&gt;Installing Jekyll&lt;/h2&gt;

&lt;p&gt;Though Jekyll installation on local PC is not necessary.It can be installed in order to preview the website  and troubleshoot issues or bugs before pushing the site on GitHub Pages.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ruby - Jekyll requires the Ruby version 1.9.3 or higher.
    &lt;ul&gt;
      &lt;li&gt;To install ruby easiest way is to download ruby install managers like &lt;strong&gt;&lt;em&gt;“rvm - The Ruby Version Manager”&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Detailed installation instruction for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rvm&lt;/code&gt; can be found at &lt;a href=&quot;http://rvm.io/rvm/install&quot;&gt;http://rvm.io/rvm/install&lt;/a&gt;  or install the software from package manager like synaptic&lt;/li&gt;
      &lt;li&gt;To install Ruby : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rvm install 2.5.2&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;This will download the ruby 2.5.2 sources and perform compilation and deployment  at “/usr/share/ruby-rvm/”&lt;/li&gt;
      &lt;li&gt;In some cases you many encounter compilation issues due to outdated version of OpenSSL,in which case install the openssl from rvm tool : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rvm pkg install openssl&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;This will install the openssl packaged within the rvm installation directory&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;While compiling ruby we can given commandline arguments so that it referes the openSSL package from the rvm install directory and not default system path :&lt;/p&gt;

        &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rvm install 2.5.2 --with-openssl-dir=/usr/share/ruby-rvm/usr&lt;/code&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bundler - Bundler is a package manager ,This can be installed as easily&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gem install jekyll bundler&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; **gem install bundler**  - Jekyll installation
 - Clone the sites repository on the local machine
 - Change the branch to gh-pages if you have created a project website
 - Create a file called GemFile in the directory with the following content
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;pre class=&quot;brush : html &quot;&gt;
                 source 'https://rubygems.org'
                 gem 'github-pages'
&lt;/pre&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; - Jekyll requires  a JavaScript runtime . You can install `xecjs` and `therubyracer gems`, or install `nodej` .
 - Run the command : `bundle install` for installing Jekyll
 - To ensure that local development environment is same as that of github regularily update the local environment 
 
   `bundle update github-pages `
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Running Jekyll : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle exec jekyll serve&lt;/code&gt;
 The website is accessible for preview at : http://localhost:4000&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;layouts-and-website-template&quot;&gt;Layouts and Website Template&lt;/h2&gt;
&lt;p&gt;Front-matter is just a set of metadata, delineated by three dashes which takes for form of valid YAML content.Any file that contains a front matter block will be processed by Jekyll as a special file.
Thus .Jekyll requires that Markdown files have front-matter defined at the top of every file.The front matter can be included in top of markdown or HTML files.&lt;/p&gt;

&lt;p&gt;Between the dashed lines you can set predefined variables ( title,layout) or set custom used defined variables.&lt;/p&gt;

&lt;p&gt;The present article was written using Markdown syntax and frontmatter included on top of the files was&lt;/p&gt;

&lt;pre class=&quot;brush : html &quot;&gt;
---
layout: post
title: Jekyll A Static Website Generator
---
Introduction
-------------
In this article we will look at **Jekyll** static site generator to generate a static website and host the same on **github**  using github pages.
&lt;/pre&gt;

&lt;p&gt;The layout tag specifies the template to be used for generating posts.The templates can be makrdown containing HTML contents.&lt;/p&gt;

&lt;p&gt;Jekyll uses the &lt;a href=&quot;https://github.com/shopify/liquid/wiki/liquid-for-designers&quot;&gt;Liquid template system&lt;/a&gt; .The variables defined in front matter and page contents can be accessed  accessed using the Liquid tags both within the files as well as any layouts that page of post relies on.&lt;/p&gt;

&lt;p&gt;Let us look at the template for post called &lt;strong&gt;post.html&lt;/strong&gt; and how contents are incorporated using Liquid markup language&lt;/p&gt;

&lt;pre class=&quot;brush : html &quot;&gt;
---
layout: default
---
&lt;article class=&quot;post&quot;&gt;

  &lt;h1&gt;{{  page.title }}&lt;/h1&gt;

  &lt;div class=&quot;entry&quot;&gt;
    {{  content }}'
  &lt;/div&gt;

  &lt;div class=&quot;date&quot;&gt;
    Written on {{  page.date | date: &quot;%B %e, %Y&quot; }}
  &lt;/div&gt;


&lt;/article&gt;
&lt;/pre&gt;

&lt;p&gt;Thus when a post is created using the template,then the title specified in the frontmatter is the post is accessed via variable page.title.The page content is accessed via variable content.
Jekyll provides numerous  predefined global variables that you can set in the front matter of a page or post.&lt;/p&gt;

&lt;p&gt;Information on some of them can be found at &lt;a href=&quot;http://jekyllrb.com/docs/frontmatter/#predefined-global-variables&quot;&gt;Frontmatter Predefined variables&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The rendered html output for above file is&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/ss8.png&quot; alt=&quot;enter link description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we have seen that in &lt;strong&gt;post.html&lt;/strong&gt; file we have included frontmatter &lt;strong&gt;“layout”&lt;/strong&gt;.This enables us to include the contents of the post.html file into another files as its contents using Liquid markup language.&lt;/p&gt;

&lt;p&gt;This enables us to maintain layout and content files separately and we can change the site layout whenever required without making any changes to the content files.The frontmatter predefined variable provides a lot of flexibility in how we can define complex layouts and themes for the website.&lt;/p&gt;

&lt;p&gt;There is a &lt;strong&gt;index.html&lt;/strong&gt; file in the project repository that is auto generated by github pages.
We modify this file so that it can be used as a base template for all the pages on the website.&lt;/p&gt;

&lt;p&gt;we create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_layouts&lt;/code&gt; directory in the root folder of the repository.This directory contains all the files that can be accessed by defining the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;layout&lt;/code&gt; variable in the frontmatter of the files.If the layout variable is assigned values post then the file post.html in the _layout directory will be accessed.&lt;/p&gt;

&lt;p&gt;we create a file default.html.The default.html file contains the html headers,javascript,stylesheets etc as well as contents to be included in header and footer of pages. Again Liquid markup language is used to specify where the content is be be included&lt;/p&gt;

&lt;pre class=&quot;brush : html &quot;&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta charset=&quot;utf-8&quot; /&gt;
    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;chrome=1&quot; /&gt;
    &lt;title&gt;pyVision by pi19404&lt;/title&gt;

    &lt;link rel=&quot;stylesheet&quot; href=&quot;/stylesheets/styles.css&quot; /&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;/stylesheets/pygment_trac.css&quot; /&gt;
    &lt;script src=&quot;javascripts/scale.fix.js&quot;&gt;&lt;/script&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, user-scalable=no&quot; /&gt;

 
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
      &lt;header&gt;
         ........ 
      &lt;/header&gt;
      &lt;section&gt;
        {{  content }}  
      &lt;/section&gt;
    &lt;/div&gt;
    &lt;footer&gt;
      .......
    &lt;/footer&gt;
    
    
  &lt;/body&gt;
&lt;/html&gt;
&lt;/pre&gt;

&lt;p&gt;If the file is markdown file then all its contents are inserted in place of contents tag.If the file is HTML then the declaration inside section tag of class content is inserted in place of contents tag.All the pages of the website including the main page index.html contains frontmatter are designed so that generated contents are inserted within default layout.&lt;/p&gt;

&lt;h2 id=&quot;posting-blog&quot;&gt;Posting Blog&lt;/h2&gt;
&lt;p&gt;All the blog posts reside in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory.The format of filename is &lt;strong&gt;year-month-day-title.ext&lt;/strong&gt;.This will generate the blogs posts in year/month/day directory of static website.
The blog posts can be html or markdown .&lt;/p&gt;

&lt;p&gt;Let us create a blog post called 2014-10-03-Jekyll.md&lt;/p&gt;

&lt;pre class=&quot;brush : html &quot;&gt;
---
layout: post
title: Jekyll A Static Website Generator
---

Introduction
-------------
In this article we will look at **Jekyll** static site generator to generate a static website and host the same on **github**  using github pages.
..........
&lt;/pre&gt;

&lt;p&gt;Now we need to provide links to access the blog content from the main page of website.This is done using Jekyll variables and adding the below content in the &lt;strong&gt;index.html&lt;/strong&gt; page&lt;/p&gt;

&lt;pre class=&quot;brush : html &quot;&gt;
---
layout: default
---
&lt;section class=&quot;content&quot;&gt;
      &lt;section&gt;
        .......
      &lt;/section&gt;

&lt;ul class=&quot;entries&quot;&gt;
  &lt;li&gt; Blog Posts -{{  site.url }}&lt;/li&gt;
  {{%  for post in site.posts %}}

  &lt;li&gt;
    &lt;a href=&quot;&quot;&gt;      
      &lt;h3&gt;{{ post.title }}&lt;/h3&gt;
    &lt;/a&gt;
  &lt;/li&gt;
 
  {{%  endfor %}}
&lt;/ul&gt;

&lt;/section&gt;
&lt;/pre&gt;

&lt;p&gt;now we launch the website on local machine by executing command&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle exec jekyll serve&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/pyVision/images/blog/images/ss9.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;now we push the repository onto github&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git push origin gh-pages&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;access the webpage and observe the similar output as on local server&lt;/p&gt;

&lt;p&gt;The markdown used is called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GitHub Flavored Markdown&lt;/code&gt;,It is different from standard markdown language.The following like gives the difference and highlights the features of &lt;a href=&quot;https://help.github.com/articles/github-flavored-markdown/&quot;&gt;Github Flavored markdown&lt;/a&gt; language&lt;/p&gt;

&lt;p&gt;now that we have created the html,say we want to use the html content on other sites like codeproject or blogger.we can access the html at _site/posts/2013/10/03/Jekyll.html.&lt;/p&gt;

&lt;p&gt;we can copy the relevant sections of html file and with slight modifications make it compatible with other websites.&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p&gt;The pyVision repository can be found at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[www.github.com/pi19404/pyvision](www.github.com/pi19404/pyvision)&lt;/code&gt; and website can be seen at &lt;a href=&quot;http://pi19404.github.io/pyVision/&quot;&gt;pyvision&lt;/a&gt;
All the files used in the preset article can be found in the gh-pages branch of the repository.&lt;/p&gt;

&lt;p&gt;The file for the present article can be found at &lt;strong&gt;_posts/2014-10-03-Jekyll.md&lt;/strong&gt;
The source files in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gh-pages branch&lt;/code&gt; of the repository files can also be downloaded from&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.codeproject.com/KB/Articles/826515/pyVision.rar&quot;&gt;Download pyVision.rar&lt;/a&gt; - 1.5 MB&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.codeproject.com/KB/Articles/826515/pyVision.zip&quot;&gt;Download pyVision.zip&lt;/a&gt; - 1.6 MB&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pi19404/pyVision/releases/download/pyVision/pyVision_ghpages.rar&quot;&gt;Alternate link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 

</feed>
